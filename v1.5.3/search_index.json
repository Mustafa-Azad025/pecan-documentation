[
["index.html", "The Predictive Ecosystem Analyzer Welcome", " The Predictive Ecosystem Analyzer 2018-03-28 Welcome Ecosystem science, policy, and management informed by the best available data and models Our Mission: Develop and promote accessible tools for reproducible ecosystem modeling and forecasting PEcAn Website Public Chat Room Github Repository "],
["project-overview.html", "1 Project Overview", " 1 Project Overview The Predictive Ecosystem Analyzer (PEcAn) is an integrated informatics toolbox for ecosystem modeling (Dietze et al. 2013, LeBauer et al. 2013). PEcAn consists of: An application program interface (API) that encapsulates an ecosystem model, providing a common interface, inputs, and output. Core utilities for handling and tracking model runs and the flows of information and uncertainties into and out of models and analyses An accessible web-based user interface and visualization tools An extensible collection of modules to handle specific types of analyses (sensitivity, uncertainty, ensemble), model-data syntheses (benchmarking, parameter data assimilation, state data assimilation), and data processing (model inputs and data constraints) This project is motivated by the fact that many of the most pressing questions about global change are limited by our ability to synthesize existing data and strategically prioritize the collection of new data. This project seeks to improve this ability by developing a framework for integrating multiple data sources in a sensible manner. The workflow system allows ecosystem modeling to be more reproducible, automated, and transparent in terms of operations applied to data, and thus ultimately more comprehensible to both peers and the public. It reduces the redundancy of effort among modeling groups, facilitate collaboration, and make models more accessible the rest of the research community. PEcAn is not itself an ecosystem model, and it can be used to with a variety of different ecosystem models; integrating a model involves writing a wrapper to convert inputs and outputs to and from the standards used by PEcAn. Currently, PEcAn supports multiple models listed PEcAn Models. Acknowledgements This material is based upon work supported by the National Science Foundation under Grant No. 1062547, 1062204, 1241894, 1261582, 1318164, 1346748, 1458021, 1638577, the National Aeronautics and Space Administration (NASA) Grant No. 13-TE13-0060, and the Energy Biosciences Institute. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or NASA. PEcAn is a collaboration among research groups at the Department of Earth And Environment at Boston University, the Energy Biosciences Institute at the University of Illinois, the Image Spatial Data Analysis group at NCSA, the Department of Atmospheric &amp; Oceanic Sciences at the University Wisconsin-Madison, and the Terrestrial Ecosystem Science &amp; Technology (TEST) Group at Brookhaven National Lab. BETY-db is a product of the Energy Biosciences Institute at the University of Illinois at Urbana-Champaign. We gratefully acknowledge the great effort of other researchers who generously made their own data available for further study. PEcAn Publications * Feng X, Uriarte M, González G, et al. Improving predictions of tropical forest response to climate change through integration of field studies and ecosystem modeling. Glob Change Biol. 2018;24:e213–e232.doi:10.1111/gcb.13863 * Shiklomanov. A, MC Dietze, T Viskari, PA Townsend, SP Serbin. 2016 “Quantifying the influences of spectral resolution on uncertainty in leaf trait estimates through a Bayesian approach to RTM inversion” Remote Sensing of the Environment 183: 226-238 * Viskari et al. 2015 Model-data assimilation of multiple phenological observations to constrain and forecast leaf area index. Ecological Applications 25(2): 546-558 * Dietze, M. C., S. P. Serbin, C. Davidson, A. R. Desai, X. Feng, R. Kelly, R. Kooper, D. LeBauer, J. Mantooth, K. McHenry, and D. Wang (2014) A quantitative assessment of a terrestrial biosphere model’s data needs across North American biomes. Journal of Geophysical Research-Biogeosciences doi:10.1002/2013jg002392 * LeBauer, D.S., D. Wang, K. Richter, C. Davidson, &amp; M.C. Dietze. (2013). Facilitating feedbacks between field measurements and ecosystem models. Ecological Monographs. doi:10.1890/12-0137.1 * Wang, D, D.S. LeBauer, and M.C. Dietze(2013) Predicting yields of short-rotation hybrid poplar (Populus spp.) for the contiguous US through model-data synthesis. Ecological Applications doi:10.1890/12-0854.1 * Dietze, M.C., D.S LeBauer, R. Kooper (2013) On improving the communication between models and data. Plant, Cell, &amp; Environment doi:10.1111/pce.12043 Longer / auto-updated list of publications that mention PEcAn’s full name in Google Scholar "],
["contributor-covenant-code-of-conduct.html", "2 Contributor Covenant Code of Conduct", " 2 Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others’ private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at pecanproj@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant version 1.4, available at http://contributor-covenant.org/version/1/4. "],
["about-the-pecan-book.html", "3 About the PEcAn Book 3.1 How to Contribute", " 3 About the PEcAn Book This book serves as documentation for the PEcAn Project. It contains descriptions of topics necessary to inform a beginner and advanced user as well as requisite materials for developers. It does not contain low-level descriptions of functions within PEcAn. Our aim for this documentation is to educate you about the PEcAn software, the possibilities of its usage, and the standards,expectations, and core workflows for developers. 3.1 How to Contribute If you wish to contribute to this book, we greatly welcome it! The same general steps to contributing PEcAn code to the main line can be used to contribute to this book. 3.1.1 Editing the book yourself Navigate to the main PEcAn github repository: https://github.com/PecanProject/pecan and fork the repository to your github account. Use git clone on the machine your are using to create a local copy of the repository. General instructions can be found here Navigate to the book_source directory and edit the page you would like to change using any text editor. Save your files in the R markdown format (.Rmd). If you are adding a new page, you must edit the _bookdown.yml file also and list your page in order for it to be properly indexed by gitbook. Within the directory book_source diectory, execute make Use git add changedfile.Rmd to stage your changes and then commit git commit -m &quot;your message&quot; your changes and push (git push) them to your fork. Create a pull request, using github, with a description of the changes you have made. 3.1.2 Feedback/Comments/Suggestions We want your ideas, thoughts, comments, and suggestions! This is an open-sourced project so please feel free to reach out to us in the following ways: Github: https://github.com/PecanProject/pecan Serving as the main hub of communication surrounding PEcAn development, check out the issues section to see what is coming down the pipeline. If there is anything you feel you can contribute to, leave a comment. ​ Slack: https://pecanproject.slack.com/ Slack serves as our day to day mode of communication. To join us in slack you will need to create an account first. This is done in 3 steps: Request an inivitation to join slack, this will be send by email to address you provided. Check your inbox for an email from Slack with subject “Rob Kooper has invited you to join a Slack workspace”. This email should have a link that you can click to join slack. When you click a webpage will open up that asks you to create an account, once that is done you can login into the slack chatrooms. Email: pecanproj[at]gmail.com If you do not wish your communication with the team to be public, send us an email at the address above and we will get back to you as soon as possible. 3.1.3 Main Topics This book is organized around these main topics: Basic User Guide - Explains a simple setup of PEcAn and how to execute a basic run Advanced User Guide- Web - Explains the advanced features offered on our web interface ​ Advanced User Guide - Cmd Line - Explains more advanced customization of PEcAn that can be run from the command line. ​ User Appendix - Supplementary materials for the user section. PEcAn Workflow - High Level explanation of the PEcAn workflow. Development Guide - Explains what you need to know in order to contibute code to PEcAn. ​ Appendix - Supplementary materials for the developer section. ​ "],
["GettingStarted.html", "4 Getting Started 4.1 How PEcAn Works in a nutshell 4.2 Download the PEcAn VM 4.3 Using AmazonWeb Services (AWS) 4.4 Working with the PEcAn Containers (Docker) [Experimental] 4.5 PEcAn Demos", " 4 Getting Started 4.1 How PEcAn Works in a nutshell PEcAn provides an interface to a variety of ecosystem models and attempts to standardize and automate the processes of model parameterization, execution, and analysis. First, you choose an ecosystem model, then the time and location of interest (a site), the plant community (or crop) that you are interested in simulating, and a source of atmospheric data from the BETY database (LeBauer et al, 2010). These are set in a “settings” file, commonly named pecan.xml which can be edited manually if desired. From here, PEcAn will take over and set up and execute the selected model using your settings. The key is that PEcAn uses models as-is, and all of the translation steps are done within PEcAn so no modifications are required of the model itself. Once the model is finished it will allow you to create graphs with the results of the simulation as well as download the results. It is also possible to see all past experiments and simulations. There are two ways of using PEcAn, via the web interface and directly within R. Even for users familiar with R, using the web interface is a good place to start because it provides a high level overview of the PEcAn workflow.The quickest way to get started is to download the virtual machine or use an AWS instance. 4.2 Download the PEcAn VM PEcAn consists of a set of scripts and code that is compiled within a Linux operating system and saved in a “virtual machine (VM)”. Virtual machines allow for running consistent set-ups without worrying about differences between operating systems, library dependencies, compiling the code, etc. To run the PEcAn VM you will need to install VirtualBox, the program that runs the virtual machine http://www.virtualbox.org. On Windows you may see a warning about Logo testing, it is okay to ignore the warning. After you have Virtual Box installed you’ll need to download the PEcAn virtual machine: http://opensource.ncsa.illinois.edu/projects/artifacts.php?key=PECAN. The virtual machine is available under the “Files” header. Click the 32 or 64 bit “.ova” file and note that the download is ~5 GB so will take from several minutes to hours depending on the connection speed. To open up the virtual machine you’ll first want to open up VirtualBox. The first time you use the VM you’ll want to use File → Import Appliance in VirtualBox in order to import the VM. This will create a virtual machine from the disk image. When asked about the Appliance Import Settings make sure you select “Reinitialize the MAC address of all network cards”. This is not selected by default and can result in networking issues since multiple machines might claim to have the same network MAC Address. That said, users who have experienced network connection difficulties within the VM have sometimes had better luck after reinstalling without reinitializing. Next, click “Import”. You only have to do this Import step once, in the future you can just skip to the next step. Start Pecan by double clicking on the icon for the VM. A terminal window will pop up showing the machine booting up which may take a minute. It is done booting when you get to the “pecan32 login:” prompt. You do not need to login as the VM behaves like a server that we will be accessing through you web browser. Feel free to minimize the VM window. If you do want to login to the VM, the credentials are as follows: username: carya, password: illinois (after the pecan tree, Carya illinoinensis). To launch the PEcAn web interface, make sure that the VM is running somewhere in the background on your machine. Then, open any web browser on the same machine and navigate to localhost:6480/pecan to start the PEcAn workflow. 4.3 Using AmazonWeb Services (AWS) Login to Amazon Web Services (AWS) and select the EC2 Dashboard. If this is your first time using AWS you will need to set up an account before you are able to access the EC2 Dashboard. Important: You will need a credit card number and access to a phone to be able to verify AWS account registration. AWS is free for one year. Choose AMI On the top right next to your name, make sure the location setting is on U.S. East (N. Virginia), not U.S. West (Oregon) On the left click, click on EC2 (Virtual servers), then click on “AMIs”, also on the left In the search window toggle to change “Owned by me” to “Public images” Type “pecan” into the search window Click on the toggle button on the left next to PEcAn1.4.6 Click on the “Launch” button at the top Choose an Instance Type Select what type of machine you want to run. For this demo the default, t2.micro, will be adequate. Be aware that different machine types incur very different costs, from 1.3 cents/hour to over $5/hr https://aws.amazon.com/ec2/pricing/ Select t2.micro, then click “Next: Configure Instance Details” Configure Instance Details The defaults are OK. Click “Next: Add Storage” Add Storage The defaults are OK. Click “Next: Tag Instance” Tag Instance You can name your instance if you want. Click “Next: Configure Security Group” Configure Security Group You will need to add two new rules: Click “Add Rule” then select “HTTP” from the pull down menu. This rule allows you to access the webserver on PEcAn. Click “Add Rule”, leave the pull down on “Custom TCP Rule”, and then change the Port Range from 0 to 8787. Set “Source” to Anywhere. This rule allows you to access RStudio Server on PEcAn. Click “Review and Launch” . You will then see this pop-up: Select the default drive volume type and click Next Review and Launch Review the settings and then click “Launch”, which will pop up a select/create Key Pair window. Key Pair Select “Create a new key pair” and give it a name. You won’t actually need this key unless you need to SSH into your PEcAn server, but AWS requires you to create one. Click on “Download Key Pair” then on “Launch Instances”. Next click on “View Instances” at the bottom of the following page. Instances You will see the status of your PEcAn VM, which will take a minute to boot up. Wait until the Instance State reads “running”. The most important piece of information here is the Public IP, which is the URL you will need in order to access your PEcAn instance from within your web browser (see Demo 1 below). Be aware that it often takes ~1 hr for AWS instances to become fully operational, so if you get an error when you put the Public IP in you web browser, most of the time you just need to wait a bit longer. Congratulations! You just started a PEcAn server in the “cloud”! When you are done using PEcAn, you will want to return to the “Instances” menu to turn off your VM. To STOP the instance (which will turn the machine off but keep your work), select your PEcAn instance and click Actions &gt; Instance state &gt; Stop. Be aware that a stopped instance will still accrue a small storage cost on AWS. To restart this instance at any point in the future you do not want to repeat all the steps above, but instead you just need to select your instance and then click Actions &gt; Instance state &gt; Start To TERMINATE the instance (which will DELETE your PEcAn machine), select your instance and click Actions &gt; Instance state &gt; Terminate. Terminated instances will not incur costs. In most cases you will also want to go to the Volumes menu and delete the storage associated with your PEcAn VM.Remember, AWS is free for one year, but will automatically charge a fee in second year if account is not cancelled. 4.4 Working with the PEcAn Containers (Docker) [Experimental] Following are the steps to setup a Docker instance of the PEcAn. This is experimental and can have a lot of bugs, If you find any bug please report it. Make sure that the machine on which have docker and docker-compose installed. For instruction on how to install docker and docker-compose please visit the official documentations. Visit the PEcAn Project on github and clone the repository to your machine. cd to root of the repository and run docker-compose up -d here -d makes it run in detached mode so it won’t show the log on the terminal The above command pull the respective docker images and also create the required images. To access the web interface can visit :8080 If using localmachine then can use localhost:8080 Only SIPNET model is included as the default model in it. 4.5 PEcAn Demos Once you have installed A VM or have a version of PEcAn, we have developed a number of demos and vignettes to guide your use of PEcAn’s many capabilities: Type Title Web Link Source Rmd Demo Basic Run html Rmd Demo Uncertainty Analysis html Rmd Demo Output Analysis html Rmd Demo MCMC html Rmd Demo Parameter Assimilation html Rmd Demo State Assimilation html Rmd Demo Sensitivity html Rmd Vignette Allometries html Rmd Vignette MCMC html Rmd Vignette Meteorological Data html Rmd Vignette Meta-Analysis html Rmd Vignette Photosynthetic Response Curves html Rmd Vignette Priors html Rmd Vignette Leaf Spectra:PROSPECT inversion html Rmd "],
["web-workflow.html", "5 Web workflow 5.1 Site and model selection 5.2 Model configuration 5.3 Choosing initial vegetation", " 5 Web workflow Model and site selection Run configuration Run execution Results Interactive visualizations Note: We recommend that all new users begin with Demo 01. The documentation below assumes you are already familiar with how to navigate to PEcAn’s interactive web interface for running models. 5.1 Site and model selection This page is used to select the model to run and the site at which you would like to run that model. NOTE: If this page does not load for you, it may be related to a known Google Maps API key issue. See issue #1269 for a possible solution. 5.1.1 Selecting a model On the Select Host webpage use the Host pull-down menu to select the server you want to run on. PEcAn is designed to allow models to be run both locally and on remote high-performance computing (HPC) resources (i.e. clusters). We recommend that users start learning PEcAn with local runs. More information about connecting your PEcAn instance to a cluster can be found here. Next, select the model you want to run under the Model pull-down menu. The list of models currently supported by PEcAn, along with information about these models, is available here. If a PEcAn-supported model is not listed, most likely it is because the model has not been installed on the server. The PEcAn team does not have permissions to redistribute all of the models that are coupled to the system and you may need to consult the PEcAn model listing for information about how to get a copy and install it on your machine. Once the model is installed and you have added the location of the model executable to the PEcAn database, your model should appear on the PEcAn Select Host page after your refresh the page. If you would like to add a new model to PEcAn please consult our guide for Adding an Ecosystem Model and contact the PEcAn team for assistance. If selecting your model causes your site to disappear from the Google Map then the site exists but there are no drivers for that site registered in the database. Click on the “Conversion” checkbox. If your site reappears then PEcAn believes that I can generate the required inputs for this site on-the-fly using its input processing workflow. If the site does not reappear, then inputs are required for a model that PEcAn cannot autogenerate. This may be because the model has unique input requirements or because it has not yet been fully coupled to the PEcAn input processing workflow. Click here for more info on diagnosing what drivers are missing. 5.1.2 Selecting a site 5.1.2.1 Site Groups PEcAn provides the option of organizing sites into groups to make them easier to find and easier to run as a group. We have pre-loaded a number of common research networks (FLUXNET, LTER, NEON, etc) but you are free to create new site groups through BETY. If you are searching for a site that is not part of an existing site group, select “All Sites” to see all sites in the PEcAn database. Note: this may take a while to render. 5.1.2.2 Using existing sites Find the site on the map The simplest way of determining if a site exists in PEcAn is through the Google Map interface of the web-based workflow. You’ll want to make sure that the Host is set to All Sites and the Model is set to All Models. Find the site in BETY If the site is not on the map there’s a chance that it still in PEcAn but just lacking geographic information. To begin you’ll want to login to your local version of the BETY database. If you’re on the PEcAn VM this will be at localhost:6480/bety. Within BETY navigate to Data &gt; Sites and use the Search window on the page to try and locate your site. If you DO find your site you will want to click Edit and add geographic information so that the site will show up on the map. It is also worth noting that the site ID number shows up in the URL for the Show or Edit pages. This ID is frequently useful to know, for example if you have to set up a PEcAn settings file by hand. If you did not find you site you will want to follow the instructions below for adding a site 5.1.2.3 Adding a new site (TODO: Move most of this out) To add a new site to PEcAn you currently have to begin by logging in to BETY Navigate to Data &gt; Citations. Before you add a Site you will need to establish a Citation to associate with the site. If you went to Data &gt; Sites and just clicked on Add Site it will take you to an error page reminding you of this. Before adding a new citation, you should Search the system to make sure the Citation you are about to add doesn’t already exist. If it does you should be able to just click on the check-box icon to select that citation and then skip the next step To create a new citation click on the New Citation button, fill in the fields, and then click Create. The field URL should contain the web address that takes you to this publication on the publisher’s website. The Pdf field should similarly be the web address for the pdf for this citation. If there are no publications associated with the site you are interested in you can leave most of the fields blank, but you could add a descriptive title, such as “EBI Farm Field Data”, and a relevant contact person as the Author so that future users can associate any site-specific data with this citation. Once the Citation is created or selected this should automatically take you to the Sites page and list any Sites already associated with this citation. To create a new site click the New Site button. When creating a new cite the most critical information is the Site name and the Lat/Lon. The Lat/Lon can be entered by hand or by clicking on the site location on the Google Map interface. It is also helpful to fill in the other location information (Cite, State, Country) and the Notes to make it easier for other users to search for the site. The topographic, climate, and soils information is optional. When you are done click Create. At this point if you refresh the PEcAn site-level run webpage the site should automatically show up. 5.1.2.4 Troubleshooting 5.1.2.4.1 My site shows up when I don’t have any model selected, but disappears once I select the model I want to run Selecting a model will cause PEcAn to filter the available sites based on whether they possess the required Inputs for a given model (e.g. meteorology). To check what Inputs are missing for a site point your browser to the pecan/checksite.php webpage (e.g. localhost:6480/pecan/checksite.php). This page looks virtually identical to the site selection page, except that it has a Check button instead of Prev and Next. If you select a Machine, Model, and Site and then click Check the page should return a list of what Inputs are missing (listing both the name and the Format ID number). Don’t forget that its possible for PEcAn to have required Inputs in its database, but just not have them for the Machine where you want to run. To see more about what Inputs a given model can accept, and which of those are required, take a look at the MODEL_TYPE table entry in the database (e.g. go to localhost:6480/bety; Select Runs &gt; Model Type; and then click on the model you want to run). For information about loading missing Inputs into the database visit How to insert new Input data, and also read the rest of the pages under this section, which will provide important information about the specific classes of Inputs (e.g. meteorology, vegetation, etc). Finally, we are continually developing and refining workflows and standards for processing Input data in a model-agnostic way. The details about what Inputs can be processed automatically are discussed input-by-input in the sections below. For those looking to dive into the code or troubleshoot further, these conversions are ultimately handled under the do_conversions workflow module. 5.2 Model configuration This page is used for basic model configuration, including when your model will run and what input data it will use. 5.2.1 Selecting Plant Functional Types (PFTs) and other parameter groupings. 5.2.1.1 Using existing PFTs PEcAn does not automatically know what vegetation types are present at your study site so you need to select the PFT. Some models, such as ED2 and LINKAGES, support competition among multiple PFTs and thus you are encouraged to highlight multiple choices. Other models, such as SIPNET and DALEC, only support one PFT at a site. Many models also have parameters that control non-vegetation processes (e.g. soil biogeochemistry and hydrology). PEcAn allows users to assign these parameters to functional groups as well (e.g. a soils PFT) 5.2.1.2 Creating new PFTs To modify or add a new Plant Functional Type (PFT), or to change a PFT’s priors, navigate on the grey menu bar to Data &gt; PFTs To add a new pft, click “new PFT” at the top and enter a name and description. (hint: we’re trying to name PFTs based on model.biome.pft, ED2 is the default model if one isn’t specified) To add new species to a PFT click on [+] View Related Species and type the species, genus, or family you are looking for into the Search box. Click on the + to add. To remove a species from a PFT, click on [+] View Related Species and click on the X of the species you want to remove from the PFT. To remove a prior, click [-] View Related Prior and click on the X of the variable who’s prior you want to remove. This will cause the parameter to be excluded from all analyses (meta-analysis, sensitivity analysis, etc) and revert to its default value. To add a prior, choose one from the white box of priors on the right to choose. To view the specification of a prior, or to add a new prior, click BETY-DB &gt; Priors and enter the information on the variable, distribution name, distribution parameters, etc. N is the sample size underlying the prior specification (0 is ok for uninformative priors). You can also got to Data &gt; Variables in order to use the search function to find an existing variable (or create a new one). Please try not to create new variables unnecessarily (e.g. changes of variable name or units to what your model uses is handled internally, so you want to find the trait with the correct MEANING). Additional information on adding PFTs, Species, and Priors can be found here 5.2.2 Choosing meteorology Once a Machine, Model, and Site have been selected, PEcAn will take you to the Input selection page. From this page you will select what Plant Functional Type (PFT) you want to run at a site, the start and end dates of the run, and various Input selections. The most common of these across all models is the need to specify meteorological forcing data. The exact name of the menu item for meteorology will vary by model because all of the Input requirements are generated individually for each model based on the MODEL_TYPE table. In general there are 3 possible cases for meteorology PEcAn already has driver files in its database PEcAn does not have drivers, but can generate them from publicly available data You need (or want) to upload your own drivers The first two cases will appear automatically in the the pull down menu. For meteorological files that already exist you will see the date range that’s available. By contrast, met that can be generated will appear as “Use ”, where is the origin of the data (e.g. “Use Ameriflux” will use the micromet from an Ameriflux eddy covariance tower, if one is present at the site). If you want to upload your own met data this can be done in three ways. The default way to add met data is to incorporate it into the overall meteorological processing workflow. This is preferred if you are working with a common meteorological data product that is not yet in PEcAn’s workflow. This case can be divided into two special cases: Data is in a common MIME-type that PEcAn already has a converter for (e.g. CSV). In this case you’ll want to create a new Format record for the meta-data so that the existing converter can process this data. See documentation for adding a new format for more details. Data is in a more complicated format or interactive database, but large/useful enough to warrent a custom conversion function. Details on creating custom met conversions is here, though at this stage you would also be strongly encouraged to contact the PEcAn development team. The second-best way is to upload data in PEcAn’s standard meteorological format (netCDF files, CF metadata). See here for details about variables and units. From this standard, PEcAn can then convert the file to the model-specific format required by the model you have chosen. This approach is preferred for a rare or one-off meterological file format, because PEcAn will also be able to convert the file into the format required by any other model as well. The last option for adding met data is to add it in a model-specific format, which is often easiest if you’ve already been running your model at a site and are just switching to using PEcAn. 5.2.2.1 Met workflow In a nutshell, the PEcAn met workflow is designed to reduce the problem of converting n possible met inputs into m possible model formats, which requires n x m conversion functions as well as numerous custom functions for downscaling, gap filling, etc. Instead, PEcAn works with a single met standard, and thus requires n conversion functions, one for converting each data source into the PEcAn standard, and then m conversion functions for converting from that standard to what an individual model requires. For a new model joining the PEcAn system the burden in particularly low – writing one conversion function provides access to n inputs. Similarly, PEcAn performs all other operations/manipulations (extracting a site, downscaling, gap filling, etc) within the PEcAn standard, which means these operations only need be implemented once. Consider a generic met data product named MET for simplicity. PEcAn will use a function, download.MET, to pull data for the selected year from a public data source (e.g. Ameriflux, North American Regional Reanalysis, etc). Next, PEcAn will use a function, met2CF.MET, to convert the data into the PEcAn standard. If the data is already at the site scale it will then gapfill the data. If the data is a regional or global data product, PEcAn will then permute the data to allow easier site-level extraction, then it will extract data for the requested site and data range. Modules to address the temporal and spatial downscaling of meteorological data products, as well as their uncertainties, are in development but not yet part of the operational workflow. All of these functions are located within the data.atmosphere module. Once data is in the standard format and processed, it will be converted to the model-specific format using a met2model.MODEL function (located in that MODEL’s module). More detailed information on how PEcAn processes inputs can be found on our Input Conversion page. 5.2.2.2 Troubleshooting meteorological conversions At the current moment, most of the issues below address possible errors that the Ameriflux meteorology workflow might report 5.2.2.2.1 Could not do gapfill … The following variables have NA’s This error message means that there were gaps in the downloaded data, for whatever variables that were listed, which were larger than the current algorithm could fill. Particularly common is missing radiation or PAR data, as Ameriflux frequently converts nighttime data to NULL, and work is in progress to detect this based on solar geometry. Also common are incomplete years (first or last year of tower operations). 5.2.2.2.2 Could not get information about . Is this an Ameriflux site? This message occurs when PEcAn believes that a site is part of Ameriflux (because it was listed on the Ameriflux or FLUXNET webpage and has a US-* site code), but no data is present on the Ameriflux server. The most common reasons for this is that you have selected a site that has not submitted data to Ameriflux yet (or that data hasn’t been processed yet), or you have selected a year that’s outside the tower’s operational period. Visit Ameriflux and FLUXNET for lists of available site years. 5.2.2.2.3 Could not download data for for the year This is similar to the previous error, but in this case PEcAn did find data for the site listed, but just not for the year requested. This can usually be fixed by just altering the years of the run to match those with available data. 5.2.2.2.4 I could not find the requested var (or dimvar) in the file! PEcAn could not find a required variable within the downloaded file. Most likely this is due to that variable not being measured at this site. The most common cause of failure is the absence of atmospheric pressure data (PRESS), but since most models have a low sensitivity to this variable we are working on methods to estimate this from other sources. 5.3 Choosing initial vegetation On the Input Selection webpage, in addition to selecting PFTs, start &amp; end dates, and meteorology, many models also require some way of specifying the initial conditions for the vegetation, which may range from setting the aboveground biomass and LAI up to detailed inventory-like data on species composition and stand structure. At the moment, PEcAn has three cases for initial conditions: If files already exist in the database, they can simply be selected from the menu. For ED2, there are 3 different veg files (site, pss, css) and it is important that you select a complete set, not mix and match. If files don’t exist they can be uploaded following the instructions on How to insert new Input data. Automated vegetation initial condition workflow As with meteorology, PEcAn is working to develop a model-agnostic workflow for converting various sources of vegetation data to common standards, developing common processing tools, and then writing out to model-specific formats. This process is in a much early stage than the meteorology workflow, as we are still researching what the options are for standard formats, but ultimately aims to be much more broad in scope, considering not just plot inventory data but also historical documentation, paleoecological proxies, satellite remote sensing (e.g. LANDSAT), airborne hyperspectral imagery, and active remote sensing (Lidar, Radar). At the moment, what is functional is a prototype workflow that works for inventory-based vegetation data. This data can come from either files that have been registered with the BETY Inputs and Formats tables or can be queried from the USFS Forest Inventory and Analysis (FIA). For more information visit Section 13.1.2.2 Vegetation Data 5.3.0.1 US FIA This tool works with an internal copy of the FIA that is uploaded to a postGRES database along side BETY, however for space reasons this database does not ship with the PEcAn VM. To turn this feature on: Download and Install the FIA database For web-base runs, specify the database settings in the config.php For R-based runs, specify the database settings in the pecan.xml More detailed information on how PEcAn processes inputs can be found on our Input Conversion page. 5.3.1 Spin up A number of ecosystem models are typically initialized by spinning up to steady state. At the moment PEcAn doesn’t handle spin up automatically (e.g. looping met, checking for stability), but there are various ways to achieve a spin-up within the system. Option 1: If there are model-specific settings in a model’s settings/config file, then that file can be accessed by clicking on the Edit model config check box. If this box is selected then PEcAn will pause the site run workflow after it has generated your model config file, but before it runs the model, and give you an opportunity to edit the file by hand, allowing you to change any model-specific spin up settings (e.g met recycling, spin up length) Option 2: Set start_year very early and set the met drivers to be a long time series (e.g. PalEON, something custom uploaded to Inputs) Option 3: In the MODEL_TYPE table, add your model’s restart format as an optional input, modify the model specific write.config function to use that restart, and then load a previous spin-up to the Inputs table Beyond these options, we hope to eventually develop more general, model-agnostic tools for spin up. In particular, we have started to explore the accelerated spin-up and semi-analytical techniques being developed by Yiqi Luo’s lab 5.3.2 Selecting a soils product Many models have requirements for soils information, which may include: site-specific soil texture and depth information; soil biogeochemical initial conditions (e.g. soil carbon and nitrogen pools); soil moisture initial conditions; and soil thermal initial conditions. As with Choosing initial vegetation, we eventually hope to develop data standards, soils workflows, and spin-up tools, but at the moment this workflow is in the early stages of development. Model requirements need to be met by inserting Input data into the database or using files that have already been uploaded. Similar to met, we recommend that this file be in the PEcAn-standard netCDF described below, but model-specific files can also be registered. 5.3.2.1 Soil texture, depth, and physical parameters A PEcAn-standard netCDF file format exists for soil texture, depth, and physical parameters, using PEcAn standard names that are largely a direct extention of the CF standard. The easiest way to create this file is with the PEcAn R function soil2netcdf as described in the Soil Data section of the Advanced Users Guide. A table of standard names and units can be listed using PEcAn.data.land::soil.units() with no arguments. More detailed information on how PEcAn processes inputs can be found on our Input Conversion page. 5.3.3 Other model inputs Finally, any other model-specific inputs (e.g. N deposition, land use history, etc), should be met by inserting Input data into the database or using files that have already been uploaded. "],
["introduction.html", "6 Introduction", " 6 Introduction This section will provide information to those wanting to take advantage of PEcAn’s customizations from the web interface. The PEcAn XML - A tour of the configuration file driving the PEcAn workflow Additional web configuration - Advanced options available from the web interface Brown Dog Sensitivity and ensemble analyses Editing model configurations [Hidden analyses] - Analyses only available from the PEcAn XML Parameter data assimilation (PDA) State data assimilation (SDA) Remote execution with PEcAn - Running analyses and generally working with external machines (HPC) in the context of PEcAn. "],
["pecanXML.html", "7 The PEcAn XML 7.1 PEcAn folders 7.2 Database Access 7.3 BETY Database Configuration 7.4 Brown Dog Configuration 7.5 PFT Selection 7.6 Meta Analysis 7.7 Ensemble Runs 7.8 Sensitivity Runs 7.9 Model Setup 7.10 Run Setup 7.11 State Data Assimilation Tags 7.12 Parameter Data Assimilation 7.13 Benchmarking 7.14 Model-specific configuration", " 7 The PEcAn XML The PEcAn system is configured using a xml file, often called settings.xml. The configuration file contains the following sections: PEcAn Folders Database Access BETY Configuration Brown Dog PFT Selection Meta Analysis Ensemble Runs Sensitivity Runs Model Setup Run Setup State Data Assimilation Parameter Data Assimilation Benchmarking 7.1 PEcAn folders The following are the tags that can be used to configure the folders used by PEcAn. All of these are optional. &lt;outdir&gt;/home/carya/testrun.pecan&lt;/outdir&gt; outdir : [optional] specifies where PEcAn will write all outputs and create folders. If this is not specified the folder pecan in the current folder will be used. 7.2 Database Access The connection to the BETY database is configured using this section. In this section you will specify what driver to use to connect to the database (PostgreSQL by default) and the connection parameters to connect to the database. This section is very picky and will only accept parameters that are passed into the connection function for each database driver, any other entries will result in an error by the database driver. 7.2.0.1 functions: db.open(), and thus by db.query() and db.check(). New format (starting with PEcAn 1.3.6) &lt;database&gt; &lt;bety&gt; &lt;dbname&gt;bety&lt;/dbname&gt; &lt;username&gt;bety&lt;/username&gt; &lt;password&gt;bety&lt;/password&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;write&gt;true&lt;/write&gt; &lt;/bety&gt; &lt;fia&gt; &lt;dbname&gt;fia5data&lt;/dbname&gt; &lt;username&gt;bety&lt;/username&gt; &lt;password&gt;bety&lt;/password&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;/fia&gt; &lt;/database&gt; The connection information under bety will be used by most of the PEcAn system, only the extraction of pss/css/site information from the FIA database will use the connection information in the fia section. driver : [optional] the driver to use to connect to the database. Default value is PostgreSQL dbname : [optional] the name of the database (was name), default value is the username of the current user logged in. user : [optional] the username to connect to the database (was userid), default value is the username of the current user logged in (PostgreSQL uses user for this field). password : [optional] the password to connect to the database (was passwd), if not specified no password is used. host : [optional] the name of the host to connect to, default value is localhost. write : [optional] should any results be written back to the database, setting this to TRUE (the default) will write any results back into the database. (since PEcAn 1.3.6) For other database drivers these parameters will change. See the driver documentation in R for the right parameters. 7.3 BETY Database Configuration This section describes how to connect to the BETY Database. This section is used for versions of PEcAn prior to version 1.3.6, starting at 1.3.6 this parameter is part of &lt;database&gt;&lt;bety&gt; and defaults to TRUE. 7.3.0.1 functions: db.check(), write.configs(), run.models() &lt;bety&gt; &lt;write&gt;TRUE&lt;/write&gt; &lt;/bety&gt; write : [optional] this can be TRUE/FALSE (the default is TRUE). If set to TRUE, runs, ensembles and workflows are written to the database. 7.4 Brown Dog Configuration This section describes how to connect to Brown Dog. This will allow for conversions of data (currently only met data). 7.4.0.1 functions: met.process() &lt;browndog&gt; &lt;url&gt;...&lt;/url&gt; &lt;username&gt;...&lt;/username&gt; &lt;password&gt;...&lt;/password&gt; &lt;/browndog&gt; url : [required] endpoint for Brown Dog to be used. username : [optional] username to be used with the endpoint for Brown Dog. password : [optional] password to be used with the endpoint for Brown Dog. 7.5 PFT Selection The PEcAn system requires at least 1 PFT (Plant Functional Type) to be specified inside the &lt;pfts&gt; section. 7.5.0.1 functions: get.traits() 7.5.0.2 tags &lt;pfts&gt; &lt;pft&gt; &lt;name&gt;sipnet.temperate.coniferous&lt;/name&gt; &lt;outdir&gt;/home/carya/testrun.pecan/pft/1/&lt;/outdir&gt; &lt;constants&gt; &lt;num&gt;1&lt;/num&gt; &lt;/constants&gt; &lt;/pft&gt; &lt;/pfts&gt; name : [required] the pft as is found inside the BETY database, this needs to be an exact match. outdir: [optional] path in which pft-specific output will be placed during meta-analysis and sensitivity analysis. If not specified it will be written into &lt;outdir&gt;/&lt;pftname&gt;. contants: [optional] this section contains information that will be written directly into the model specific configuration files. PEcAn does not look at the information in this section. 7.6 Meta Analysis 7.6.0.1 functions: run.meta.analysis() 7.6.0.2 tags &lt;meta.analysis&gt; &lt;iter&gt;1000&lt;/iter&gt; &lt;random.effects&gt;FALSE&lt;/random.effects&gt; &lt;update&gt;FALSE&lt;/update&gt; &lt;threshold&gt;1.2&lt;/threshold&gt; &lt;/meta.analysis&gt; The section meta.analysis needs to exists for a meta.analysis to be executed, even though all tags inside are optional. iter : [optional] [MCMC](http:/en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (Markov Chain Monte Carlo) chain length, i.e. the total number of posterior samples in the meta-analysis, default is 3000. Smaller numbers will run faster but produce larger errors. random.effects : [optional] Whether to include random effects (site, treatment) in meta-analysis model. Can be set to FALSE to work around convergence problems caused by an over parameterized model (e.g. too many sites, not enough data). The default value is TRUE. update : [optional] Should previous results of meta.analysis and get.traits be re-used. If set to TRUE the meta-analysis and get.trait.data will always be executed. Setting this to FALSE will try and reuse existing results. Future versions will allow for AUTO as well which will try and reuse if the PFT/traits have not changed. The default value is FALSE. threshold threshold for Gelman-Rubin convergence diagnostic (MGPRF); default is 1.2 7.7 Ensemble Runs Only if this section is defined an ensemble analysis is done. 7.7.0.1 functions: write.configs(), run.ensemble.analysis() 7.7.0.2 tags &lt;ensemble&gt; &lt;size&gt;5&lt;/size&gt; &lt;variable&gt;GPP&lt;/variable&gt; &lt;variable&gt;NPP&lt;/variable&gt; &lt;/ensemble&gt; size : [required] the number of runs in the ensemble. variable: [optional] name of one (or more) variables the analysis should be run for. If not specified, sensitivity.analysis variable is used, otherwise default is GPP. Note: if the ensemble size is set to 1, PEcAn will select the posterior median parameter values rather than taking a single random draw from the posterior 7.8 Sensitivity Runs Only if this section is defined a sensitivity analysis is done. This section will have &lt;quantile&gt; or &lt;sigma&gt; nodes. If neither are given, the default is to use the median +/- [1 2 3] x sigma (e.g. the 0.00135 0.0228 0.159 0.5 0.841 0.977 0.999 quantiles); If the 0.5 (median) quantile is omitted, it will be added in the code. 7.8.0.1 functions: write.configs(), run.sensitivity.analysis() 7.8.0.2 tags &lt;sensitivity.analysis&gt; &lt;quantiles&gt; &lt;quantile&gt;&lt;/quantile&gt; &lt;sigma&gt;-3&lt;/sigma&gt; &lt;sigma&gt;-2&lt;/sigma&gt; &lt;sigma&gt;-1&lt;/sigma&gt; &lt;sigma&gt;1&lt;/sigma&gt; &lt;sigma&gt;2&lt;/sigma&gt; &lt;sigma&gt;3&lt;/sigma&gt; &lt;/quantiles&gt; &lt;variable&gt;GPP&lt;/variable&gt; &lt;perpft&gt;TRUE&lt;/perpft&gt; &lt;start.year&gt;2004&lt;/start.year&gt; &lt;end.year&gt;2006&lt;/end.year&gt; &lt;/sensitivity.analysis&gt; quantiles : [optional] Quantiles of parameter distributions at which the model should be evaluated when running sensitivity analysis. Values greater than 0 and less than 1 can be used. sigma : [optional] Any real number can be used to indicate the quantiles to be used in units of normal probability. quantile : [optional] Which quantile should be used. start.date : [required?] start date of the sensitivity analysis (in YYYY/MM/DD format) end.date : [required?] end date of the sensitivity analysis (in YYYY/MM/DD format) NOTE: start.date and end.date are distinct from values set in the run tag because this analysis can be done over a subset of the run. ** variable** : [optional] name of one (or more) variables the analysis should be run for. If not specified, sensitivity.analysis variable is used, otherwise default is GPP. ** perpft** : [optional] if TRUE a sensitivity analysis on PFT-specific outputs will be run. This is only possible if your model provides PFT-specific outputs for the variable requested. This tag only affects the output processing, not the number of samples proposed for the analysis nor the model execution. 7.9 Model Setup This section is required and tells PEcAn what model to run. This section should either specify &lt;id&gt; or both &lt;name&gt; and &lt;binary&gt; of the model. If both id and name and/or binary are specified the id is used to check the specified name and/or binary. As of version 1.3.7 the &lt;name&gt; tag has been renamed &lt;type&gt;. The &lt;type&gt; tag refers to the “type” of model and is used for a) identifying appropriate pfts and b) identifying whether appropriate inputs (i.e. met data) are available for the given site and model (or if they can be created). To ensure compatability, the code will automatically convert from &lt;name&gt; to &lt;type&gt;. 7.9.0.1 functions: write.configs(), run.models() 7.9.0.2 tags &lt;model&gt; &lt;id&gt;7&lt;/id&gt; &lt;type&gt;ED2&lt;/type&gt; &lt;binary&gt;/usr/local/bin/ed2.r82&lt;/binary&gt; &lt;job.sh&gt;module load hdf5&lt;/job.sh&gt; &lt;config.header&gt; &lt;!--...xml code passed directly to config file...--&gt; &lt;/config.header&gt; &lt;/model&gt; id : [optional/required] id in the models database table, see above. name : OBSOLETE name of the model, use type from version 1.3.7 type : [optional/required] type of model, see above. binary : [optional/required] path to the model executable, see above. job.sh : [optional] additional options to add to the job.sh at the top config.headers : [optional] XML that will appear at the start of generated config files. 7.9.0.3 ED2 specific tags Following variables are ED specific and are used in the ED2 Configuration. Starting at 1.3.7 the tags for inputs have moved to &lt;run&gt;&lt;inputs&gt;. This includes, veg, soil, psscss, inputs. &lt;edin&gt;/home/carya/runs/PEcAn_4/ED2IN.template&lt;/edin&gt; &lt;config.header&gt; &lt;radiation&gt; &lt;lai_min&gt;0.01&lt;/lai_min&gt; &lt;/radiation&gt; &lt;ed_misc&gt; &lt;output_month&gt;12&lt;/output_month&gt; &lt;/ed_misc&gt; &lt;/config.header&gt; &lt;phenol.scheme&gt;0&lt;/phenol.scheme&gt; edin : [required] template used to write ED2IN file veg : OBSOLETE [required] location of VEG database, now part of ` soil : OBSOLETE [required] location of soild database, now part of ` psscss : OBSOLETE [required] location of site inforation, now part of &lt;run&gt;&lt;inputs&gt;. Should be specified as &lt;pss&gt;, &lt;css&gt; and &lt;site&gt;. inputs : OBSOLETE [required] location of additional input files (e.g. data assimilation data), now part of &lt;run&gt;&lt;inputs&gt;. Should be specified as &lt;lu&gt; and &lt;thsums&gt;. 7.10 Run Setup 7.10.0.1 tags &lt;run&gt; &lt;jobtemplate&gt;/home/carya/path/to/template&lt;/jobtemplate&gt; &lt;start.date&gt;2002-01-01 00:00:00&lt;/start.date&gt; &lt;end.date&gt;2005-12-31 00:00:00&lt;/end.date&gt; &lt;/dbfiles&gt;/home/carya/.pecan/dbfiles&lt;/dbfiles&gt; &lt;site&gt; &lt;id&gt;772&lt;/id&gt; &lt;name&gt;Niwot Ridge Forest/LTER NWT1 (US-NR1)&lt;/name&gt; &lt;lat&gt;40.032900&lt;/lat&gt; &lt;lon&gt;-105.546000&lt;/lon&gt; &lt;met.start&gt;2002/01/01&lt;/met.start&gt; &lt;met.end&gt;2005/12/31&lt;/met.end&gt; &lt;/site&gt; &lt;inputs&gt; &lt;met&gt; &lt;id&gt;10000000001&lt;/id&gt; &lt;path&gt;/fs/data1/pecan.data/input/&lt;/path&gt; &lt;source&gt;Ameriflux&lt;/source&gt; &lt;username&gt;carya&lt;/username&gt; &lt;/met&gt; &lt;/inputs&gt; &lt;/run&gt; &lt;host&gt; &lt;name&gt;localhost&lt;/name&gt; &lt;rundir&gt;/home/carya/testrun.pecan/run/&lt;/rundir&gt; &lt;outdir&gt;/home/carya/testrun.pecan/out/&lt;/outdir&gt; &lt;scratchdir&gt;/tmp/carya&lt;/scratchdir&gt; &lt;clearscratch&gt;TRUE&lt;/clearscratch&gt; &lt;qsub&gt;qsub -N @NAME@ -o @STDOUT@ -e @STDERR@ -S /bin/bash&lt;/qsub&gt; &lt;qsub.jobid&gt;Your job ([0-9]+) .*&lt;/qsub.jobid&gt; &lt;qstat&gt;qstat -j @JOBID@ &amp;&gt; /dev/null || echo DONE&lt;/qstat&gt; &lt;job.sh&gt;module load udunits R/R-3.0.0_gnu-4.4.6&lt;/job.sh&gt; &lt;/host&gt; jobtemplate : [optional] the template used when creating a job.sh file which is used to launch the actual model. Each model has it’s own template in the inst folder of the module. The following variables can e used: @SITE_LAT@, @SITE_LON@, @SITE_MET@, @START_DATE@, @END_DATE@, @OUTDIR@, @RUNDIR@ which all come variables in the pecan.xml file. The following two command can be used to copy and clean the results from a scratch folder (specified as scratch in the run section below, for example local disk vs network disk) : @SCRATCH_COPY@, @SCRATCH_CLEAR@ . start.date : [required] the first day of the simulation end.date : [required] the last day of the simulation dbfiles : [optional] location where pecan should write files that will be stored in the database. The default is store them in ${HOME}/.pecan/dbfiles Site specific information is specified in the &lt;site&gt; subsection. Either &lt;id&gt; or &lt;name&gt;, &lt;lat&gt; and &lt;lon&gt; should be specified. If id and any of the others are specified the values will be compared with those from BETYdb. id : [optional/required] id of the site in the BETY database, see above. name : [optional/required] site name, see above. lat : [optional/required] site latitude, see above. lon : [optional/required] site longitude, see above. Inputs specific information for each model type is specified in the &lt;inputs&gt; subsection. Each input will have three tags: * id : [optional/required] An id that points to the input if already in the database. * path : [optional/required] The path to the dataset if already downloaded. * source : [optional/required] The input data type. This tag name needs to match the names in the conversion functions. In general, the path should be filled in by PEcAn and not by the user. If PEcAn is processing all the input, id is not required. Alternatively, if you’re using data processed by hand, source is not required. One common input will be the weather data, often specified in &lt;met&gt;. Other model types will have different model specific inputs. PEcAn will find the location of the file on the host you run PEcAn on. met : [model type specific] most models will have a met tag to specify location of the weather data. pss : [ED2, required] location of patch file css : [ED2, required] location of cohort file site : [ED2, optional] location of site file lu : [ED2, required] location of land use file thsums : [ED2, required] location of thermal sums file veg : [ED2, required] location of vegetation data soil : [ED2, required] location of soil data Host on which the simulation will run is specified in the &lt;host&gt; subsection. If this section is not specified it is assumed the simulation will run on localhost. If qsub is specified (can be empty in which case the defaults will be used) the models will be executed using qsub. name : [optional] name of host server where model is located and executed, if not specified localhost is assumed. rundir : [optional/required] location where all the configuration files are written. For localhost this is optional (&lt;outdir&gt;/run is the default), for any other host this is required. outdir : [optional/required] location where all the outputs of the model are written. For localhost this is optional (&lt;outdir&gt;/out is the default), for any other host this is required. scratchdir : [optional] location where output is written. If specified the output from the model is written to this folder and copied to the outdir when the model is finished, this could significantly speed up the model execution (by using local or ram disk). clearscratch : [optional] if set to TRUE the scratchfolder is cleaned up after copying the results to the outdir, otherwise the folder will be left. The default is to clean up after copying. qsub : [optional] the command to submit a job to the queuing system. There are 3 parameters you can use when specifying the qsub command, you can add additional values for your specific setup (for example -l walltime to specify the walltime, etc). You can specify @NAME@ the pretty name, @STDOUT@ where to write stdout and @STDERR@, where to write stderr. You can specify an empty element () in which case it will use the default value is “qsub -V -N @NAME@ -o @STDOUT@ -e @STDERR@ -s /bin/bash”. qsub.jobid : [optional] the regular expression used to find the jobid returned from qsub. If not specified (and qsub is) it will use the default value is “Your job ([0-9]+) .*&quot; qstat : [optional] the command to execute to check if a job is finished, this should return DONE if the job is finished. There is one parameter this command should take @JOBID@ which is the id of the job as returned by qsub.jobid. If not specified (and qsub is) it will use the default value is “qstat -j @JOBID@ || echo DONE” job.sh : [optional] additional options to add to the job.sh at the top 7.11 State Data Assimilation Tags The following tags can be used for state data assimilation. More detailed information can be found here: State Data Assimilation Documentation &lt;state.data.assimilation&gt; &lt;process.variance&gt;FALSE&lt;/process.variance&gt; &lt;sample.parameters&gt;FALSE&lt;/sample.parameters&gt; &lt;state.variables&gt; &lt;variable&gt;AGB.pft&lt;/variable&gt; &lt;variable&gt;TotSoilCarb&lt;/variable&gt; &lt;/state.variables&gt; &lt;spin.up&gt; &lt;start.date&gt;2004/01/01&lt;/start.date&gt; &lt;end.date&gt;2006/12/31&lt;/end.date&gt; &lt;/spin.up&gt; &lt;forecast.time.step&gt;1&lt;/forecast.time.step&gt; &lt;start.date&gt;2004/01/01&lt;/start.date&gt; &lt;end.date&gt;2006/12/31&lt;/end.date&gt; &lt;/state.data.assimilation&gt; process.variance : [optional] TRUE/FLASE flag for if process variance should be estimated (TRUE) or not (FALSE). If TRUE, a generalized ensemble filter will be used. If FALSE, an ensemble Kalman filter will be used. Default is FALSE. sample.parameters : [optional] TRUE/FLASE flag for if parameters should be sampled for each ensemble member or not. This allows for more spread in the intial conditions of the forecast. NOTE: If TRUE, you must also assign a vector of trait names to pick.trait.params within the sda.enkf function. state.variable : [required] State variable that is to be assimilated (in PEcAn standard format). Default is “AGB” - Above Ground Biomass. spin.up : [required] start.date and end.date for model spin up. NOTE: start.date and end.date are distinct from values set in the run tag because spin up can be done over a subset of the run. forecast.time.step : [optional] start.date and end.date for model spin up. start.date : [required?] start date of the state data assimilation (in YYYY/MM/DD format) end.date : [required?] end date of the state data assimilation (in YYYY/MM/DD format) NOTE: start.date and end.date are distinct from values set in the run tag because this analysis can be done over a subset of the run. 7.12 Parameter Data Assimilation The following tags can be used for state data assimilation. More detailed information can be found here: Parameter Data Assimilation Documentation 7.12.0.1 tags Coming soon… 7.13 Benchmarking Coming soon… 7.14 Model-specific configuration See the following: ED2 SIPNET BIOCRO "],
["additional-web-configuration.html", "8 Additional web configuration 8.1 Brown Dog 8.2 Advanced Setup 8.3 Editing model configurations 8.4 Basic Setups", " 8 Additional web configuration Additional settings for web configuration: Brown Dog [Editing model configuration files] Advanced setup [Sensitivity analysis] [Uncertainty analysis] 8.1 Brown Dog The Browndog service provides PEcAn with access to large and diverse sets of data at the click of a button in the format that PEcAn needs. By clicking the checkbox you will be using the Browndog Service to process data. For more information regarding meteorological data check out Available Meteorological Drivers ** More Informatoin can be found at the Browndog website** 8.2 Advanced Setup (TODO: Under construction…) 8.3 Editing model configurations (TODO: Under construction…) 8.4 Basic Setups There are few options which you can change via web interface. To visit the configuration page either you can just click on the setups link on the introduction page alternatively can type &lt;host&gt;/setups/. The list of configuration available Database configuration : BETYdb(Biofuel Ecophysiological Traits and Yields database) configuration details, can be edited according to need. Browndog configuration : Browndog configuration details, Used to connect browndog. Its included by default in VM. FIA Database : FIA(Forest Inventory and Analysis) Database configuration details, Can be used to add additional data to models. Google MapKey : Google Map key, used to access the google map by PEcAn. Change Password : A small infomation to change the VM user password. (if using Docker image it won’t work) Automatic Sync : If ON then it will sync the database between local machine and the remote servers. Still unders testing part might be buggy. Still work on the adding other editing feature going on, this page will be updated as new configuration will be available. "],
["settings-configured-analyses.html", "9 Settings-configured analyses 9.1 Parameter data assimilation (PDA) 9.2 For a detailed usage of the module, please see the vignette under /modules/assim.batch/vignettes 9.3 State data assimilation (SDA) 9.4 MultiSettings 9.5 Benchmarking", " 9 Settings-configured analyses These analyses can be run through the web interface, but lack graphical interfaces and currently can only be configured throughthe XML settings. To run these analyses use the Edit pecan.xml checkbox on the Input configuration page. Eventually, these modules will be integrated into the web user interface. Parameter Data Assimilation (PDA) State Data Assimilation (SDA) MultiSettings Benchmarking (TODO: Add links) 9.1 Parameter data assimilation (PDA) All functions pertaining to Parameter Data Assimilation are housed within: pecan/modules/assim.batch 9.1.1 pda.mcmc.R This is the main PDA code. It performs Bayesian MCMC on model parameters by proposing parameter values, running the model, calculating a likelihood (between model output and supplied observations), and accepting or rejecting the proposed parameters (Metropolis algorithm). Additional notes: The first argument is settings, followed by others that all default to NULL.settings is a list used throughout Pecan, which contains all the user options for whatever analyses are being done. The easiest thing to do is just pass that whole object all around the Pecan code and let different functions access whichever settings they need. That’s what a lot of the rest of the Pecan code does. But the flexibility to override most of the relevant settings in settings is there by providing them directly as arguments to the function. The if(FALSE)… : If you’re trying to step through the function you probably will have the settings object around, but those other variables will be undefined. If you set them all to NULL then they’ll be ignored without causing errors. It is there for debugging purposes. The next step calls pda.settings(), which is in the file pda.utils.R (see below). It checks whether any settings are being overridden by arguments, and in most cases supplies default values if it can’t find either. In the MCMC setup section The code is set up to allow you to start a new MCMC chain, or to continue a previous chain as specified in settings. The code writes a simple text file of parameter samples at every iteration, which lets you get some results and even re-start an MCMC that fails for some reason. The code has adaptive jump distributions. So you can see some initialization of the jump distributions and associated variables here. Finally, note that after all this setup a new XML settings file is saved. The idea is that the original pecan.xml you create is preserved for provenance, and then periodically throughout the workflow the settings (likely containing new information) are re-saved with descriptive filenames. MCMC loop Periodically adjust jump distribution to make acceptance rate closer to target Propose new parameters one at a time. For each: First, note that Pecan may be handling many more parameters than are actually being targeted by PDA. Pecan puts priors on any variables it has information for (in the BETY database), and then these get passed around throughout the analysis and every step (meta-, sensitivity, ensemble analyses, etc.). But for PDA, you specify a separate list of probably far fewer parameters to constrain with data. These are the ones that get looped over and varied here. The distinction between all parameters and only those dealt with in PDA is dealt with in the setup code above. First a new value is proposed for the parameter of interest. Then, a new model run is set up, identical to the previous except with the new proposed value for the one parameter being updated on this run. The model run is started, and outputs collected after waiting for it to finish. A new likelihood is calculated based on the model outputs and the observed dataset provided. Standard Metropolis acceptance criteria is used to decide whether to keep the proposed parameter. Periodically (at interval specified in settings), a diagnostic figure is saved to disk so you can check on progress. This works only for NEE currently 9.1.2 pda.mcmc.bs.R This file is basically identical to pda.mcm.R, but rather than propose parameters one at a time, it proposes new values for all parameters at once (“bs” stands for “block sampling”). You choose which option to use by specifying settings\\(assim.batch\\)method: * “bruteforce” means sample parameters one at a time * “bruteforce.bs” means use this version, sampling all parameters at once * “emulator” means use the emulated-likelihood version 9.1.3 pda.emulator This version of the PDA code again looks quite similar to the basic “bruteforce” one, but its mechanics are very different. The basic idea is, rather than running thousands of model iterations to explore parameter space via MCMC, run a relatively smaller number of runs that have been carefully chosen to give good coverage of parameter space. Then, basically interpolate the likelihood calculated for each of those runs (actually, fit a Gaussian process to it), to get a surface that “emulates” the true likelihood. Now, perform regular MCMC (just like the “bruteforce” approach), except instead of actually running the model on every iteration to get a likelihood, just get an approximation from the likelihood emulator. Since the latter step takes virtually no time, you can run as long of an MCMC as you need at little computational cost, once you have done the initial model runs to create the likelihood emulator. 9.1.4 pda.mcmc.recover.R This function is for recovering a failed PDA MCMC run. 9.1.5 pda.utils.R This file contains most of the individual functions used by the main PDA functions (pda.mcmc.*.R). assim.batch is the main function Pecan calls to do PDA. It checks which method is requested (bruteforce, bruteforce.bs, or emulator) and call the appropriate function described above. pda.setting handles settings. If a setting isn’t found, the code can usually supply a reasonable default. pda.load.priors is fairly self explanatory, except that it handles a lot of cases and gives different options priority over others. Basically, the priors to use for PDA parameters can come from either a Pecan prior.distns or post.distns object (the latter would be, e.g., the posteriors of a meta-analysis or previous PDA), or specified either by file path or BETY ID. If not told otherwise, the code tries to just find the most recent posterior in BETY, and use that as prior for PDA. pda.create.ensemble gets an ensemble ID for the PDA. All model runs associated with an individual PDA (any of the three methods) are considered part of a single ensemble. This function does is register a new ensemble in BETY, and return the ID that BETY gives it. pda.define.prior.fn creates R functions for all of the priors the PDA will use. pda.init.params sets up the parameter matrix for the run, which has one row per iteration, and one column per parameter. Columns include all Pecan parameters, not just the (probably small) subset that are being updated by PDA. This is for compatibility with other Pecan components. If starting a fresh run, the returned matrix is just a big empty matrix to fill in as the PDA runs. If continuing an existing MCMC, then it will be the previous params matrix, with a bunch of blank rows added on for filling in during this round of PDA. pda.init.run This is basically a big wrapper for Pecan’s write.config function (actually functions [plural], since every model in Pecan has its own version). For the bruteforce and bruteforce.bs methods this will be run once per iteration, whereas the emulator method knows about all its runs ahead of time and this will be a big batch of all runs at once. pda.adjust.jumps tweaks the jump distributions for the standard MCMC method, and pda.adjust.jumps.bs does the same for the block-sampled version. pda.calc.llik calculates the log-likelihood of the model given all datasets provided to compare it to. pda.generate.knots is for the emulator version of PDA. It uses a Latin hypercube design to sample a specified number of locations in parameter space. These locations are where the model will actually be run, and then the GP interpolates the likelihood surface in between. pda.plot.params provides basic MCMC diagnostics (trace and density) for parameters being sampled. pda.postprocess prepares the posteriors of the PDA, stores them to files and the database, and performs some other cleanup functions. pda.load.data.r This is the function that loads in data that will be used to constrain the PDA. It’s supposed to be eventually more integrated with Pecan, which will know how to load all kinds of data from all kinds of sources. For now, it can do NEE from Ameriflux. pda.define.llik.r A simple helper function that defines likelihood functions for different datasets. Probably in the future this should be queried from the database or something. For now, it is extremely limited. The original test case of NEE assimilation uses a heteroskedastic Laplacian distribution. pda.get.model.output.R Another function that will eventually grow to handle many more cases, or perhaps be replaced by a better system altogether. For now though, it again just handles Ameriflux NEE. 9.1.6 get.da.data.*.R, plot.da.R Old codes written by Carl Davidson. Defunct now, but may contain good ideas so currently left in. 9.2 For a detailed usage of the module, please see the vignette under /modules/assim.batch/vignettes 9.3 State data assimilation (SDA) sda.enkf.R is housed within: /pecan/modules/assim.sequential/R The tree ring tutorial is housed within: /pecan/documentation/tutorials/StateAssimilation 9.3.1 sda.enkf.R Description This is the main ensemble Kalman filter and generalized filter code. Originally, this was just ensemble Kalman filter code. Mike Dietze and Ann Raiho added a generalized ensemble filter to avoid filter divergence. The output of this function will be all the of run outputs, a PDF of diagnostics, and an Rdata object that includes three lists: FORECAST will be the ensemble forecasts for each year ANALYSIS will be the updated ensemble sample given the NPP observations enkf.params contains the prior and posterior mean vector and covariance matrix for each time step. 9.3.2 sda.enkf.R Arguments settings - (required) pecan.SDA.xml settings object obs.mean - (required) a list of observation means named with dates in YYYY/MM/DD format obs.cov - (required) a list of observation covariances names with dates in YYYY/MM/DD format IC - (optional) initial condition matrix (dimensions: ensemble memeber # by state variables). Default is NULL. Q - (optional) process covariance matrix (dimensions: state variable by state variables). Defualt is NULL. 9.3.3 State Data Assimilation Workflow Before running sda.enkf, these tasks must be completed (in no particular order), Read in a pecan.SDA.xml settings file with tags listed below. i.e. read.settings(‘pecan.SDA.xml’) Load data means (obs.mean) and covariances (obs.cov) as lists with PEcAn naming and unit conventions. Each observation must have a date in YYYY/MM/DD format (optional time) associated with it. If there are missing data, the date must still be represented in the list with an NA as the list object. Create initial conditions matrix (IC) that is state variables columns by ensemble members rows in dimension. sample.IC.MODEL can be used to create the IC matrix, but it is not required. This IC matrix is fed into write.configs for the initial model runs. The main parts of the SDA function are: Setting up for initial runs: Set parameters Load initial run inputs via split.inputs.MODEL Open database connection Get new workflow ids Create ensemble ids Performing the initial set of runs Set up for data assimilation Loop over time read.restart.MODEL - read model restart files corresponding to start.time and stop.time that you want to assimilate data into Analysis - There are four choices based on if process variance is TRUE or FALSE and if there is data or not. See explaination below. write.restart.MODEL - This function has two jobs. First, to insert adjusted state back into model restart file. Second, to update start.time, stop.time, and job.sh. run model Save outputs Create diagnostics 9.3.4 State Data Assimilation Tags Example &lt;state.data.assimilation&gt; &lt;n.ensemble&gt;25&lt;/n.ensemble&gt; &lt;process.variance&gt;FALSE&lt;/process.variance&gt; &lt;sample.parameters&gt;FALSE&lt;/sample.parameters&gt; &lt;state.variables&gt; &lt;variable&gt; &lt;variable.name&gt;AGB.pft&lt;/variable.name&gt; &lt;unit&gt;MgC/ha/yr&lt;/unit&gt; &lt;min_value&gt;0&lt;/min_value&gt; &lt;max_value&gt;100000000&lt;/max_value&gt; &lt;/variable&gt; &lt;variable&gt; &lt;variable.name&gt;TotSoilCarb&lt;/variable.name&gt; &lt;unit&gt;KgC/m^2&lt;/unit&gt; &lt;min_value&gt;0&lt;/min_value&gt; &lt;max_value&gt;100000000&lt;/max_value&gt; &lt;/variable&gt; &lt;/state.variables&gt; &lt;spin.up&gt; &lt;start.date&gt;1950/01/01&lt;/start.date&gt; &lt;end.date&gt;1960/12/31&lt;/end.date&gt; &lt;/spin.up&gt; &lt;forecast.time.step&gt;1&lt;/forecast.time.step&gt; &lt;start.date&gt;1961/01/01&lt;/start.date&gt; &lt;end.date&gt;2010/12/31&lt;/end.date&gt; &lt;/state.data.assimilation&gt; 9.3.5 State Data Assimilation Tags Descriptions n.ensemble : [required] Number of ensemble members. Should be much larger than number of state variables you are assimilating. process.variance : [optional] TRUE/FLASE flag for if process variance should be estimated (TRUE) or not (FALSE). If TRUE, a generalized ensemble filter will be used. If FALSE, an ensemble Kalman filter will be used. Default is FALSE. sample.parameters : [optional] TRUE/FLASE flag for if parameters should be sampled for each ensemble member or not. This allows for more spread in the initial conditions of the forecast. state.variable : [required] State variable that is to be assimilated (in PEcAn standard format). spin.up : [required] start.date and end.date for initial model runs. NOTE: start.date and end.date are distinct from values set in the run tag because initial runs can be done over a subset of the full run. forecast.time.step : [optional] In the future, this will be used to allow the forecast time step to vary from the data time step. start.date : [optional] start date of the state data assimilation (in YYYY/MM/DD format) end.date : [optional] end date of the state data assimilation (in YYYY/MM/DD format) NOTE: start.date and end.date are distinct from values set in the run tag because this analysis can be done over a subset of the run. 9.3.6 Model Specific Functions for SDA Workflow 9.3.6.1 read.restart.MODEL.R The purpose of read.restart is to read model restart files and return a matrix that is site rows by state variable columns. The state variables must be in PEcAn names and units. The arguments are: outdir - output directory runid - ensemble member run ID stop.time - used to determine which restart file to read (in POSIX format) settings - pecan.SDA.xml settings object var.names - vector with state variable names with PEcAn standard naming. Example: c(‘AGB.pft’, ‘TotSoilCarb’) params - parameters used by ensemble member (same format as write.configs) 9.3.6.2 write.restart.MODEL.R This model specific function takes in new state and new parameter matrices from sda.enkf.R after the analysis step and translates new variables back to the model variables. Then, updates start.time, stop.time, and job.sh so that start.model.runs() does the correct runs with the new states. In write.restart.LINKAGES and write.restart.SIPNET, job.sh is updated by using write.configs.MODEL. outdir - output directory runid - run ID for ensemble member start.time - beginning of model run (in POSIX format) stop.time - end of model run (in POSIX format) settings - pecan.SDA.xml settings object new.state - matrix from analysis of updated state variables with PEcAn names (dimensions: site rows by state variables columns) new.params - In the future, this will allow us to update parameters based on states (same format as write.configs) inputs - model specific inputs from split.inputs.MODEL used to run the model from start.time to stop.time RENAME - [optional] Flag used in write.restart.LINKAGES.R for development. 9.3.6.3 split.inputs.MODEL.R This model specific function gives the correct met and/or other model inputs to settings\\(run\\)inputs. This function returns settings\\(run\\)inputs to an inputs argument in sda.enkf.R. But, the inputs will not need to change for all models and should return settings\\(run\\)inputs unchanged if that is the case. settings - pecan.SDA.xml settings object start.time - start time for model run (in POSIX format) stop.time - stop time for model run (in POSIX format) 9.3.6.4 sample.IC.MODEL.R This model specific function is optional. But, it can be used to create initial condition matrix (IC) with # state variables columns by # ensemble rows. This IC matrix is used for the initial runs in sda.enkf.R in the write.configs.MODEL function. ne - number of ensemble members state - matrix of state variables to get initial conditions from year - used to determine which year to sample initial conditions from 9.3.7 Analysis Options There are four options depending on whether process variance is TRUE/FALSE and whether or not there is data or not. If there is no data and process variance = FALSE, there is no analysis step. If there is no data and process variance = TRUE, process variance is added to the forecast. If there is data and process variance = TRUE, the generalized ensemble filter is implemented with MCMC. If there is data and process variance = FALSE, the Kalman filter is used and solved analytically. 9.3.8 The Generalized Ensemble Filter An ensemble filter is a sequential data assimilation algorithm with two procedures at every time step: a forecast followed by an analysis. The forecast ensembles arise from a model while the analysis makes an adjustment of the forecasts ensembles from the model towards the data. An ensemble Kalman filter is typically suggested for this type of analysis because of its computationally efficient analytical solution and its ability to update states based on an estimate of covariance structure. But, in some cases, the ensemble Kalman filter fails because of filter divergence. Filter divergence occurs when forecast variability is too small, which causes the analysis to favor the forecast and diverge from the data. Models often produce low forecast variability because there is little internal stochasticity. Our ensemble filter overcomes this problem in a Bayesian framework by including an estimation of model process variance. This methodology also maintains the benefits of the ensemble Kalman filter by updating the state vector based on the estimated covariance structure. This process begins after the model is spun up to equilibrium. The likelihood function uses the data vector \\(\\left(\\boldsymbol{y_{t}}\\right)\\) conditional on the estimated state vector \\(\\left(\\boldsymbol{x_{t}}\\right)\\) such that \\(\\boldsymbol{y}_{t}\\sim\\mathrm{multivariate\\:normal}(\\boldsymbol{x}_{t},\\boldsymbol{R}_{t})\\) where \\(\\boldsymbol{R}_{t}=\\boldsymbol{\\sigma}_{t}^{2}\\boldsymbol{I}\\) and \\(\\boldsymbol{\\sigma}_{t}^{2}\\) is a vector of data variances. To obtain an estimate of the state vector \\(\\left(\\boldsymbol{x}_{t}\\right)\\), we use a process model that incorporates a process covariance matrix \\(\\left(\\boldsymbol{Q}_{t}\\right)\\). This process covariance matrix differentiates our methods from past ensemble filters. Our process model contains the following equations \\(\\boldsymbol{x}_{t} \\sim \\mathrm{multivariate\\: normal}(\\boldsymbol{x}_{model_{t}},\\boldsymbol{Q}_{t})\\) \\(\\boldsymbol{x}_{model_{t}} \\sim \\mathrm{multivariate\\: normal}(\\boldsymbol{\\mu}_{forecast_{t}},\\boldsymbol{P}_{forecast_{t}})\\) where \\(\\boldsymbol{\\mu}_{forecast_{t}}\\) is a vector of means from the ensemble forecasts and \\(\\boldsymbol{P}_{forecast_{t}}\\) is a covariance matrix calculated from the ensemble forecasts. The prior for our process covariance matrix is \\(\\boldsymbol{Q}_{t}\\sim\\mathrm{Wishart}(\\boldsymbol{V}_{t},n_{t})\\) where \\(\\boldsymbol{V}_{t}\\) is a scale matrix and \\(n_{t}\\) is the degrees of freedom. The prior shape parameters are updated at each time step through moment matching such that \\(\\boldsymbol{V}_{t+1} = n_{t}\\bar{\\boldsymbol{Q}}_{t}\\) \\(n_{t+1} = \\frac{\\sum_{i=1}^{I}\\sum_{j=1}^{J}\\frac{v_{ijt}^{2}+v_{iit}v_{jjt}}{Var(\\boldsymbol{\\bar{Q}}_{t})}}{I\\times J}\\) where we calculate the mean of the process covariance matrix \\(\\left(\\bar{\\boldsymbol{Q}_{t}}\\right)\\) from the posterior samples at time t. Degrees of freedom for the Wishart are typically calculated element by element where \\(v_{ij}\\) are the elements of \\(\\boldsymbol{V}_{t}\\). \\(I\\) and \\(J\\) index rows and columns of \\(\\boldsymbol{V}\\). Here, we calculate a mean number of degrees of freedom for \\(t+1\\) by summing over all the elements of the scale matrix \\(\\left(\\boldsymbol{V}\\right)\\) and dividing by the count of those elements \\(\\left(I\\times J\\right)\\). We fit this model sequentially through time in the R computing environment using R package ‘rjags.’ 9.4 MultiSettings (TODO: Under construction…) 9.5 Benchmarking Benchmarking is the process of comparing model outputs against either experimental data or against other model outputs as a way to validate model performance. We have a suit of statistical comparisons that provide benchmarking scores as well as visual comparisons that help in diagnosing data-model and/or model-model differences. 9.5.1 Data Preparation All data that you want to compare with model runs must be registered in the database. This is currently a step that must be done by hand either from the command line or through the online BETY interface. The data must have three records: An input record (Instructions here) A database file record (Instructions here) A format record (Instructions here) 9.5.2 Model Runs Model runs can be setup and executed - Using the PEcAn web interface online or with a VM (see setup) - By hand using the pecan.xml 9.5.3 The Benchmarking Shiny App The entire benchmarking process can be done through the Benchmarking R Shiny app. When the model run has completed, navigate to the workflow visualization Shiny app. Load model data Select the workflow and run id Make sure that your model output is loading properly (i.e. you can see plots of your data) Load benchmarking data Again make sure that you can see the uploaded data plotted alongside the model output. In the future there will be more tools for double checking that your uploaded data is appropriate for benchmarking, but for now you may need to do the sanity checks by hand. 9.5.3.1 Create a reference run record Navigate to the Benchmarking tab The first step is to register the new model run as a reference run in the database. Benchmarking cannot be done before this step is completed. When the reference run record has been created, additional menus for benchmarking will appear. 9.5.3.2 Setup Benchmarks and metrics From the menus select The variables in the uploaded data that you wish to compare with model output. The numerical metrics you would like to use in your comparison. Additional comparison plots that you would like to see. Note: All these selections populate the benchmarking section of the pecan.BENCH.xml which is then saved in the same location as the original run output. This xml is purely for reference. 9.5.3.2.1 Benchmarking Output All benchmarking results are stored in the benchmarking directory which is created in the same folder as the original model run. The benchmaking directory contains subdirectories for each of the datasets compared with the model output. The names of these directories are the same as the corresponding data set’s input id in BETY. Each input directory contains benchmarking.output.Rdata, an Rdata file contianing all the results of the benchmarking workflow. load(benchmarking.output.Rdata) loads a list called result.out which contains the following: bench.results: a data frame of all numeric benchmarking scores format: a data frame that can be used to see how the input data was transformed to make it comparable to the model output. This involves converting from the original variable names and units to the internal pecan standard. aligned.dat: a data frame of the final aligned model and input values. All plots are saved as pdf files with names with “benchmark_plot-type_variable_input-id.pdf” To view interactive results, naviage to the Benchmarking Plots tab in the shiny app. 9.5.4 Benchmarking in pecan.xml Before reading this section, it is recommended that you familiarize yourself with basics of the pecan.xml file. The pecan.xml has an optional benchmarking section. Below are all the tags in the benchmarking section explained. Many of these field are filled in automatically during the benchmarking process when using the benchmarking shiny app. The only time one should edit the benchmarking section by hand is for performing clone runs. See clone run documentation. &lt;benchmarking&gt; settings: ensemble_id: the id of the ensemble that you will be using - the settings from this ensemble will be saved in a reference run record and then ensemble_id will be replaced with reference_run_id new_run: TRUE = create new run, FALSE = use existing run (required, default FALSE) It is possible to look at more than one benchmark with a particular run. The specific settings related to each benchmark are in a sub section called benchmark input_id: the id of the benchmarking data (required) variable_id: the id of the variable of interest within the data. If you leave this blank, all variables that are shared between the input and model output will be used. metric_id: the id(s) of the metric(s) to be calculated. If you leave this blank, all metrics will be used. Example: In this example, - we are using a pre-existing run from ensemble_id = 1000010983 (new_run = FALSE) - the output will be compared to data from input_id = 1000013743, specifically two variables of interest: variable_id = 411, variable_id = 18 - for variable_id = 411 we will perform only one metric of comparison metric_id = 1000000001 - for for variable_id = 18 we will perform two metrics of comparison metric_id = 1000000001, metric_id = 1000000002 &lt;benchmarking&gt; &lt;ensemble_id&gt;1000010983&lt;/ensemble_id&gt; &lt;new_run&gt;FALSE&lt;/new_run&gt; &lt;benchmark&gt; &lt;input_id&gt;1000013743&lt;/input_id&gt; &lt;variable_id&gt;411&lt;/variable_id&gt; &lt;site_id&gt;853&lt;/site_id&gt; &lt;metrics&gt; &lt;metric_id&gt;1000000001&lt;/metric_id&gt; &lt;/metrics&gt; &lt;/benchmark&gt; &lt;benchmark&gt; &lt;input_id&gt;1000013743&lt;/input_id&gt; &lt;variable_id&gt;18&lt;/variable_id&gt; &lt;site_id&gt;853&lt;/site_id&gt; &lt;metrics&gt; &lt;metric_id&gt;1000000001&lt;/metric_id&gt; &lt;metric_id&gt;1000000002&lt;/metric_id&gt; &lt;/metrics&gt; &lt;/benchmark&gt; &lt;/benchmarking&gt; "],
["remote-execution-with-pecan.html", "10 Remote execution with PEcAn 10.1 Basics of SSH 10.2 SSH authentication – password vs. SSH key 10.3 Basic remote execute functions 10.4 Remote model execution with PEcAn 10.5 Running PEcAn code for modules remotely", " 10 Remote execution with PEcAn Remote execution allows the user to leverage the power and storage of high performance computing clusters, AWS instances, or specially configured virtual machines, but without leaving their local working environment. PEcAn uses remote execution primarily to run ecosystem models. The infrastructure for remote execution lives in the PEcAn.remote package (base/remote in the PEcAn repository). This section describes the following: Basics of command line SSH SSH authentication with keys and passwords Basics of SSH tunnels, and how they are used in PEcAn Basic remote exectuion R functions in PEcAn.remote Remote model execution configuration in the pecan.xml and config.php Additional information about preparing remote servers for execution 10.1 Basics of SSH All of the PEcAn remote infrastructure depends on the system ssh utility, so it’s important to make sure this works before attempting the advanced remote execution functionality in PEcAn. To connect to a remote server interactively, the command is simply: ssh &lt;username&gt;@&lt;hostname&gt; For instance, my connection to the BU shared computing cluster looks like: ssh ashiklom@geo.bu.edu …which will prompt me for my BU password, and, if successful, will drop me into a login shell on the remote machine. Alternatively to the login shell, ssh can be used to execute arbitrary code, whose output will be returned exactly as it would if you ran the command locally. For example, the following: ssh ashiklom@geo.bu.edu pwd …will run the pwd command, and return the path to my home directory on the BU SCC. The more advanced example below will run some simple R code on the BU SCC and return the output as if it was run locally. ssh ashiklom@geo.bu.edu Rscript -e &quot;seq(1, 10)&quot; 10.2 SSH authentication – password vs. SSH key Because this server uses passwords for authentication, this command will then prompt me for my password. An alternative to password authentication is using SSH keys. Under this system, the host machine (say, your laptop, or the PEcAn VM) has to generate a public and private key pair (using the ssh-keygen command). The private key (by default, a file in ~/.ssh/id_rsa) lives on the host machine, and should never be shared with anyone. The public key will be distributed to any remote machines to which you want the host to be able to connect. On each remote machine, the public key should be added to a list of authorized keys located in the ~/.ssh/authorized_keys file (on the remote machine). The authorized keys list indicates which machines (technically, which keys – a single machine, and even a single user, can have many keys) are allowed to connect to it. This is the system used by all of the PEcAn servers (pecan1, pecan2, test-pecan). 10.2.1 SSH tunneling SSH authentication can be more advanced than indicated above, especially on systems that require dual authentication. Even simple password-protection can be tricky in scripts, since (by design) it is fairly difficult to get SSH to accept a password from anything other than the raw keyboard input (i.e. SSH doesn’t let you pass passwords as input or arguments, because this exposes your password as plain text). A convenient and secure way to follow SSH security protocol, but prevent having to go through the full authentication process every time, is to use SSH tunnels (or “sockets”, which are effectively synonymous). Essentially, an SSH socket is a read- and write-protectected file that contains all of the information about an SSH connection. To create an SSH tunnel, use a command like the following: ssh -n -N -f -o ControlMaster=yes -S /path/to/socket/file &lt;username&gt;@&lt;hostname&gt; If appropriate, this will prompt you for your password (if using password authentication), and then will drop you back to the command line (thanks to the -N flag, which runs SSH without executing a command, the -f flag, which pushes SSH into the background, and the -n flag, which prevents ssh from reading any input). It will also create the file /path/to/socket/file. To use this socket with another command, use the -S /path/to/file flag, pointing to the same tunnel file you just created. ssh -S /path/to/socket/file &lt;hostname&gt; &lt;optional command&gt; This will let you access the server without any sort of authentication step. As before, if &lt;optional command&gt; is blank, you will be dropped into an interactive shell on the remote, or if it’s a command, that command will be executed and the output returned. To close a socket, use the following: ssh -S /path/to/socket/file &lt;hostname&gt; -O exit This will delete the socket file and close the connection. Alternatively, a scorched earth approach to closing the SSH tunnel if you don’t remember where you put the socket file is something like the following: pgrep ssh # See which processes will be killed pkill ssh # Kill those processes …which will kill all user processes called ssh. To automatically create tunnels following a specific pattern, you can add the following to your ~/.ssh/config Host &lt;hostname goes here&gt; ControlMaster auto ControlPath /tmp/%r@%h:%p For more information, see man ssh. 10.2.2 SSH tunnels and PEcAn Many of the PEcAn.remote functions assume that a tunnel is already open. If working from the web interface, the tunnel will be opened for you by some under-the-hood PHP and Bash code, but if debugging or working locally, you will have to create the tunnel yourself. The best way to do this is to create the tunnel first, outside of R, as described above. (In the following examples, I’ll use my username ashiklom connecting to the test-pecan server with a socket stored in /tmp/testpecan. To follow along, replace these with your own username and designated server, respectively). ssh -nNf -o ControlMaster=yes -S /tmp/testpecan ashiklom@test-pecan.bu.edu Then, in R, create a host object, which is just a list containing the elements name (hostname) and tunnel (path to tunnel file). my_host &lt;- list(name = &quot;test-pecan.bu.edu&quot;, tunnel = &quot;/tmp/testpecan&quot;) This host object can then be used in any of the remote execution functions. 10.3 Basic remote execute functions The PEcAn.remote::remote.execute.cmd function runs a system command on a remote server (or on the local server, if host$name == &quot;localhost&quot;). x &lt;- PEcAn.remote::remote.execute.cmd(host = my_host, cmd = &quot;echo&quot;, args = &quot;Hello world&quot;) x Note that remote.execute.cmd is similar to base R’s system2, in that the base command (in this case, echo) is passed separately from its arguments (&quot;Hello world&quot;). Note also that the output of the remote command is returned as a character. For R code, there is a special wrapper around remote.execute.cmd – PEcAn.remote::remote.execute.R, which runs R code (passed as a string) on a remote and returns the output. code &lt;- &quot; x &lt;- 2:4 y &lt;- 3:1 x ^ y &quot; out &lt;- PEcAn.remote::remote.execute.R(code = code, host = my_host) For additional functions related to remote file operations and other stuff, see the PEcAn.remote package documentation. 10.4 Remote model execution with PEcAn The workhorse of remote model execution is the PEcAn.remote::start.model.runs function, which distributes execution of each run in a list of runs (e.g. multiple runs in an ensemble) to the local machine or a remote based on the configuration in the PEcAn settings. Broadly, there are three major types of model execution: Serialized (PEcAn.remote::start_serial) – This runs models one at a time, directly on the local machine or remote (i.e. same as calling the executables one at a time for each run). Via a queue system, (PEcAn.remote::start_qsub) – This uses a queue management system, such as SGE (e.g. qsub, qstat) found on the BU SCC machines, to submit jobs. For computationally intensive tasks, this is the recommended way to go. Via a model launcher script (PEcAn.remote::setup_modellauncher) – This is a highly customizable approach where task submission is controlled by a user-provided script (launcher.sh). 10.4.1 XML configuration The relevant section of the PEcAn XML file is the &lt;host&gt; block. Here is a minimal example from one of my recent runs: &lt;host&gt; &lt;name&gt;geo.bu.edu&lt;/name&gt; &lt;user&gt;ashiklom&lt;/user&gt; &lt;tunnel&gt;/home/carya/output//PEcAn_99000000008/tunnel/tunnel&lt;/tunnel&gt; &lt;/host&gt; Breaking this down: name – The hostname of the machine where the runs will be performed. Set it to localhost to run on the local machine. user – Your username on the remote machine (note that this may be different from the username on your local machine). tunnel – This is the tunnel file for the connection used by all remote execution files. The tunnel is created automatically by the web interface, but must be created by the user for command line execution. This configuration will run in serialized mode. To use qsub, the configuration is slightly more involved: &lt;host&gt; &lt;name&gt;geo.bu.edu&lt;/name&gt; &lt;user&gt;ashiklom&lt;/user&gt; &lt;qsub&gt;qsub -V -N @NAME@ -o @STDOUT@ -e @STDERR@ -S /bin/bash&lt;/qsub&gt; &lt;qsub.jobid&gt;Your job ([0-9]+) .*&lt;/qsub.jobid&gt; &lt;qstat&gt;qstat -j @JOBID@ || echo DONE&lt;/qstat&gt; &lt;tunnel&gt;/home/carya/output//PEcAn_99000000008/tunnel/tunnel&lt;/tunnel&gt; &lt;/host&gt; The additional fields are as follows: qsub – The command used to submit jobs to the queue system. Despite the name, this can be any command used for any queue system. The following variables are available to be set here: @NAME@ – Job name to display @STDOUT@ – File to which stdout will be redirected @STDERR@ – File to which stderr will be redirected qsub.jobid – A regular expression, from which the job ID will be determined. This string will be parsed by R as jobid &lt;- gsub(qsub.jobid, &quot;\\\\1&quot;, output) – note that the first pattern match is taken as the job ID. qstat – The command used to check the status of a job. Internally, PEcAn will look for the DONE string at the end, so a structure like &lt;some command indicating if any jobs are still running&gt; || echo DONE is required. The @JOBID@ here is the job ID determined from the qsub.jobid parsing. Documentation for using the model launcher is currently unavailable. 10.4.2 Configuration for PEcAn web interface The config.php has a few variables that will control where the web interface can run jobs, and how to run those jobs. These variables are $hostlist, $qsublist, $qsuboptions, and $SSHtunnel. In the near future $hostlist, $qsublist, $qsuboptions will be combined into a single list. $SSHtunnel : points to the script that creates an SSH tunnel. The script is located in the web folder and the default value of dirname(__FILE__) . DIRECTORY_SEPARATOR . &quot;sshtunnel.sh&quot;; most likely will work. $hostlist : is an array with by default a single value, only allowing jobs to run on the local server. Adding any other servers to this list will show them in the pull down menu when selecting machines, and will trigger the web page to be show to ask for a username and password for the remote execution (make sure to use HTTPS setup when asking for password to prevent it from being send in the clear). $qsublist : is an array of hosts that require qsub to be used when running the models. This list can include $fqdn to indicate that jobs on the local machine should use qsub to run the models. $qsuboptions : is an array that lists options for each machine. Currently it support the following options (see also PEcAn-Configuration) array(&quot;geo.bu.edu&quot; =&gt; array(&quot;qsub&quot; =&gt; &quot;qsub -V -N @NAME@ -o @STDOUT@ -e @STDERR@ -S /bin/bash&quot;, &quot;jobid&quot; =&gt; &quot;Your job ([0-9]+) .*&quot;, &quot;qstat&quot; =&gt; &quot;qstat -j @JOBID@ || echo DONE&quot;, &quot;job.sh&quot; =&gt; &quot;module load udunits R/R-3.0.0_gnu-4.4.6&quot;, &quot;models&quot; =&gt; array(&quot;ED2&quot; =&gt; &quot;module load hdf5&quot;)) In this list qsub is the actual command line for qsub, jobid is the text returned from qsub, qstat is the command to check to see if the job is finished. job.sh and the value in models are additional entries to add to the job.sh file generated to run the model. This can be used to make sure modules are loaded on the HPC cluster before running the actual model. 10.5 Running PEcAn code for modules remotely You can compile and install the model specific code pieces of PEcAn on the cluster easily without having to install the full code base of PEcAn (and all OS dependencies). All of the code pieces depend on PEcAn.utils to install this you can run the following on the cluster: devtools::install_github(&quot;pecanproject/pecan&quot;, subdir = &#39;utils&#39;) Next we need to install the model specific pieces, this is done almost the same: devtools::install_github(&quot;pecanproject/pecan&quot;, subdir = &#39;models/ed&#39;) This should install dependencies required. Following are some notes on how to install the model specifics on different HPC clusters. 10.5.1 geo.bu.edu Following modules need to be loaded: module load hdf5 udunits R/R-3.0.0_gnu-4.4.6 Next the following packages need to be installed, otherwise it will fall back on the older versions install site-library install.packages(c(&#39;udunits2&#39;, &#39;lubridate&#39;), configure.args=c(udunits2=&#39;--with-udunits2-lib=/project/earth/packages/udunits-2.1.24/lib --with-udunits2-include=/project/earth/packages/udunits-2.1.24/include&#39;), repos=&#39;http://cran.us.r-project.org&#39;) Finally to install support for both ED and SIPNET: devtools::install_github(&quot;pecanproject/pecan&quot;, subdir = &#39;utils&#39;) devtools::install_github(&quot;pecanproject/pecan&quot;, subdir = &#39;models/sipnet&#39;) devtools::install_github(&quot;pecanproject/pecan&quot;, subdir = &#39;models/ed&#39;) "],
["introduction-1.html", "11 Introduction", " 11 Introduction For those wanting to take full advantage of PEcAn and its tools, this section will provide you with the necessary information to do so. In this section we hope to answer the question: How can I use PEcAn to its fullest? PEcAn workflow (web/workflow.R) How the workflow works How each module is called How to do outside of web interface Link to “folder structure” section below for detailed descriptions Adding to PEcAn Case studies Adding a model Adding new species, PFTs, and traits from a new site Add a site Add some species Add PFT Add trait data Adding a benchmark Adding a met driver Reference (How to change tables) Models Species PFTs Traits Inputs DB files Variables Formats (Link each section to relevant Bety tables) Modules – additional tools RTM Photosynthesis Allometry benchmark::load_data "],
["pecan-workflow-webworkflow-r.html", "12 PEcAn workflow (web/workflow.R) 12.1 Read Settings 12.2 Input Conversions 12.3 Meteorological Data 12.4 Traits 12.5 Meta Analysis 12.6 Model Configuration 12.7 Run Execution 12.8 Post Run Analysis 12.9 Advanced Analysis", " 12 PEcAn workflow (web/workflow.R) How the workflow works How each module is called How to do outside of web interface Link to “folder structure” section below for detailed descriptions 12.1 Read Settings 12.2 Input Conversions 12.2.1 Input Data Models require input data as drivers, parameters, and boundary conditions. In order to make a variety of data sources that have unique formats compatible with models, conversion scripts are written to convert them into a PEcAn standard format. That format is a netcdf file with variables names and specified to our standard variable table. Within the PEcAn repository, code pertaining to input conversion is in the MODULES directory under the data.atmosphere and data.land directories. 12.2.2 Initial Conditions Initial Conditions 12.3 Meteorological Data To convert meterological data into the PEcAn Standard and then into model formats we follow four main steps: Downloading raw data Currently supported products Example Code Converting raw data into a CF standard Example Code Downscaling and gapfilling Example Code Coverting to Model Specific format Example Code Common Questions regarding Met Data: How do I add my Meterological data product to PEcAn? How do I use PEcAn to convert Met data outide the workflow? The main script that handles Met Processing, is met.process. It acts as a wrapper function that calls individual modules to facilitate the processing of meteorological data from it’s original form to a pecan standard, and then from that standard to model specific formats. It also handles recording these processes in the BETY database. Downloading raw data Available Meteorological Drivers Example Code to download Ameriflux data Converting raw data into a CF standard (if needed) Example Code to convert from raw csv to CF standard Downscaling and gapfilling(if needed) Example Code to gapfill Coverting to Model Specific format Example Code to convert Standard into Sipnet format 12.3.1 Downloading Raw data (Description of Process) Given the information passed from the pecan.xml met.process will call the download.raw.met.module to facilitate the execution of the necessary functions to download raw data. &lt;met&gt; &lt;source&gt;AmerifluxLBL&lt;/source&gt; &lt;output&gt;SIPNET&lt;/output&gt; &lt;username&gt;pecan&lt;/username&gt; &lt;/met&gt; 12.4 Traits 12.5 Meta Analysis 12.6 Model Configuration 12.7 Run Execution 12.8 Post Run Analysis 12.9 Advanced Analysis "],
["adding-to-pecan.html", "13 Adding to PEcAn 13.1 Case studies", " 13 Adding to PEcAn Case studies Adding a model Adding new species, PFTs, and traits from a new site Add a site Add some species Add PFT Add trait data Adding a benchmark Adding a met driver Reference (How to change tables) Models Species PFTs Traits Inputs DB files Variables Formats (Link each section to relevant Bety tables) 13.1 Case studies Adding a model Adding new species, PFTs, and traits from a new site Add a site Add some species Add PFT Add trait data Adding a benchmark Adding a met driver 13.1.1 Adding An Ecosystem Model Adding a model to PEcAn involves two activities: Updating the PEcAn database to register the model Writing the interface modules between the model and PEcAn Note that coupling a model to PEcAn should not require any changes to the model code itself. A key aspect of our design philosophy is that we want it to be easy to add models to the system and we want to using the working version of the code that is used by all other model users, not a special branch (which would rapidly end up out-of-date). 13.1.1.1 PEcAn Database To run a model within PEcAn requires that the PEcAn database know about the model – this includes a MODEL_TYPE designation, the types of inputs the model requires, the location of the model executable, and the plant functional types used by the model. The instructions below assume that you will be specifying this information using the BETYdb web-based interface. This can be done either on your local VM (localhost:3280/bety or localhost:6480/bety) or on a server installation of BETYdb, though in either case we’d encourage you to set up your PEcAn instance to support database syncs so that these changes can be shared and backed-up across the PEcAn network. The figure below summarizes the relevant database tables that need to be updated to add a new model and the primary variables that define each table. 13.1.1.2 Define MODEL_TYPE The first step to adding a model is to create a new MODEL_TYPE, which defines the abstract model class which we will then use to specify input requirements, define plant functional types, and keep track of different model versions. A MODEL_TYPE is created by selecting Runs &gt; Model Type and then clicking on New Model Type. The MODEL_TYPE name should be identical to the MODEL package name (see Interface Module below) and is case sensitive. 13.1.1.3 MACHINE The PEcAn design acknowledges that the same model executables and input files may exist on multiple computers. Therefore, we need to define the machine that that we are using. If you are running on the VM then the local machine is already defined as pecan32 or pecan64 for the 32-bit and 64-bit versions respectively. Otherwise, you will need to select Runs &gt; Machines, click New Machine, and enter the URL of your server (e.g. pecan2.bu.edu). 13.1.1.4 MODEL Next we are going to tell PEcAn where the model executable is. Select Runs &gt; Files, and click ADD. Use the pull down menu to specify the machine you just defined above and fill in the path and name for the executable. For example, if SIPNET is installed at /usr/local/bin/sipnet then the path is /usr/local/bin/ and the file (executable) is sipnet. Now we will create the model record and associate this with the File we just registered. The first time you do this select Runs &gt; Models and click New Model. Specify a descriptive name of the model (which doesn’t have to be the same as MODEL_TYPE), select the MODEL_TYPE from the pull down, and provide a revision identifier for the model (e.g. v3.2.1). Once the record is created select it from the Models table and click EDIT RECORD. Click on “View Related Files” and when the search window appears search for the model executable you just added (if you are unsure which file to choose you can go back to the Files menu and look up the unique ID number). You can then associate this Model record with the File by clicking on the +/- symbol. By contrast, clicking on the name itself will take you to the File record. In the future, if you set up the SAME MODEL VERSION on a different computer you can add that Machine and File to PEcAn and then associate this new File with this same Model record. A single version of a model should only be entered into PEcAn once. If a new version of the model is developed that is derived from the current version you should add this as a new Model record but with the same MODEL_TYPE as the original. Furthermore, you should set the previous version of the model as Parent of this new version. 13.1.1.5 FORMATS The PEcAn database keep track of all the input files passed to models, as well as any data used in model validation or data assimilation. Before we start to register these files with PEcAn we need to define the format these files will be in. To create a new format see Formats Documentation. 13.1.1.5.1 MODEL_TYPE -&gt; Formats For each of the input formats you specify for your model, you will need to edit your MODEL_TYPE record to add an association between the format and the MODEL_TYPE. Go to Runs &gt; Model Type, select your record and click on the Edit button. Next, click on “Edit Associated Formats” and choose the Format you just defined from the pull down menu. If the Input box is checked then all matching Input records will be displayed in the PEcAn site run selection page when you are defining a model run. In other words, the set of model inputs available through the PEcAn web interface is model-specific and dynamically generated from the associations between MODEL_TYPEs and Formats. If you also check the Required box, then the Input will be treated as required and PEcAn will not run the model if that input is not available. Furthermore, on the site selection webpage, PEcAn will filter the available sites and only display pins on the Google Map for sites that have a full set of required inputs (or where those inputs could be generated using PEcAn’s workflows). Similarly, to make a site appear on the Google Map, all you need to do is specify Inputs, as described in the next section, and the point should automatically appear on the map. 13.1.1.6 INPUTS After a file Format has been created then input files can be registered with the database. Creating Inputs can be found under How to insert new Input data. 13.1.1.7 PFTS (Plant Functional Types) Since many of the PEcAn tools are designed to keep track of parameter uncertainties and assimilate data into models, to use PEcAn with a model it is important to define Plant Functional Types for the sites or regions that you will be running the model. PFTs are MODEL_TYPE specific, so when you create a new PFT entry (Data &gt; PFTs; New PFT) you will want to choose your MODEL_TYPE from the pull down and then give the PFT a descriptive name (e.g. temperate deciduous). 13.1.1.7.1 Species Within PEcAn there are no predefined PFTs and user can create new PFTs very easily at whatever taxonomic level is most appropriate, from PFTs for individual species up to one PFT for all plants globally. To allow PEcAn to query its trait database for information about a PFT, you will want to associate species with the PFT record by choosing Edit and then “View Related Species”. Species can be searched for by common or scientific name and then added to a PFT using the +/- button. 13.1.1.7.2 Cultivars You can also define PFTs whose members are cultivars instead of species. This is designed for analyses where you want to want to perform meta-analysis on within-species comparisons (e.g. cultivar evaluation in an agricultural model) but may be useful for other cases when you want to specify different priors for some member of a species. You cannot associate both species and cultivars with the same PFT, but the cultivars in a cultivar PFT may come from different species, potentially including all known cultivars from some of the species, if you wish to and have thought about how to interpret the results. It is not yet possible to add a cultivar PFT through the BETYdb web interface. See this GithHub comment for an example of how to define one manually in PostgreSQL. 13.1.1.8 PRIORS In addition to adding species, a PFT is defined in PEcAn by the list of variables associated with the PFT. PEcAn takes a fundamentally Bayesian approach to representing model parameters, so variables are not entered as fixed constants but as Prior probability distributions (see below). Once Priors are defined for each model variable then you Edit the PFT and use “View Related Priors” to search for and add Prior distributions for each model parameter. It is important to note that the priors are defined for the variable name and units as specified in the Variables table. If the variable name or units is different within the model it is the responsibility of write.configs.MODEL function to handle name and unit conversions (see Interface Modules below). This can also include common but nonlinear transformations, such as converting SLA to LMA or changing the reference temperature for respiration rates. There are a wide variety of priors already defined in the PEcAn database that often range from very diffuse and generic to very informative priors for specific PFTs. If the current set of Priors for a variable are inadequate, or if a prior needs to be specified for a new variable, this can be done under Data &gt; Priors then “New Prior”. After using the pull-down menu to select the Variable you want to generate a prior for, the prior is defined by choosing a probability distribution and specifying values for that distribution’s parameters. These are labeled Parameter a &amp; b but their exact meaning depends upon the distribution chosen. For example, for the Normal distribution a and b are the mean and standard deviation while for the Uniform they are the minimum and maximum. All parameters are defined based on their standard parameterization in the R language. If the prior is based on observed data (independent of data in the PEcAn database) then you can also specify the prior sample size, N. The Phylogeny variable allows one to specify what taxonomic grouping the prior is defined for, at it is important to note that this is just for reference and doesn’t have to be specified in any standard way nor does it have to be monophyletic (i.e. it can be a functional grouping). Finally, the Citation is a required variable that provides a reference for how the prior was defined. That said, there are a number of unpublished Citations in current use that simply state the expert opinion of an individual. Additional information on adding PFTs, Species, and Priors can be found under [[Choosing PFTs]] 13.1.1.9 Interface Modules 13.1.1.9.1 Setting up the module directory (required) PEcAn assumes that the interface modules are available as an R package in the models directory named after the model in question. The simplest way to get started on that R package is to make a copy the template directory in the pecan/models folder and re-name it to the name of your model. In the code, filenames, and examples below you will want to substitute the word MODEL for the name of your model (note: R is case-sensitive). If you do not want to write the interface modules in R then it is fairly simple to set up the R functions describe below to just call the script you want to run using R’s system command. Scripts that are not R functions should be placed in the inst folder and R can look up the location of these files using the function system.file which takes as arguments the local path of the file within the package folder and the name of the package (typically PEcAn.MODEL). For example ## Example met conversion wrapper function met2model.MODEL &lt;- function(in.path, in.prefix, outfolder, start_date, end_date){ myMetScript &lt;- system.file(&quot;inst/met2model.MODEL.sh&quot;, &quot;PEcAn.MODEL&quot;) system(paste(myMetScript, file.path(in.path, in.prefix), outfolder, start_date, end_date)) } would execute the following at the Linux command line inst/met2model.MODEL.sh in.path/in.prefix outfolder start_date end_date ` 13.1.1.9.2 DESCRIPTION Within the module folder open the DESCRIPTION file and change the package name to PEcAn.MODEL. Fill out other fields such as Title, Author, Maintainer, and Date. 13.1.1.9.3 NAMESPACE Open the NAMESPACE file and change all instances of MODEL to the name of your model. If you are not going to implement one of the optional modules (described below) at this time then you will want to comment those out using the pound sign #. For a complete description of R NAMESPACE files see here. If you create additional functions in your R package that you want to be used make sure you include them in the NAMESPACE as well (internal functions don’t need to be declared) 13.1.1.9.4 Building the package Once the package is defined you will then need to add it to the PEcAn build scripts. From the root of the pecan directory, go into the scripts folder and open the file build.sh. Within the section of code that includes PACKAGES= add model/MODEL to the list of packages to compile. If, in writing your module, you add any other R packages to the system you will want to make sure those are listed in the DESCRIPTION and in the script scripts/install.dependencies.R. Next, from the root pecan directory open all/DESCRIPTION and add your model package to the Suggests: list. At any point, if you want to check if PEcAn can build your MODEL package successfully, just go to the linux command prompt and run scripts/build.sh. You will need to do this before the system can use these packages. 13.1.1.9.5 write.config.MODEL (required) This module performs two primary tasks. The first is to take the list of parameter values and model input files that it receives as inputs and write those out in whatever format(s) the MODEL reads (e.g. a settings file). The second is to write out a shell script, jobs.sh, which, when run, will start your model run and convert its output to the PEcAn standard (netCDF with metadata currently equivalent to the MsTMIP standard). Within the MODEL directory take a close look at inst/template.job and the example write.config.MODEL to see an example of how this is done. It is important that this script writes or moves outputs to the correct location so that PEcAn can find them. The example function also shows an example of writing a model-specific settings/config file, also by using a template. You are encouraged to read the section above on defining PFTs before writing write.config.MODEL so that you understand what model parameters PEcAn will be passing you, how they will be named, and what units they will be in. Also note that the (optional) PEcAn input/driver processing scripts are called by separate workflows, so the paths to any required inputs (e.g. meteorology) will already be in the model-specific format by the time write.config.MODEL receives that info. 13.1.1.9.6 Output Conversions The module model2netcdf.MODEL converts model output into the PEcAn standard (netCDF with metadata currently equivalent to the MsTMIP standard). This function was previously required, but now that the conversion is called within jobs.sh it may be easier for you to convert outputs using other approaches (or to just directly write outputs in the standard). Whether you implement this function or convert outputs some other way, please note that PEcAn expects all outputs to be broken up into ANNUAL files with the year number as the file name (i.e. YEAR.nc), though these files may contain any number of scalars, vectors, matrices, or arrays of model outputs, such as time-series of each output variable at the model’s native timestep. Note: PEcAn reads all variable names from the files themselves so it is possible to add additional variables that are not part of the MsTMIP standard. Similarly, there are no REQUIRED output variables, though time is highly encouraged. We are shortly going establish a canonical list of PEcAn variables so that if users add additional output variables they become part of the standard. We don’t want two different models to call the same output with two different names or different units as this would prohibit the multi-model syntheses and comparisons that PEcAn is designed to facilitate. 13.1.1.9.7 met2model.MODEL met2model.MODEL(in.path, in.prefix, outfolder, start_date, end_date) Converts meteorology input files from the PEcAn standard (netCDF, CF metadata) to the format required by the model. This file is optional if you want to load all of your met files into the Inputs table as described in How to insert new Input data, which is often the easiest way to get up and running quickly. However, this function is required if you want to benefit from PEcAn’s meteorology workflows and model run cloning. You’ll want to take a close look at [Adding-an-Input-Converter] to see the exact variable names and units that PEcAn will be providing. Also note that PEcAn splits all meteorology up into ANNUAL files, with the year number explicitly included in the file name, and thus what PEcAn will actually be providing is in.path, the input path to the folder where multiple met files may stored, and in.prefix, the start of the filename that precedes the year (i.e. an individual file will be named &lt;in.prefix&gt;.YEAR.nc). It is valid for in.prefix to be blank. The additional REQUIRED arguments to met2model.MODEL are outfolder, the output folder where PEcAn wants you to write your meteorology, and start_date and end_date, the time range the user has asked the meteorology to be processed for. 13.1.1.9.8 Commit changes Once the MODEL modules are written, you should follow the Using-Git instructions on how to commit your changes to your local git repository, verify that PEcAn compiles using scripts/build.sh, push these changes to Github, and submit a pull request so that your model module is added to the PEcAn system. It is important to note that while we encourage users to make their models open, adding the PEcAn interface module to the Github repository in no way requires that the model code itself be made public. It does, however, allow anyone who already has a copy of the model code to use PEcAn so we strongly encourage that any new model modules be committed to Github. "],
["NewFormat.html", "14 Formats records in BETY", " 14 Formats records in BETY The PEcAn database keeps track of all the input files passed to models, as well as any data used in model validation or data assimilation. Before we start to register these files with PEcAn we need to define the format these files will be in. The main goal is to take all the meta-data we have about a data file and create a record of it that pecan can use as a guide when parsing the data file. This information is stored in a Format record in the bety database. Make sure to read through the current Formats before deciding to make a new one. "],
["creating-a-new-format-record-in-bety.html", "15 Creating a new Format record in BETY 15.1 Formats -&gt; Variables 15.2 Retrieving Format Information 15.3 Input records in BETY 15.4 Create a database file record for the input data 15.5 Creating a new Input record in BETY", " 15 Creating a new Format record in BETY If the Format you are looking for is not available, you will need to create a new record. Before entering information into the database, you need to be able to answer the following questions about your data: What is the file MIME type? We have a suit of functions for loading in data in open formats such as CSV, txt, netCDF, etc. PEcAn has partnered with the NCSA BrownDog project to create a service that can read and convert as many data formats as possible. If your file type is less common or a proprietary type, you can use the BrownDog DAP to convert it to a format that can be used with PEcAn. If BrownDog cannot convert your data, you will need to contact us about writing a data specific load function. What variables does the file contain? What are the variables named? What are the variable units? How do the variable names and units in the data map to PEcAn variables in the BETY database? See below for an example. It is most likely that you will NOT need to add variables to BETY. However, identifying the appropriate variables matches in the database may require some work. We are always available to help answer your questions. Is there a timestamp on the data? What are the units of time? Here is an example using a fake dataset: example_data This data started out as an excel document, but was saved as a CSV file. To create a Formats record for this data, in the web interface of BETY, select Runs &gt; Formats and click New Format. You will need to fill out the following fields: MIME type: File type (you can search for other formats in the text field) Name: The name of your format (this can be whatever you want) Header: Boolean that denotes whether or not your data contains a header as the first line of the data. (1 = TRUE, 0 = FALSE) Skip: The number of lines above the data that should be skipped. For example, metadata that should not be included when reading in the data or blank spaces. Notes: Any additional information about the data such as sources and citations. Here is the Formats record for the example data: When you have finished this section, hit Create. The final record will be displayed on the screen. 15.1 Formats -&gt; Variables After a Format entry has been created, you are encouraged to edit the entry to add relationships between the file’s variables and the Variables table in PEcAn. Not only do these relationships provide meta-data describing the file format, but they also allow PEcAn to search and (for some MIME types) read files. To enter this data, select Edit Record and on the edit screen select View Related Variable. Here is the record for the example data after adding related variables: format_record_2 15.1.1 Name and Unit For each variable in the file you will want at a minimum to specify the NAME of the variable within your file and match that to the equivalent Variable in the pulldown. Make sure to search for your variables under Data &gt; Variables before suggesting that we create a new variable record. This may not always be a straightforward process. For example bety contains a record for Net Primary Productivity: var_record This record does not have the same variable name or the same units as NPP in the example data. You may have to do some reading to confirm that they are the same variable. In this case - Both the data and the record are for Net Primary Productivity (the notes section provides additional resources for interpreting the variable.) - The units of the data can be converted to those of the vairiable record (this can be checked by running udunits2::ud.are.convertible(&quot;g C m-2 yr-1&quot;, &quot;Mg C ha-1 yr-1&quot;)) Differences between the data and the variable record can be accounted for in the data Formats record. Under Variable, select the variable as it is recorded in bety. Under Name, write the name the variable has in your data file. Under Unit, write the units the variable has in your data file. NOTE: All units must be written in a udunits compliant format. To check that your units can be read by udunits, in R, load the udunits2 package and run udunits2::is.parseable(&quot;g C m-2 yr-1&quot;) If the name or the units are the same, you can leave the Name and Unit fields blank. This is can be seen with the variable LAI. 15.1.2 Storage Type Storage Type only needs to be specified if the variable is stored in a format other than what would be expected (e.g. if numeric values are stored as quoted character strings). One such example is time variables. PEcAn converts all dates into POSIX format using R functions such as strptime. These functions require that the user specify the format in which the date is written. The default is &quot;%Y-%m-%d %H:%M:%S&quot; which would look like &quot;2017-01-01 00:00:00&quot; A list of date formats can be found in the R documentation for the function strptime Below are some commonly used codes: %d Day of the month as decimal number (01–31). %D Date format such as %m/%d/%y. %H Hours as decimal number (00–23). %m Month as decimal number (01–12). %M Minute as decimal number (00–59). %S Second as integer (00–61), allowing for up to two leap-seconds (but POSIX-compliant implementations will ignore leap seconds). %T Equivalent to %H:%M:%S. %y Year without century (00–99). On input, values 00 to 68 are prefixed by 20 and 69 to 99 by 19 – that is the behaviour specified by the 2004 and 2008 POSIX standards, but they do also say ‘it is expected that in a future version the default century inferred from a 2-digit year will change’. %Y Year with century. 15.1.3 Column Number If your data is in text format with variables in a standard order then you can specify the Column Number for the variable. This is required for text files that lack headers. 15.2 Retrieving Format Information To acquire Format information from a Format record, use the R function query.format.vars 15.2.1 Inputs bety: connection to BETY input.id=NA and/or format.id=NA: Input or Format record ID from BETY At least one must be specified. Defaults to format.id if both provided. var.ids=NA: optional vector of variable IDs. If provided, limits results to these variables. 15.2.2 Output R list object containing many things. Fill this in. 15.3 Input records in BETY All model input data or data used for model calibration/validation must be registered in the BETY database. Before creating a new Input record, you must make sure that the format type of your data is registered in the database. If you need to make a new format record, see Creating a new format record in BETY. 15.4 Create a database file record for the input data An input record contains all the metadata required to identify the data, however, this record does not include the location of the data file. Since the same data may be stored in multiple places, every file has its own dbfile record. From your BETY interface: Create a DBFILES entry for the path to the file From the menu click RUNS then FILES Click “New File” Select the machine your file is located at Fill in the File Path where your file is located (aka folder or directory) NOT including the name of the file itself Fill in the File Name with the name of the file itself. Note that some types of input records will refer to be ALL the files in a directory and thus File Name can be blank Click Update 15.5 Creating a new Input record in BETY From your BETY interface: Create an INPUT entry for your data From the menu click RUNS then INPUTS Click “New Input” Select the SITE that this data is associated with the input data set Other required fields are a unique name for the input, the start and end dates of the data set, and the format of the data. If the data is not in a currently known format you will need to create a NEW FORMAT and possibly a new input converter. Instructions on how to do add a converter can be found here Input conversion. Instructions on how to add a format record can be found here Parent ID is an optional variable to indicated that one dataset was derived from another. Click “Create” Associate the DBFILE with the INPUT In the RUNS -&gt; INPUTS table, search and find the input record you just created Click on the EDIT icon Select “View related Files” In the Search window, search for the DBFILE you just created Once you have found the DBFILE, click on the “+” icon to add the file Click on “Update” at the bottom when you are done. 15.5.1 Input Conversion Three Types of data conversions are discussed below: Meteorological data, Vegetation data, and Soil data. Each section provides instructions on how to convert data from their raw formats into a PEcAn standard format, whether it be from a database or if you have raw data in hand. Also, see [PEcAn standard formats]. 15.5.1.1 Meterological Data conversion 15.5.1.1.1 Adding a function to PEcAn to convert a met data source In general, you will need to write a function to download the raw met data and one to convert it to the PEcAn standard. Downloading raw data function are named download.&lt;source&gt;.R. These functions are stored within the PEcAn directory: /modules/data.atmosphere/R. Conversion function from raw to standard are named met2CF.&lt;source&gt;.R. These functions are stored within the PEcAn directory: /modules/data.atmosphere/R. Current Meteorological products that are coupled to PEcAn can be found in our Available Meteorological Drivers page. Note: Unless you are also adding a new model, you will not need to write a script to convert from PEcAn standard to PEcAn models. Those conversion scripts are written when a model is added and can be found within each model’s PEcAn directory. 15.5.1.1.2 Dimensions: CF standard-name units time days since 1700-01-01 00:00:00 UTC longitude degrees_east latitude degrees_north General Note: dates in the database should be date-time (preferably with timezone), and datetime passed around in PEcAn should be of type POSIXct. 15.5.1.1.3 The variable names should be standard_name CF standard-name units bety isimip cruncep narr ameriflux air_temperature K airT tasAdjust tair air TA (C) air_temperature_max K tasmaxAdjust NA tmax air_temperature_min K tasminAdjust NA tmin air_pressure Pa air_pressure PRESS (KPa) mole_fraction_of_carbon_dioxide_in_air mol/mol CO2 moisture_content_of_soil_layer kg m-2 soil_temperature K soilT TS1 (NOT DONE) relative_humidity % relative_humidity rhurs NA rhum RH specific_humidity 1 specific_humidity NA qair shum CALC(RH) water_vapor_saturation_deficit Pa VPD VPD (NOT DONE) surface_downwelling_longwave_flux_in_air W m-2 same rldsAdjust lwdown dlwrf Rgl surface_downwelling_shortwave_flux_in_air W m-2 solar_radiation rsdsAdjust swdown dswrf Rg surface_downwelling_photosynthetic_photon_flux_in_air mol m-2 s-1 PAR PAR (NOT DONE) precipitation_flux kg m-2 s-1 cccc prAdjust rain acpc PREC (mm/s) degrees wind_direction WD wind_speed m/s Wspd WS eastward_wind m/s eastward_wind CALC(WS+WD) northward_wind m/s northward_wind CALC(WS+WD) preferred variables indicated in bold wind_direction has no CF equivalent and should not be converted, instead the met2CF functions should convert wind_direction and wind_speed to eastward_wind and northward_wind standard_name is CF-convention standard names units can be converted by udunits, so these can vary (e.g. the time denominator may change with time frequency of inputs) soil moisture for the full column, rather than a layer, is soil_moisture_content A full list of PEcAn standard variable names, units and dimensions can be found here: https://github.com/PecanProject/pecan/blob/develop/base/utils/data/standard_vars.csv For example, in the MsTMIP-CRUNCEP data, the variable rain should be precipitation_rate. We want to standardize the units as well as part of the met2CF.&lt;product&gt; step. I believe we want to use the CF “canonical” units but retain the MsTMIP units any time CF is ambiguous about the units. The key is to process each type of met data (site, reanalysis, forecast, climate scenario, etc) to the exact same standard. This way every operation after that (extract, gap fill, downscale, convert to a model, etc) will always have the exact same inputs. This will make everything else much simpler to code and allow us to avoid a lot of unnecessary data checking, tests, etc being repeated in every downstream function. 15.5.1.1.4 Adding Single-Site Specific Meteorological Data Perhaps you have meteorological data specific to one site, with a unique format that you would like to add to PEcAn. Your steps would be to: 1. write a script or function to convert your files into the netcdf PEcAn standard 2. insert that file as an input record for your site following these instructions 15.5.1.1.5 Processing Met data outside of the workflow using PEcAn functions Perhaps you would like to obtain data from one of the sources coupled to PEcAn on its own. To do so you can run PEcAn functions on their own. 15.5.1.1.5.1 Example 1: Processing data from a database Download Amerifluxlbl from Niwot Ridge for the year 2004: raw.file &lt;-PEcAn.data.atmosphere::download.AmerifluxLBL(sitename = &quot;US-NR1&quot;, outfolder = &quot;.&quot;, start_date = &quot;2004-01-01&quot;, end_date = &quot;2004-12-31&quot;) Using the information returned as the object raw.file you will then convert the raw files into a standard file. Open a connection with BETY. You may need to change the host name depending on what machine you are hosting BETY. You can find the hostname listed in the machines table of BETY. bety &lt;- dplyr::src_postgres(dbname = &#39;bety&#39;, host =&#39;localhost&#39;, user = &quot;bety&quot;, password = &quot;bety&quot;) con &lt;- bety$con Next you will set up the arguments for the function in.path &lt;- &#39;.&#39; in.prefix &lt;- raw.file$dbfile.name outfolder &lt;- &#39;.&#39; format.id &lt;- 5000000002 format &lt;- PEcAn.DB::query.format.vars(format.id=format.id,bety = bety) lon &lt;- -105.54 lat &lt;- 40.03 format$time_zone &lt;- &quot;America/Chicago&quot; Note: The format.id can be pulled from the BETY database if you know the format of the raw data. Once these arguments are defined you can execute the met2CF.csv function PEcAn.data.atmosphere::met2CF.csv(in.path = in.path, in.prefix =in.prefix, outfolder = &quot;.&quot;, start_date =&quot;2004-01-01&quot;, end_date = &quot;2004-12-01&quot;, lat= lat, lon = lon, format = format) 15.5.1.1.5.2 Example 2: Processing data from data already in hand If you have Met data already in hand and you would like to convert into the PEcAn standard follow these instructions. Update BETY with file record, format record and input record according to this page How to Insert new Input Data If your data is in a csv format you can use the met2CF.csvfunction to convert your data into a PEcAn standard file. Open a connection with BETY. You may need to change the host name depending on what machine you are hosting BETY. You can find the hostname listed in the machines table of BETY. bety &lt;- dplyr::src_postgres(dbname = &#39;bety&#39;, host =&#39;localhost&#39;, user = &quot;bety&quot;, password = &quot;bety&quot;) con &lt;- bety$con Prepare the arguments you need to execute the met2CF.csv function in.path &lt;- &#39;path/where/the/raw/file/lives&#39; in.prefix &lt;- &#39;prefix_of_the_raw_file&#39; outfolder &lt;- &#39;path/to/where/you/want/to/output/thecsv/&#39; format.id &lt;- formatid of the format your created format &lt;- PEcAn.DB::query.format.vars(format.id=format.id,bety = bety) lon &lt;- longitude of your site lat &lt;- latitude of your site format$time_zone &lt;- time zone of your site start_date &lt;- Start date of your data in &quot;y-m-d&quot; end_date &lt;- End date of your data in &quot;y-m-d&quot; Next you can execute the function: PEcAn.data.atmosphere::met2CF.csv(in.path = in.path, in.prefix =in.prefix, outfolder = &quot;.&quot;, start_date = start_date, end_date = end_date, lat= lat, lon = lon, format = format) 15.5.1.2 Vegetation Data Vegetation data will be required to parameterize your model. In these examples we will go over how to produce a standard initial condition file. The main function to process cohort data is the ic.process.R function. As of now however, if you require pool data you will run a separate function, pool_ic_list2netcdf.R. 15.5.1.2.0.1 Example 1: Processing Veg data from data in hand. In the following example we will process vegetation data that you have in hand using PEcAn. First, you’ll need to create a input record in BETY that will have a file record and format record reflecting the location and format of your file. Instructions can be found in our How to Insert new Input Data page. Once you have created an input record you must take note of the input id of your record. An easy way to take note of this is in the URL of the BETY webpage that shows your input record. In this example we use an input record with the id 1000013064 which can be found at this url: https://psql-pecan.bu.edu/bety/inputs/1000013064# . Note that this is the Boston University BETY database. If you are on a different machine, your url will be different. With the input id in hand you can now edit a pecan XML so that the PEcAn function ic.process will know where to look in order to process your data. The inputs section of your pecan XML will look like this. As of now ic.process is set up to work with the ED2 model so we will use ED2 settings and then grab the intermediary Rds data file that is created as the standard PEcAn file. For your Inputs section you will need to input your input id wherever you see the source.ic flag. &lt;inputs&gt; &lt;css&gt; &lt;source&gt;FFT&lt;/source&gt; &lt;output&gt;css&lt;/output&gt; &lt;username&gt;pecan&lt;/username&gt; &lt;source.id&gt;1000013064&lt;/source.id&gt; &lt;useic&gt;TRUE&lt;/useic&gt; &lt;meta&gt; &lt;trk&gt;1&lt;/trk&gt; &lt;age&gt;70&lt;/age&gt; &lt;/meta&gt; &lt;/css&gt; &lt;pss&gt; &lt;source&gt;FFT&lt;/source&gt; &lt;output&gt;pss&lt;/output&gt; &lt;username&gt;pecan&lt;/username&gt; &lt;source.id&gt;1000013064&lt;/source.id&gt; &lt;useic&gt;TRUE&lt;/useic&gt; &lt;/pss&gt; &lt;site&gt; &lt;source&gt;FFT&lt;/source&gt; &lt;output&gt;site&lt;/output&gt; &lt;username&gt;pecan&lt;/username&gt; &lt;source.id&gt;1000013064&lt;/source.id&gt; &lt;useic&gt;TRUE&lt;/useic&gt; &lt;/site&gt; &lt;met&gt; &lt;source&gt;CRUNCEP&lt;/source&gt; &lt;output&gt;ED2&lt;/output&gt; &lt;/met&gt; &lt;lu&gt; &lt;id&gt;294&lt;/id&gt; &lt;/lu&gt; &lt;soil&gt; &lt;id&gt;297&lt;/id&gt; &lt;/soil&gt; &lt;thsum&gt; &lt;id&gt;295&lt;/id&gt; &lt;/thsum&gt; &lt;veg&gt; &lt;id&gt;296&lt;/id&gt; &lt;/veg&gt; &lt;/inputs&gt; Once you edit your PEcAn.xml you can than create a settings object using PEcAn functions. Your pecan.xml must be in your working directory. settings &lt;- PEcAn.settings::read.settings(&quot;pecan.xml&quot;) settings &lt;- PEcAn.settings::prepare.settings(settings, force=FALSE) You can then execute the ic.process function to convert data into a standard Rds file: input &lt;- settings$run$inputs dir &lt;- &quot;.&quot; ic.process(settings, input, dir, overwrite = FALSE) Note that the argument dir is set to the current directory. You will find the final ED2 file there. More importantly though you will find the .Rds file within the same directory. 15.5.1.2.0.2 Example 3 Pool Initial Condition files If you have pool vegetation data, you’ll need the pool_ic_list2netcdf.R function to convert the pool data into PEcAn standard. The function stands alone and requires that you provide a named list of netcdf dimensions and values, and a named list of variables and values. Names and units need to match the standard_vars.csv table found here. #Create a list object with necessary dimensions for your site input&lt;-list() dims&lt;- list(lat=-115,lon=45, time= 1) variables&lt;- list(SoilResp=8,TotLivBiom=295) input$dims &lt;- dims input$vals &lt;- variables Once this is done, set outdir to where you’d like the file to write out to and a siteid. Siteid in this can be used as an file name identifier. Once part of the automated workflow siteid will reflect the site id within the BET db. outdir &lt;- &quot;.&quot; siteid &lt;- 772 pool_ic_list2netcdf(input = input, outdir = outdir, siteid = siteid) You should now have a netcdf file with initial conditions. 15.5.1.3 Soil Data 15.5.1.3.0.1 Example 1: Converting Data in hand Local data that has the correct names and units can easily be written out in PEcAn standard using the function soil2netcdf. soil.data &lt;- list(volume_fraction_of_sand_in_soil = c(0.3,0.4,0.5), volume_fraction_of_clay_in_soil = c(0.3,0.3,0.3), soil_depth = c(0.2,0.5,1.0)) soil2netcdf(soil.data,&quot;soil.nc&quot;) At the moment this file would need to be inserted into Inputs manually. By default, this function also calls soil_params, which will estimate a number of hydraulic and thermal parameters from texture. Be aware that at the moment not all model couplers are yet set up to read this file and/or convert it to model-specific formats. 15.5.1.3.0.2 Example 2: Converting PalEON data In addition to location-specific soil data, PEcAn can extract soil texture information from the PalEON regional soil product, which itself is a subset of the MsTMIP Unified North American Soil Map. If this product is installed on your machine, the appropriate step in the do_conversions workflow is enabled by adding the following tag under &lt;inputs&gt; in your pecan.xml &lt;soil&gt; &lt;id&gt;1000012896&lt;/id&gt; &lt;/soil&gt; In the future we aim to extend this extraction to a wider range of soil products. "],
["reference-runs.html", "16 Reference Runs 16.1 Editing records 16.2 Standalone tools (modules) 16.3 Loading Data in PEcAn", " 16 Reference Runs The purpose of the reference run record in BETY is to store all the settings from a run that are necessary in exactly recreating it. The pecan.xml file is the home of absolutely all the settings for a particular run in pecan. However, much of the information in the pecan.xml file is server and user specific and more importantly, the pecan.xml files are stored on individual servers and may not be available to the public. When a run that is performed using pecan is registered as a reference run, the settings that were used to make that run are made available to all users through the database. All completed runs are not automatically registered as reference runs. To register a run, navigate to the benchmarking section of the workflow visualizations Shiny app. 16.1 Editing records Models Species PFTs Traits Inputs DB files Variables Formats (Link each section to relevant Bety tables) 16.2 Standalone tools (modules) RTM Photosynthesis Allometry benchmark::load_data (Link to vignettes) 16.3 Loading Data in PEcAn If you are loading data in to PEcAn for benchmarking, using the Benchmarking shiny app [provide link?] is recommended. Data can be loaded manually using the load_data function which in turn requires providing data format information using query.format.vars and the path to the data using query.file.path. Below is a description of the load_data function an a simple example of loading data manually. 16.3.1 Function load_data 16.3.1.1 Inputs Required data.path: path to the data that is the output of the function query.file.path (see example below) format: R list object that is the output of the function query.format.vars (see example below) Optional start_year = NA: end_year = NA: site = NA vars.used.index=NULL 16.3.1.2 Output R data frame containing the requested variables converted in to PEcAn standard name and units and time steps in POSIX format. 16.3.2 Example The data for this example has already been entered in to the database. To add new data go to new data documentation. To load the Ameriflux data for the Harvard Forest (US-Ha1) site. Create a connection to the BETY database. This can be done using R function R bety = PEcAn.DB::betyConnect(php.config = &quot;pecan/web/config.php&quot;) where the complete path to the config.php is specified. See here for an example config.php file. Look up the inputs record for the data in BETY. Input_ID_name To find the input ID, either look at The url of the record (see image above) In R run R library(dplyr) input_name = &quot;AmerifluxLBL_site_0-758&quot; #copied directly from online input.id = tbl(bety,&quot;inputs&quot;) %&gt;% filter(name == input_name) %&gt;% pull(id) Additional arguments to query.format.vars are optional If you only want to load a subset of dates in the data, specify start and end year, otherwise all data will be loaded. If you only want to load a select list of variables from the data, look up their IDs in BETY, otherwise all variables will be loaded. In R run R format = PEcAn.DB::query.format.vars(bety, input.id) Examine the resulting R list object to make sure it returned the correct information. The example format contains the following objects: ```R $file_name [1] “AMERIFLUX_BASE_HH” $mimetype [1] “csv” $skip [1] 2 $header [1] 1 $na.strings [1] “-9999” “-6999” “9999” “NA” $time.row [1] 4 $site [1] 758 $lat [1] 42.5378 $lon [1] -72.1715 $time_zone [1] “America/New_York” ``` The first 4 rows of the table format$vars looks like this: bety_name variable_id input_name input_units storage_type column_number bety_units mstmip_name mstmip_units pecan_name pecan_units air_pressure 554 PA kPa 19 Pa Psurf Pa Psurf Pa airT 86 TA celsius 4 degrees C Tair K Tair K co2atm 135 CO2_1 umol mol-1 20 umol mol-1 CO2air micromol mol-1 CO2air micromol mol-1 datetime 5000000001 TIMESTAMP_START ymd_hms %Y%m%d%H%M 1 ymd_hms NA NA datetime ymd_hms Get the path to the data R data.path = PEcAn.DB::query.file.path( input.id = input.id, host_name = PEcAn.remote::fqdn(), con = bety$con) Load the data R data = PEcAn.benchmark::load_data(data.path = data.path, format = format) "],
["working-with-the-vm.html", "17 Working with the VM 17.1 Connecting to the VM via SSH 17.2 Connecting to bety on the VM via SSh", " 17 Working with the VM 17.1 Connecting to the VM via SSH Once the VM is running anywhere on your machine, you can connect to it from a separate terminal via SSH as follows: ssh -p 6422 carya@localhost You will be prompted for a password. Like everywhere else in PEcAn, the username is carya and the password is illinois. The same password is used for any system maintenance you wish to do on the VM via sudo. As a shortcut, you can add the following to your ~/.ssh/config file (or create one if it does not exist). Host pecan-vm Hostname localhost Port 6422 user carya ForwardX11Trusted yes This will allow you to SSH into the VM with the simplified command, ssh pecan-vm. 17.2 Connecting to bety on the VM via SSh Sometimes, you may want to develop code locally but connect to an instance of Bety on the VM. To do this, first open a new terminal and connect to the VM while enabling port forwarding (with the -L flag) and setting XXXX to any available port (more or less any 4 digit number – a reasonable choice is 3333). ssh -L XXXX:localhost:5432 carya@localhost:6422 This makes port XXXX on the local machine match port 5432 on the VM. This means that connecting to localhost:XXXX will give you access to Bety on the VM. To test this on the command line, try the following command, which, if successful, will drop you into the psql console. psql -d bety -U bety -h localhost -p XXXX To test this in R, open a Postgres using the analogous parameters: library(RPostgres) con &lt;- dbConnect( Postgres(), user = &quot;bety&quot;, password = &quot;bety&quot;, dbname = &quot;bety&quot;, host = &quot;localhost&quot;, port = XXXX ) dbListTables(con) # This should return a vector of bety tables Note that the same general approach will work on any Bety server where port forwarding is enabled. "],
["vm-desktop-conversion.html", "18 VM Desktop Conversion", " 18 VM Desktop Conversion sudo apt-get update sudo apt-get install xfce4 xorg For a more refined desktop environment, try sudo apt-get install --no-install-recommends xubuntu-desktop replace xubuntu- with ubuntu-, lubuntu-, or other preferred desktop enviornment the --no-install-recommends eliminates additional applications, removing it will add a word processor, a browser, and lots of other applications included in the default operating system. Reinstall Virtual Box additions for better integration adding X/mouse support sudo mount /dev/cdrom /mnt sudo /mnt/VBoxLinuxAdditions.run sudo umount /mnt 18.0.1 Install RStudio Desktop wget http://download1.rstudio.org/rstudio-0.97.551-amd64.deb apt-get install libjpeg621 dpkg -i rstudio-* rm rstudio-* "],
["introduction-developer-guide.html", "19 Introduction: Developer Guide", " 19 Introduction: Developer Guide This section is for users who will be actively developing PEcAn. "],
["pecan-setup.html", "20 PEcAn Setup 20.1 Installing the PEcAn VM 20.2 Install PEcAn by hand 20.3 AWS Setup 20.4 Shiny Setup 20.5 Thredds Setup", " 20 PEcAn Setup 20.1 Installing the PEcAn VM We recommend that new users download the PEcAn VM instead of creating from one from scratch. Additional Setup - AWS - Shiny - Thredds 20.2 Install PEcAn by hand These instructions are provided to document how to install PEcAn on different Operating Systems. Prerequisites OS specific installations Install BETY Install models Clone and Compile PEcAn source code Test Run 20.2.1 Installation Prerequisites Please review the following topics before starting PEcAn installation VM Creation Installing PEcAn Data Enabling Remote Execution 20.2.1.1 Creating a Virtual Machine First create virtual machine # ---------------------------------------------------------------------- # CREATE VM USING FOLLOWING: # - VM NAME = PEcAn # - CPU = 2 # - MEMORY = 2GB # - DISK = 100GB # - HOSTNAME = pecan # - FULLNAME = PEcAn Demo User # - USERNAME = xxxxxxx # - PASSWORD = yyyyyyy # - PACKAGE = openssh # ---------------------------------------------------------------------- To enable tunnels run the following on the host machine: VBoxManage modifyvm &quot;PEcAn&quot; --natpf1 &quot;ssh,tcp,,6422,,22&quot; VBoxManage modifyvm &quot;PEcAn&quot; --natpf1 &quot;www,tcp,,6480,,80&quot; Make sure machine is up to date. UBUNTU sudo apt-get update sudo apt-get -y dist-upgrade sudo reboot CENTOS/REDHAT sudo yum -y update sudo reboot Install compiler and other packages needed and install the tools. UBUNTU sudo apt-get -y install build-essential linux-headers-server dkms CENTOS/REDHAT sudo yum -y groupinstall &quot;Development Tools&quot; sudo yum -y install wget Install Virtual Box additions for better integration sudo mount /dev/cdrom /mnt sudo /mnt/VBoxLinuxAdditions.run sudo umount /mnt sudo usermod -a -G vboxsf carya Finishing up the machine Add a message to the login: sudo -s export PORT=$( hostname | sed &#39;s/pecan//&#39; ) cat &gt; /etc/motd &lt;&lt; EOF PEcAn version 1.4.3 For more information about: Pecan - http://pecanproject.org BETY - http://www.betydb.org For a list of all models currently navigate [here](../users_guide/basic_users_guide/models_table.md) You can access this system using a webbrowser at http://&lt;hosting machine&gt;:${PORT}80/ or using SSH at ssh -l carya -p ${PORT}22 &lt;hosting machine&gt; where &lt;hosting machine&gt; is the machine where the VM runs on. EOF exit Finishing up Script to clean the VM and remove as much as possible history cleanvm.sh wget -O ~/cleanvm.sh http://isda.ncsa.uiuc.edu/~kooper/EBI/cleanvm.sh chmod 755 ~/cleanvm.sh Make sure machine has SSH keys rc.local sudo wget -O /etc/rc.local http://isda.ncsa.illinois.edu/~kooper/EBI/rc.local Change the resolution of the console sudo sed -i -e &#39;s/#GRUB_GFXMODE=640x480/GRUB_GFXMODE=1024x768/&#39; /etc/default/grub sudo update-grub Once all done, stop the virtual machine history -c &amp;&amp; ${HOME}/cleanvm.sh 20.2.1.2 Installing data for PEcAn PEcAn assumes some of the data to be installed on the machine. This page will describe how to install this data. 20.2.1.2.1 Site Information These are large-ish files that contain data used with ED2 and SIPNET rm -rf sites curl -o sites.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/sites.tgz tar zxf sites.tgz sed -i -e &quot;s#/home/kooper/Projects/EBI#${PWD}#&quot; sites/*/ED_MET_DRIVER_HEADER rm sites.tgz rm -rf inputs curl -o inputs.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/inputs.tgz tar zxf inputs.tgz rm inputs.tgz 20.2.1.2.2 FIA database FIA database is large and will add an extra 10GB to the installation. # download and install database curl -o fia5data.psql.gz http://isda.ncsa.illinois.edu/~kooper/EBI/fia5data.psql.gz dropdb --if-exists fia5data createdb -O bety fia5data gunzip fia5data.psql.gz psql -U bety -d fia5data &lt; fia5data.psql rm fia5data.psql 20.2.1.2.3 Flux Camp Following will install the data for flux camp (as well as the demo script for PEcAn). cd curl -o plot.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/plot.tgz tar zxf plot.tgz rm plot.tgz 20.2.1.2.4 Harvard for ED tutorial Add datasets and runs curl -o Santarem_Km83.zip http://isda.ncsa.illinois.edu/~kooper/EBI/Santarem_Km83.zip unzip -d sites Santarem_Km83.zip sed -i -e &quot;s#/home/pecan#${HOME}#&quot; sites/Santarem_Km83/ED_MET_DRIVER_HEADER rm Santarem_Km83.zip curl -o testrun.s83.zip http://isda.ncsa.illinois.edu/~kooper/EBI/testrun.s83.zip unzip testrun.s83.zip sed -i -e &quot;s#/home/pecan#${HOME}#&quot; testrun.s83/ED2IN rm testrun.s83.zip curl -o ed2ws.harvard.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/ed2ws.harvard.tgz tar zxf ed2ws.harvard.tgz mkdir ed2ws.harvard/analy ed2ws.harvard/histo sed -i -e &quot;s#/home/pecan#${HOME}#g&quot; ed2ws.harvard/input_harvard/met_driver/HF_MET_HEADER ed2ws.harvard/ED2IN ed2ws.harvard/*.r rm ed2ws.harvard.tgz curl -o testrun.PDG.zip http://isda.ncsa.illinois.edu/~kooper/EBI/testrun.PDG.zip unzip testrun.PDG.zip sed -i -e &quot;s#/home/pecan#${HOME}#&quot; testrun.PDG/Met/PDG_MET_DRIVER testrun.PDG/Template/ED2IN sed -i -e &#39;s#/n/scratch2/moorcroft_lab/kzhang/PDG/WFire_Pecan/##&#39; testrun.PDG/Template/ED2IN rm testrun.PDG.zip curl -o create_met_driver.tar.gz http://isda.ncsa.illinois.edu/~kooper/EBI/create_met_driver.tar.gz tar zxf create_met_driver.tar.gz rm create_met_driver.tar.gz 20.2.2 OS Specific Installations Ubuntu CentOS OSX RedHat Check this vs. CentOS … ? 20.2.2.1 Ubuntu These are specific notes for installing PEcAn on Ubuntu (14.04) and will be referenced from the main installing PEcAn page. You will at least need to install the build environment and Postgres sections. If you want to access the database/PEcAn using a web browser you will need to install Apache. To access the database using the BETY interface, you will need to have Ruby installed. This document also contains information on how to install the Rstudio server edition as well as any other packages that can be helpful. 20.2.2.1.1 Install build environment sudo -s # point to latest R echo &quot;deb http://cran.rstudio.com/bin/linux/ubuntu `lsb_release -s -c`/&quot; &gt; /etc/apt/sources.list.d/R.list apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9 # update package list apt-get -y update # install packages needed for PEcAn apt-get -y install build-essential gfortran git r-base-core r-base-dev jags liblapack-dev libnetcdf-dev netcdf-bin bc libcurl4-gnutls-dev curl udunits-bin libudunits2-dev libgmp-dev python-dev libgdal1-dev libproj-dev expect # install packages needed for ED2 apt-get -y install openmpi-bin libopenmpi-dev # install requirements for DALEC apt-get -y install libgsl0-dev # install packages for webserver apt-get -y install apache2 libapache2-mod-php5 php5 # install packages to compile docs apt-get -y install texinfo texlive-latex-base texlive-latex-extra texlive-fonts-recommended # install devtools echo &#39;install.packages(&quot;devtools&quot;, repos=&quot;http://cran.rstudio.com/&quot;)&#39; | R --vanilla # done as root exit 20.2.2.1.2 Install Postgres Documentation: http://trac.osgeo.org/postgis/wiki/UsersWikiPostGIS21UbuntuPGSQL93Apt sudo -s # point to latest PostgreSQL echo &quot;deb http://apt.postgresql.org/pub/repos/apt `lsb_release -s -c`-pgdg main&quot; &gt; /etc/apt/sources.list.d/pgdg.list wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add - # update package list apt-get -y update # install packages for postgresql (using a newer version than default) apt-get -y install libdbd-pgsql postgresql postgresql-client libpq-dev postgresql-9.4-postgis-2.1 postgresql-9.4-postgis-2.1-scripts # install following if you want to run pecan through the web apt-get -y install php5-pgsql # enable bety user to login with trust by adding the following lines after # the ability of postgres user to login in /etc/postgresql/9.4/main/pg_hba.conf local   all             bety                                    trust host    all             bety            127.0.0.1/32            trust host    all             bety            ::1/128                 trust # Once done restart postgresql /etc/init.d/postgresql restart exit To install the BETYdb database .. ##### Apache Configuration PEcAn # become root sudo -s # get index page rm /var/www/html/index.html ln -s ${HOME}/pecan/documentation/index_vm.html /var/www/html/index.html # setup a redirect cat &gt; /etc/apache2/conf-available/pecan.conf &lt;&lt; EOF Alias /pecan ${HOME}/pecan/web &lt;Directory ${HOME}/pecan/web&gt; DirectoryIndex index.php Options +ExecCGI Require all granted &lt;/Directory&gt; EOF a2enconf pecan /etc/init.d/apache2 restart # done as root exit 20.2.2.1.3 Apache Configuration BETY sudo -s # install all ruby related packages apt-get -y install ruby2.0 ruby2.0-dev libapache2-mod-passenger # link static content ln -s ${HOME}/bety/public /var/www/html/bety # setup a redirect cat &gt; /etc/apache2/conf-available/bety.conf &lt;&lt; EOF RailsEnv production RailsBaseURI /bety PassengerRuby /usr/bin/ruby2.0 &lt;Directory /var/www/html/bety&gt; Options +FollowSymLinks Require all granted &lt;/Directory&gt; EOF a2enconf bety /etc/init.d/apache2 restart 20.2.2.1.4 Rstudio-server NOTE This will allow anybody to login to the machine through the rstudio interface and run any arbitrary code. The login used however is the same as the system login/password. Based on version of ubuntu 32/64 use either of the following 32bit only wget http://download2.rstudio.org/rstudio-server-0.98.1103-i386.deb 64bit only wget http://download2.rstudio.org/rstudio-server-0.98.1103-amd64.deb # bceome root sudo -s # install required packages apt-get -y install libapparmor1 apparmor-utils libssl0.9.8 # install rstudio dpkg -i rstudio-server-* rm rstudio-server-* echo &quot;www-address=127.0.0.1&quot; &gt;&gt; /etc/rstudio/rserver.conf echo &quot;r-libs-user=~/R/library&quot; &gt;&gt; /etc/rstudio/rsession.conf rstudio-server restart # setup rstudio forwarding in apache a2enmod proxy_http cat &gt; /etc/apache2/conf-available/rstudio.conf &lt;&lt; EOF ProxyPass /rstudio/ http://localhost:8787/ ProxyPassReverse /rstudio/ http://localhost:8787/ RedirectMatch permanent ^/rstudio$ /rstudio/ EOF a2enconf rstudio /etc/init.d/apache2 restart # all done, exit root exit 20.2.2.1.5 Additional packages HDF5 Tools, netcdf, GDB and emacs sudo apt-get -y install hdf5-tools cdo nco netcdf-bin ncview gdb emacs ess nedit 20.2.2.2 CentOS/RedHat These are specific notes for installing PEcAn on CentOS (7) and will be referenced from the main installing PEcAn page. You will at least need to install the build environment and Postgres sections. If you want to access the database/PEcAn using a web browser you will need to install Apache. To access the database using the BETY interface, you will need to have Ruby installed. This document also contains information on how to install the Rstudio server edition as well as any other packages that can be helpful. 20.2.2.2.1 Install build environment sudo -s # install packages needed for PEcAn yum -y groupinstall &#39;Development Tools&#39; yum -y install git netcdf-fortran-openmpi-devel R bc curl libxml2-devel openssl-devel ed udunits2 udunits2-devel netcdf netcdf-devel gmp-devel python-devel gdal-devel proj-devel proj-epsg expect # jags yum -y install http://download.opensuse.org/repositories/home:/cornell_vrdc/CentOS_7/x86_64/jags3-3.4.0-54.1.x86_64.rpm yum -y install http://download.opensuse.org/repositories/home:/cornell_vrdc/CentOS_7/x86_64/jags3-devel-3.4.0-54.1.x86_64.rpm # fix include folder for udunits2 ln -s /usr/include/udunits2/* /usr/include/ # install packages needed for ED2 yum -y install environment-modules openmpi-bin libopenmpi-dev # install requirements for DALEC yum -y install gsl-devel # install packages for webserver yum -y install httpd php systemctl enable httpd systemctl start httpd firewall-cmd --zone=public --add-port=80/tcp --permanent firewall-cmd --reload # install packages to compile docs #apt-get -y install texinfo texlive-latex-base texlive-latex-extra texlive-fonts-recommended # install devtools echo &#39;install.packages(&quot;devtools&quot;, repos=&quot;http://cran.rstudio.com/&quot;)&#39; | R --vanilla # done as root exit echo &quot;module load mpi&quot; &gt;&gt; ~/.bashrc module load mpi 20.2.2.2.2 Install Postgres sudo -s # point to latest PostgreSQL yum install -y epel-release yum -y install http://yum.postgresql.org/9.4/redhat/rhel-7-x86_64/pgdg-centos94-9.4-1.noarch.rpm # install packages for postgresql (using a newer version than default) yum -y install postgresql94-server postgresql94-contrib postgis2_94 postgresql94-devel # install following if you want to run pecan through the web yum -y install php-pgsql # enable bety user to login with trust by adding the following lines after # the ability of postgres user to login in /var/lib/pgsql/9.4/data/pg_hba.conf local all bety trust host all bety 127.0.0.1/32 trust host all bety ::1/128 trust # Create database /usr/pgsql-9.4/bin/postgresql94-setup initdb # Enable postgres systemctl enable postgresql-9.4 systemctl start postgresql-9.4 exit 20.2.2.2.3 Apache Configuration PEcAn # become root sudo -s # get index page rm /var/www/html/index.html ln -s /home/carya/pecan/documentation/index_vm.html /var/www/html/index.html # fix selinux context (does this need to be done after PEcAn is installed?) chcon -R -t httpd_sys_content_t /home/carya/pecan /home/carya/output # setup a redirect cat &gt; /etc/httpd/conf.d/pecan.conf &lt;&lt; EOF Alias /pecan /home/carya/pecan/web &lt;Directory /home/carya/pecan/web&gt; DirectoryIndex index.php Options +ExecCGI Require all granted &lt;/Directory&gt; EOF a2enconf pecan /etc/init.d/apache2 restart # done as root exit 20.2.2.2.4 Apache Configuration BETY sudo -s # install all ruby related packages sudo curl --fail -sSLo /etc/yum.repos.d/passenger.repo https://oss-binaries.phusionpassenger.com/yum/definitions/el-passenger.repo yum -y install ruby ruby-devel mod_passenger # link static content ln -s /home/carya/bety/public /var/www/html/bety # fix GemFile echo &#39;gem &quot;test-unit&quot;&#39; &gt;&gt; bety/Gemlile # fix selinux context (does this need to be done after bety is installed?) chcon -R -t httpd_sys_content_t /home/carya/bety # setup a redirect cat &gt; /etc/httpd/conf.d/bety.conf &lt;&lt; EOF RailsEnv production RailsBaseURI /bety PassengerRuby /usr/bin/ruby &lt;Directory /var/www/html/bety&gt; Options +FollowSymLinks Require all granted &lt;/Directory&gt; EOF systemctl restart httpd 20.2.2.2.5 Rstudio-server NEED FIXING NOTE This will allow anybody to login to the machine through the rstudio interface and run any arbitrary code. The login used however is the same as the system login/password. Based on version of ubuntu 32/64 use either of the following 32bit only wget http://download2.rstudio.org/rstudio-server-0.98.1103-i386.deb 64bit only wget http://download2.rstudio.org/rstudio-server-0.98.1103-amd64.deb # bceome root sudo -s # install required packages apt-get -y install libapparmor1 apparmor-utils libssl0.9.8 # install rstudio dpkg -i rstudio-server-* rm rstudio-server-* echo &quot;www-address=127.0.0.1&quot; &gt;&gt; /etc/rstudio/rserver.conf echo &quot;r-libs-user=~/R/library&quot; &gt;&gt; /etc/rstudio/rsession.conf rstudio-server restart # setup rstudio forwarding in apache a2enmod proxy_http cat &gt; /etc/apache2/conf-available/rstudio.conf &lt;&lt; EOF ProxyPass /rstudio/ http://localhost:8787/ ProxyPassReverse /rstudio/ http://localhost:8787/ RedirectMatch permanent ^/rstudio$ /rstudio/ EOF a2enconf rstudio /etc/init.d/apache2 restart # all done, exit root exit 20.2.2.2.6 Additional packages NEED FIXING HDF5 Tools, netcdf, GDB and emacs sudo apt-get -y install hdf5-tools cdo nco netcdf-bin ncview gdb emacs ess nedit 20.2.2.3 Mac OSX These are specific notes for installing PEcAn on Mac OSX and will be referenced from the main installing PEcAn page. You will at least need to install the build environment and Postgres sections. If you want to access the database/PEcAn using a web browser you will need to install Apache. To access the database using the BETY interface, you will need to have Ruby installed. This document also contains information on how to install the Rstudio server edition as well as any other packages that can be helpful. 20.2.2.3.1 Install build environment # install R # download from http://cran.r-project.org/bin/macosx/ # install gfortran # download from http://cran.r-project.org/bin/macosx/tools/ # install OpenMPI curl -o openmpi-1.6.3.tar.gz http://www.open-mpi.org/software/ompi/v1.6/downloads/openmpi-1.6.3.tar.gz tar zxf openmpi-1.6.3.tar.gz cd openmpi-1.6.3 ./configure --prefix=/usr/local make all sudo make install cd .. # install szip curl -o szip-2.1-MacOSX-intel.tar.gz ftp://ftp.hdfgroup.org/lib-external/szip/2.1/bin/szip-2.1-MacOSX-intel.tar.gz tar zxf szip-2.1-MacOSX-intel.tar.gz sudo mv szip-2.1-MacOSX-intel /usr/local/szip # install HDF5 curl -o hdf5-1.8.11.tar.gz http://www.hdfgroup.org/ftp/HDF5/current/src/hdf5-1.8.11.tar.gz tar zxf hdf5-1.8.11.tar.gz cd hdf5-1.8.11 sed -i -e &#39;s/-O3/-O0/g&#39; config/gnu-flags ./configure --prefix=/usr/local/hdf5 --enable-fortran --enable-cxx --with-szlib=/usr/local/szip make # make check sudo make install # sudo make check-install cd .. 20.2.2.3.2 Install Postgres For those on a Mac I use the following app for postgresql which has postgis already installed (http://postgresapp.com/) To get postgis run the following commands in psql: ##### Enable PostGIS (includes raster) CREATE EXTENSION postgis; ##### Enable Topology CREATE EXTENSION postgis_topology; ##### fuzzy matching needed for Tiger CREATE EXTENSION fuzzystrmatch; ##### Enable US Tiger Geocoder CREATE EXTENSION postgis_tiger_geocoder; To check your postgis run the following command again in psql: SELECT PostGIS_full_version(); 20.2.2.3.3 Additional installs 20.2.2.3.3.1 Install JAGS Download JAGS from http://sourceforge.net/projects/mcmc-jags/files/JAGS/3.x/Mac%20OS%20X/JAGS-Mavericks-3.4.0.dmg/download 20.2.2.3.3.2 Install udunits Installing udunits-2 on MacOSX is done from source. download most recent version of Udunits here instructions for compiling from source curl -o udunits-2.1.24.tar.gz ftp://ftp.unidata.ucar.edu/pub/udunits/udunits-2.1.24.tar.gz tar zxf udunits-2.1.24.tar.gz cd udunits-2.1.24 ./configure make sudo make install 20.2.2.3.4 Apache Configuration Mac does not support pdo/postgresql by default. The easiest way to install is use: http://php-osx.liip.ch/ To enable pecan to run from your webserver. cat &gt; /etc/apache2/others/pecan.conf &lt;&lt; EOF Alias /pecan ${PWD}/pecan/web &lt;Directory ${PWD}/pecan/web&gt; DirectoryIndex index.php Options +All Require all granted &lt;/Directory&gt; EOF 20.2.2.3.5 Ruby The default version of ruby should work. Or use JewelryBox. 20.2.2.3.6 Rstudio Server For the mac you can download Rstudio Desktop. 20.2.2.4 CentOS / RHEL These are specific notes for installing PEcAn on RedHat/CentOS and will be referenced from the main installing PEcAn page. You will at least need to install the build environment and Postgres sections. If you want to access the database/PEcAn using a web browser you will need to install Apache. To access the database using the BETY interface, you will need to have Ruby installed. This document also contains information on how to install the Rstudio server edition as well as any other packages that can be helpful. 20.2.2.4.1 Install build environment 20.2.2.4.1.1 Install and configure PostgreSQL, udunits2, NetCDF Reference: centoshelp.org yum install -y epel-release yum -y install http://yum.postgresql.org/9.4/redhat/rhel-7-x86_64/pgdg-centos94-9.4-1.noarch.rpm yum -y install git R postgresql94-server postgresql94-contrib postgis2_94 udunits2 netcdf /usr/pgsql-9.4/bin/postgresql94-setup initdb systemctl enable postgresql-9.4 systemctl start postgresql-9.4 20.2.2.4.1.2 Install and start Apache yum -y install httpd systemctl enable httpd systemctl start httpd 20.2.2.4.1.3 Install PHP sudo yum -y install php php-pgsql 20.2.2.4.1.4 Install ruby-netcdf gem cd $RUBY_APPLICATION_HOME export $NETCDF_URL=http://www.gfd-dennou.org/arch/ruby/products/ruby-netcdf/release/ruby-netcdf-0.6.6.tar.gz export $NETCDF_DIR=/usr/local/netcdf gem install narray export NARRAY_DIR=&quot;$(ls $GEM_HOME/gems | grep &#39;narray-&#39;)&quot; export NARRAY_PATH=&quot;$GEM_HOME/gems/$NARRAY_DIR&quot; cd $MY_RUBY_HOME/bin wget $NETCDF_URL -O ruby-netcdf.tgz tar zxf ruby-netcdf.tgz &amp;&amp; cd ruby-netcdf-0.6.6/ ruby -rubygems extconf.rb --with-narray-include=$NARRAY_PATH --with-netcdf-dir=/usr/local/netcdf-4.3.0 sed -i &#39;s|rb/$|rb|&#39; Makefile make make install cd ../ &amp;&amp; sudo rm -rf ruby-netcdf* cd $RUBY_APPLICATION bundle install --without development 20.2.2.4.2 Apache Configuration 20.2.2.4.3 Install and configure Rstudio-server based on Rstudio Server documentation add PATH=$PATH:/usr/sbin:/sbin to /etc/profile cat &quot;PATH=$PATH:/usr/sbin:/sbin; export PATH&quot; &gt;&gt; /etc/profile add rstudio.conf to /etc/httpd/conf.d/ wget https://gist.github.com/dlebauer/6921889/raw/d1e0f945228e5519afa6223d6f49d6e0617262bd/rstudio.conf sudo mv rstudio.conf /httpd/conf.d/ download and install server: wget http://download2.rstudio.org/rstudio-server-0.97.551-i686.rpm sudo yum install --nogpgcheck rstudio-server-0.97.551-i686.rpm restart server sudo httpd restart now you should be able to access http://&lt;server&gt;/rstudio 20.2.2.4.4 Install Postgres See documentation under the BETYdb Wiki 20.2.3 Installing BETY 20.2.3.1 Install Postgres See OS-specific instructions for installing Postgres + PostGIS * Ubuntu * OSX * RedHat / CentOS 20.2.3.2 Install Database + Data note To install BETYdb without PEcAn, first download the load.bety.sh script # install database (code assumes password is bety) sudo -u postgres createuser -d -l -P -R -S bety sudo -u postgres createdb -O bety bety sudo -u postgres ./scripts/load.bety.sh -c YES -u YES -r 0 sudo -u postgres ./scripts/load.bety.sh -r 1 sudo -u postgres ./scripts/load.bety.sh -r 2 # configure for PEcAn web app (change password if needed) cp web/config.example.php web/config.php # add models to database (VM only) ./scripts/add.models.sh # add data to database ./scripts/add.data.sh # create outputs folder mkdir ~/output chmod 777 ~/output 20.2.3.3 Installing BETYdb Web Application There are two flavors of BETY, PHP and RUBY. The PHP version allows for a minimal interaction with the database while the RUBY version allows for full interaction with the database. 20.2.3.3.1 PHP version The php version comes with PEcAn and is already configured. 20.2.3.3.2 RUBY version The RUBY version requires a few extra packages to be installed first. Next we install the web app. # install bety cd git clone https://github.com/PecanProject/bety.git # install gems cd bety sudo gem2.0 install bundler bundle install --without development:test:javascript_testing:debug and configure BETY # create folders for upload folders mkdir paperclip/files paperclip/file_names chmod 777 paperclip/files paperclip/file_names # create folder for log files mkdir log touch log/production.log chmod 0666 log/production.log # fix configuration for vm cp config/additional_environment_vm.rb config/additional_environment.rb chmod go+w public/javascripts/cache/ # setup bety database configuration cat &gt; config/database.yml &lt;&lt; EOF production: adapter: postgis encoding: utf-8 reconnect: false database: bety pool: 5 username: bety password: bety EOF # setup login tokens cat &gt; config/initializers/site_keys.rb &lt;&lt; EOF REST_AUTH_SITE_KEY = &#39;thisisnotasecret&#39; REST_AUTH_DIGEST_STRETCHES = 10 EOF 20.2.4 Install Models This page contains instructions on how to download and install ecosystem models that have been or are being coupled to PEcAn. These instructions have been tested on the PEcAn unbuntu VM. Commands may vary on other operating systems. Also, some model downloads require permissions before downloading, making them unavailable to the general public. Please contact the PEcAn team if you would like access to a model that is not already installed on the default PEcAn VM. BioCro CLM 4.5 DALEC ED2 FATES GDAY JULES LINKAGES LPJ-GUESS MAESPA SIPNET 20.2.4.1 BioCro # Public echo &#39;devtools::install_github(&quot;ebimodeling/biocro&quot;)&#39; | R --vanilla # Development: echo &#39;devtools::install_github(&quot;ebimodeling/biocro-dev&quot;)&#39; | R --vanilla BioCro Developers: request from [@dlebauer on GitHub](https://github.com/dlebauer) 20.2.4.2 CLM 4.5 The version of CLM installed on PEcAn is the ORNL branch provided by Dan Ricciuto. This version includes Dan’s point-level CLM processing scripts Download the code (~300M compressed), input data (1.7GB compressed and expands to 14 GB), and a few misc inputs. mkdir models cd models wget ftp://nacp.ornl.gov/synthesis/2008/firenze/site/clm4_5_1_r085.tar.gz wget ftp://nacp.ornl.gov/synthesis/2008/firenze/site/clm/ccsm_inputdata.tar.gz tar -xvzf clm4_5* tar -xvzf ccsm_inputdata.tar.gz #Parameter file: mkdir /home/carya/models/ccsm_inputdata/lnd/clm2/paramdata cd /home/carya/models/ccsm_inputdata/lnd/clm2/paramdata wget ftp://nacp.ornl.gov/synthesis/2008/firenze/site/clm_params.c130821.nc wget ftp://nacp.ornl.gov/synthesis/2008/firenze/site/clm_params.c140423.nc #Domain file: cd /home/carya/models/ccsm_inputdata/share/domains/domain.clm/ wget ftp://nacp.ornl.gov/synthesis/2008/firenze/site/domain.lnd.1x1pt_US-UMB_navy.nc #Aggregated met data file: cd /home/carya/models/ccsm_inputdata/atm/datm7/CLM1PT_data/1x1pt_US-UMB wget ftp://nacp.ornl.gov/synthesis/2008/firenze/site/all_hourly.nc ## lightning database cd /home/carya/models/ccsm_inputdata/atm/datm7/NASA_LIS/ wget ftp://nacp.ornl.gov/synthesis/2008/firenze/site/clmforc.Li_2012_climo1995-2011.T62.lnfm_Total_c140423.nc ## surface data cd /home/carya/models/ccsm_inputdata/lnd/clm2/surfdata wget ftp://nacp.ornl.gov/synthesis/2008/firenze/site/clm/surfdata_360x720cru_simyr1850_c130927.nc cd /home/carya/models/ccsm_inputdata/lnd/clm2/surfdata_map wget ftp://nacp.ornl.gov/synthesis/2008/firenze/site/clm/surfdata_1x1pt_US-UMB_I1850CLM45CN_simyr1850.nc_new mv surfdata_1x1pt_US-UMB_I1850CLM45CN_simyr1850.nc_new surfdata_1x1pt_US-UMB_I1850CLM45CN_simyr1850.nc Required libraries sudo apt-get install mercurial csh tcsh subversion cmake sudo ln -s /usr/bin/make /usr/bin/gmake Compile and build default inputs cd ~/carya/models/clm4_5_1_r085/scripts python runCLM.py --site US-UMB ––compset I1850CLM45CN --mach ubuntu --ccsm_input /home/carya/models/ccsm_inputdata --tstep 1 --nopointdata --coldstart --cpl_bypass --clean_build 20.2.4.2.1 CLM Test Run You will see a new directory in scripts: US-UMB_I1850CLM45CN Enter this directory and run (you shouldn’t have to do this normally, but there is a bug with the python script and doing this ensures all files get to the right place): ./US-UMB_I1850CLM45CN.build Next you are ready to go to the run directory: /home/carya/models/clm4_5_1_r085/run/US-UMB_I1850CLM45CN/run Open to edit file: datm.streams.txt.CLM1PT.CLM_USRDAT and check file paths such that all paths start with /home/carya/models/ccsm_inputdata From this directory, launch the executable that resides in the bld directory: /home/carya/clm4_5_1_r085/run/US-UMB_I1850CLM45CN/bld/cesm.exe not sure this was the right location, but wherever the executable is You should begin to see output files that look like this: US-UMB_I1850CLM45CN.clm2.h0.yyyy-mm.nc (yyyy is year, mm is month) These are netcdf files containing monthly averages of lots of variables. The lnd_in file in the run directory can be modified to change the output file frequency and variables. 20.2.4.3 DALEC cd curl -o dalec_EnKF_pub.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/dalec_EnKF_pub.tgz tar zxf dalec_EnKF_pub.tgz rm dalec_EnKF_pub.tgz cd dalec_EnKF_pub make dalec_EnKF make dalec_seqMH sudo cp dalec_EnKF dalec_seqMH /usr/local/bin 20.2.4.4 ED2 20.2.4.4.1 ED2.2 r46 (used in PEcAn manuscript) # ---------------------------------------------------------------------- # Get version r46 with a few patches for ubuntu cd curl -o ED.r46.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/ED.r46.tgz tar zxf ED.r46.tgz rm ED.r46.tgz # ---------------------------------------------------------------------- # configure and compile ed cd ~/ED.r46/ED/build/bin curl -o include.mk.VM http://isda.ncsa.illinois.edu/~kooper/EBI/include.mk.opt.`uname -s` make OPT=VM sudo cp ../ed_2.1-VM /usr/local/bin/ed2.r46 Perform a test run using pre configured ED settings for ED2.2 r46 # ---------------------------------------------------------------------- # Create sample run cd mkdir testrun.ed.r46 cd testrun.ed.r46 curl -o ED2IN http://isda.ncsa.illinois.edu/~kooper/EBI/ED2IN.r46 sed -i -e &quot;s#\\$HOME#$HOME#&quot; ED2IN curl -o config.xml http://isda.ncsa.illinois.edu/~kooper/EBI/config.r46.xml # execute test run time ed2.r46 20.2.4.4.2 ED 2.2 r82 cd curl -o ED.r82.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/ED.r82.tgz tar zxf ED.r82.tgz rm ED.r82.tgz cd ED.r82 curl -o ED.r82.patch http://isda.ncsa.illinois.edu/~kooper/EBI/ED.r82.patch patch -p1 &lt; ED.r82.patch cd ED/build/bin curl -o include.mk.VM http://isda.ncsa.illinois.edu/~kooper/EBI/include.mk.opt.`uname -s` make OPT=VM sudo cp ../ed_2.1-VM /usr/local/bin/ed2.r82 Perform a test run using pre configured ED settings for ED2.2 r82 cd mkdir testrun.ed.r82 cd testrun.ed.r82 curl -o ED2IN http://isda.ncsa.illinois.edu/~kooper/EBI/ED2IN.r82 sed -i -e &quot;s#\\$HOME#$HOME#&quot; ED2IN curl -o config.xml http://isda.ncsa.illinois.edu/~kooper/EBI/config.r82.xml # execute test run time ed2.r82 20.2.4.4.3 ED 2.2 bleeding edge cd git clone https://github.com/EDmodel/ED2.git cd ED2/ED/build/bin curl -o include.mk.VM http://isda.ncsa.illinois.edu/~kooper/EBI/include.mk.opt.`uname -s` ./generate_deps.sh make OPT=VM sudo cp ../ed_2.1-VM /usr/local/bin/ed2.git 20.2.4.5 CLM-FATES Prerequisites sudo apt-get upgrade libnetcdf-dev sudo apt-get install subversion sudo apt-get install csh sudo apt-get install cmake sudo ln -s /usr/bin/make /usr/bin/gmake sudo rm /bin/sh sudo ln -s /bin/bash /bin/sh wget https://github.com/Unidata/netcdf-fortran/archive/v4.4.4.tar.gz cd netcdf-4.4.4 ./configure make sudo make install you might need to mess around with installing netcdf and netcdf-fortran to get a version FATES likes… Get code from Github (currently private) and go to cime/scripts directory git clone git@github.com:NGEET/ed-clm.git cd ed-clm/cime/scripts/ Within CLM-FATES, to be able to build an executable we need to create a reference run. We’ll also use this reference run to grab defaults from, so we’ll be registering the location of both the reference case (location of executable, scripts, etc) and the reference inputs with the PEcAn database. To begin, copy reference run script from pecan cp ~/pecan/models/fates/inst/create_1x1_ref_case.sh . Edit reference case script to set NETCDF_HOME, CROOT (reference run case), DIN_LOC_ROOT (reference run inputs). Also, make sure DIN_LOC_ROOT exists as FATES will not create it itself. Then run the script ./create_1x1_ref_case.sh Be aware that this script WILL ask you for your password on the NCAR server to download the reference case input data (the guest password may work, haven’t tried this). If it gives an error at the pio stage check the log, but the most likely error is it being unable to find a version of netcdf it likes. Once FATES is installed, set the whole reference case directory as the Model path (leave filename blank) and set the whole inputs directory as an Input with format clm_defaults. 20.2.4.6 GDAY Navigate to a directory you would like to store GDAY and run the following: git clone https://github.com/mdekauwe/GDAY.git cd GDAY cd src make gday is your executable. 20.2.4.7 JULES INSTALL STEPS: 1) Download JULES and FCM JULES: Model requires registration to download. Not to be put on PEcAn VM Registration: https://jules.jchmr.org/software-and-documentation Documentation: http://jules-lsm.github.io/vn4.2/index.html FCM: https://github.com/metomi/fcm/ wget https://github.com/metomi/fcm/archive/2015.05.0.tar.gz edit makefile ```bash open etc/fcm-make/make.cfg set JULES_NETCDF = actual instead of dummy set path (e.g. /usr/) and lib_path /lib64 to netCDF libraries ``` compile JULES cd etc/fcm-make/ {path.to.fcm}/fcm make -f etc/fcm-make/make.cfg --new UBUNTU VERSION: installed without having to add any perl libraries #perl stuff that I had to install on pecan2 not PEcAN VM sudo yum install perl-Digest-SHA sudo yum install perl-Time-modules sudo yum install cpan curl -L http://cpanmin.us | perl - --sudo App::cpanminus sudo cpanm Time/Piece.pm sudo cpanm IO/Uncompress/Gunzip.pm Executable is under build/bin/jules.exe Example rundir: examples/point_loobos 20.2.4.8 LINKAGES 20.2.4.8.1 R Installation # Public echo &#39;devtools::install_github(&quot;araiho/linkages_package&quot;)&#39; | R --vanilla 20.2.4.8.2 FORTRAN VERSION #FORTRAN VERSION cd git clone https://github.com/araiho/Linkages.git cd Linkages gfortran -o linkages linkages.f sudo cp linkages /usr/local/bin/linkages.git 20.2.4.9 LPJ-GUESS Instructions to download source code Go to LPJ-GUESS website for instructions to access code. 20.2.4.10 MAESPA Navigate to a directory you would like store MAESPA and run the following: git clone https://bitbucket.org/remkoduursma/maespa.git cd maespa make maespa.out is your executable. Example input files can be found in the inpufiles directory. Executing measpa.out from within one of the example directories will produce output. MAESPA developers have also developed a wrapper package called Maeswrap. The usual R package installation method install.packages may present issues with downloading an unpacking a dependency package called rgl. Here are a couple of solutions: 20.2.4.10.1 Solution 1 ### From the Command Line sudo apt-get install r-cran-rgl then from within R install.packages(&quot;Maeswrap&quot;) 20.2.4.10.2 Solution 2 ### From the Command line sudo apt-get install libglu1-mesa-dev then from within R install.packages(&quot;Maeswrap&quot;) 20.2.4.11 SIPNET cd curl -o sipnet_unk.tar.gz http://isda.ncsa.illinois.edu/~kooper/EBI/sipnet_unk.tar.gz tar zxf sipnet_unk.tar.gz rm sipnet_unk.tar.gz cd sipnet_unk make sudo cp sipnet /usr/local/bin/sipnet.runk 20.2.4.11.1 SIPNET testrun cd curl -o testrun.sipnet.tar.gz http://isda.ncsa.illinois.edu/~kooper/EBI/testrun.sipnet.tar.gz tar zxf testrun.sipnet.tar.gz rm testrun.sipnet.tar.gz cd testrun.sipnet sipnet.runk 20.2.5 Download and Compile PEcAn Set R_LIBS_USER CRAN Reference # point R to personal lib folder echo &#39;export R_LIBS_USER=${HOME}/R/library&#39; &gt;&gt; ~/.profile source ~/.profile mkdir -p ${R_LIBS_USER} 20.2.5.1 Download, compile and install PEcAn from GitHub # download pecan cd git clone https://github.com/PecanProject/pecan.git # compile pecan cd pecan ./scripts/build.sh --dependencies Following will run a small script to setup some hooks to prevent people from using the pecan demo user account to check in any code. # prevent pecan user from checking in code ./scripts/create-hooks.sh 20.2.6 PEcAn Testrun Do the run, this assumes you have installed the BETY database, sites tar file and SIPNET. # create folder cd mkdir testrun.pecan cd testrun.pecan # copy example of pecan workflow and configuration file cp ../pecan/tests/pecan32.sipnet.xml pecan.xml cp ../pecan/scripts/workflow.R workflow.R # exectute workflow rm -rf pecan ./workflow.R pecan.xml NB: pecan.xml is configured for the virtual machine, you will need to change the field from ‘/home/carya/’ to wherever you installed your ‘sites’, usually $HOME 20.2.7 PEcAn Customizations 20.2.7.1 PalEON version of PEcAn Install the following packages: gnome2, xfce4, firefox sudo apt-get -y install gdm gnome-shell xfce4 firefox Install Rstudio-desktop (+ libjpeg62 seems to be a dependency) PROC=$( uname -m | sed -e &#39;s/x86_64/amd64/&#39; -e &#39;s/i686/i386/&#39; ) wget http://download1.rstudio.org/rstudio-0.98.507-${PROC}.deb sudo apt-get install libjpeg62 sudo dpkg -i rstudio-*.deb rm rstudio-*.deb Additional pieces of software wget http://chrono.qub.ac.uk/blaauw/LinBacon_2.2.zip unzip LinBacon_2.2.zip rm LinBacon_2.2.zip wget http://chrono.qub.ac.uk/blaauw/clam.zip unzip clam.zip rm clam.zip Additional R packages: cat &lt;&lt; EOF | R --vanilla list.of.packages &lt;- c(&#39;R2jags&#39;, &#39;RCurl&#39;, &#39;RJSONIO&#39;, &#39;reshape2&#39;, &#39;plyr&#39;, &#39;fields&#39;, &#39;maps&#39;, &#39;maptools&#39;, &#39;ggplot2&#39;, &#39;mvtnorm&#39;, &#39;devtools&#39;) new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] if(length(new.packages)) { print(&quot;installing : &quot;) print(new.packages) install.packages(new.packages, repos=&quot;http://cran.rstudio.com/&quot;) } require(devtools) install_github(&quot;neotoma&quot;, &quot;ropensci&quot;) EOF 20.3 AWS Setup 20.3.1 Porting VM to AWS The following are Mike’s rough notes from a first attempt to port the PEcAn VM to the AWS. This was done on a Mac These notes are based on following the instructions here 20.3.1.1 Convert PEcAn VM AWS allows upload of files as VMDK but the default PEcAn VM is in OVA format If you haven’t done so already, download the PEcAn VM Split the OVA file into OVF and VMDK files tar xf &lt;ovafile&gt; 20.3.1.2 Set up an account on AWS After you have an account you need to set up a user and save your access key and secret key In my case I created a user named ‘carya’ Note: the key that ended up working had to be made at https://console.aws.amazon.com/iam/home#security_credential, not the link above. 20.3.1.3 Install EC2 command line tools wget http://s3.amazonaws.com/ec2-downloads/ec2-api-tools.zip sudo mkdir /usr/local/ec2 sudo unzip ec2-api-tools.zip -d /usr/local/ec2 If need be, download and install JDK export JAVA_HOME=$(/usr/libexec/java_home) export EC2_HOME=/usr/local/ec2/ec2-api-tools-&lt;version&gt; export PATH=$PATH:$EC2_HOME/bin Then set your user credentials as environment variables: export AWS_ACCESS_KEY=xxxxxxxxxxxxxx export AWS_SECRET_KEY=xxxxxxxxxxxxxxxxxxxxxx Note: you may want to add all the variables set in the above EXPORT commands above into your .bashrc or equivalent. 20.3.1.4 Create an AWS S3 ‘bucket’ to upload VM to Go to https://console.aws.amazon.com/s3 and click “Create Bucket” In my case I named the bucket ‘pecan’ 20.3.1.5 Upload In the code below, make sure to change the PEcAn version, the name of the bucket, and the name of the region. Make sure that the architecture matches the version of PEcAn you downloaded (i386 for 32 bit, x86_64 for 64 bit). Also, you may want to choose a considerably larger instance type. The one chosen below is that corresponding to the AWS Free Tier ec2-import-instance PEcAn32bit_1.2.6-disk1.vmdk --instance-type t2.micro --format VMDK --architecture i386 --platform Linux --bucket pecan --region us-east-1 --owner-akid $AWS_ACCESS_KEY --owner-sak $AWS_SECRET_KEY Make sure to note the ID of the image since you’ll need it to check the VM status. Once the image is uploaded it will take a while (typically about an hour) for Amazon to convert the image to one it can run. You can check on this progress by running ec2-describe-conversion-tasks &lt;image.ID&gt; 20.3.1.6 Configuring the VM On the EC2 management webpage, https://console.aws.amazon.com/ec2, if you select Instances on the left hand side (LHS) you should be able to see your new PEcAn image as an option under Launch Instance. Before launching, you will want to update the firewall to open up additional ports that PEcAn needs – specifically port 80 for the webpage. Port 22 (ssh/sftp) should be open by default. Under “Security Groups” select “Inbound” then “Edit” and then add “HTTP”. Select “Elastic IPs” on the LHS, and “Allocate New Address” in order to create a public IP for your VM. Next, select “Network Interfaces” on the LHS and then under Actions select “Associate Addresses” then choose the Elastic IP you just created. See also http://docs.aws.amazon.com/AmazonVPC/latest/GettingStartedGuide/GetStarted.html 20.3.2 Set up multiple instances (optional) For info on setting up multiple instances with load balancing see: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/gs-ec2VPC.html Select “Load Balancers” on the LHS, click on “Create Load Balancer”, follow Wizard keeping defaults. To be able to launch multiple VMs: Under “Instances” convert VM to an Image. When done, select Launch, enable multiple instances, and associate with the previous security group. Once running, go back to “Load Balancers” and add the instances to the load balancer. Each instance can be accessed individually by it’s own public IP, but external users should access the system more generally via the Load Balancers DNS. 20.3.2.1 Booting the VM Return to “Instances” using the menu on the LHS. To boot the VM select “Actions” then “Instance State” then “Start”. In the future, once you have the VM loaded and configured this last step is the only one you will need to repeat to turn your VM on and off. The menu provided should specify the Public IP where the VM has launched 20.4 Shiny Setup Installing and configuring Shiny for PEcAn authors - Alexey Shiklomanov - Rob Kooper NOTE: Instructions are only tested for CentOS 6.5 and Ubuntu 16.04 NOTE: Pretty much every step here requires root access. 20.4.1 Install the Shiny R package and Shiny server Follow the instructions on the Shiny download page for the operating system you are using. 20.4.2 Modify the shiny configuration file The Shiny configuration file is located in /etc/shiny-server/shiny-server.conf. Comment out the entire file and add the following, replacing &lt;username&gt; with your user name and &lt;location&gt; with the URL location you want for your app. This will allow you to run Shiny apps from your web browser at https://your.server.edu/shiny/your-location run as shiny; server { listen 3838; location /&lt;location&gt;/ { run as &lt;username&gt;; site_dir /path/to/your/shiny/app; log_dir /var/log/shiny-server; directory_index on; } } For example, my configuration on the old test-pecan looks like this. run as shiny; server { listen 3838; location /ashiklom/ { run as ashiklom; site_dir /home/ashiklom/fs-data/pecan/shiny/; log_dir /var/log/shiny-server; directory_index on; } } …and I can access my Shiny apps at, for instance, https://test-pecan.bu.edu/shiny/ashiklom/workflowPlots. You can add as many location &lt;loc&gt; { ... } fields as you would like. run as shiny; server { listen 3838; location /ashiklom/ { ... } location /bety/ { ... } } If you change the configuration, for example to add a new location, you will need to restart Shiny server. If you are setting up a new instance of Shiny, skip this step and continue with the guide, since there are a few more steps to get Shiny working. If there is an instance of Shiny already running, you can restart it with: ## On CentOS sudo service shiny-server stop sudo service shiny-server start ## On Ubuntu sudo systemctl stop shiny-server.service sudo systemctl start shiny-server.service 20.4.3 Set the Apache proxy Create a file with the following name, based on the version of the operating system you are using: Ubuntu 16.04 (pecan1, pecan2, test-pecan) – /etc/apache2/conf-available/shiny.conf CentOS 6.5 (psql-pecan) – /etc/httpd/conf.d/shiny.conf Into this file, add the following: ProxyPass /shiny/ http://localhost:3838/ ProxyPassReverse /shiny/ http://localhost:3838/ RedirectMatch permanent ^/shiny$ /shiny/ 20.4.3.1 Ubuntu only: Enable the new shiny configuration sudo a2enconf shiny This will create a symbolic link to the newly created shiny.conf file inside the /etc/apache2/conf-enabled directory. You can do ls -l /etc/apache2/conf-enabled to confirm that this worked. 20.4.4 Enable and start the shiny server, and restart apache 20.4.4.1 On CentOS sudo ln -s /opt/shiny-server/config/init.d/redhat/shiny-server /etc/init.d sudo service shiny-server stop sudo service shiny-server start sudo service httpd restart You can check that Shiny is running with service shiny-server status. 20.4.4.2 On Ubuntu Enable the Shiny server service. This will make sure Shiny runs automatically on startup. sudo systemctl enable shiny-server.service Restart Apache. sudo apachectl restart Start the Shiny server. sudo systemctl start shiny-server.service If there are problems, you can stop the shiny-server.service with… sudo systemctl stop shiny-server.service …and then use start again to restart it. 20.4.5 Troubleshooting Refer to the log files for shiny (/var/log/shiny-server.log) and httpd (on CentOS, /var/log/httpd/error-log; on Ubuntu, /var/log/apache2/error-log). 20.4.6 Further reading Shiny server configuration reference 20.5 Thredds Setup Installing and configuring Thredds for PEcAn authors - Rob Kooper NOTE: Instructions are only tested for Ubuntu 16.04 on the VM, if you have instructions for CENTOS/RedHat please update this documentation NOTE: Pretty much every step here requires root access. 20.5.1 Install the Tomcat 8 and Thredds webapp The Tomcat 8 server can be installed from the default Ubuntu repositories. The thredds webapp will be downloaded and installed from unidata. 20.5.2 Ubuntu First step is to install Tomcat 8 and configure it. The flag -Dtds.content.root.path should point to the location of where the thredds folder is located. This needs to be writeable by the user for tomcat. -Djava.security.egd is a special flag to use a different random number generator for tomcat. The default would take to long to generate a random number. apt-get -y install tomcat8 openjdk-8-jdk echo JAVA_OPTS=\\&quot;-Dtds.content.root.path=/home/carya \\${JAVA_OPTS}\\&quot; &gt;&gt; /etc/default/tomcat8 echo JAVA_OPTS=\\&quot;-Djava.security.egd=file:/dev/./urandom \\${JAVA_OPTS}\\&quot; &gt;&gt; /etc/default/tomcat8 service tomcat8 restart Next is to install the webapp. mkdir /home/carya/thredds chmod 777 /home/carya/thredds wget -O /var/lib/tomcat8/webapps/thredds.war ftp://ftp.unidata.ucar.edu/pub/thredds/4.6/current/thredds.war Finally we configure Apache to prox the thredds server cat &gt; /etc/apache2/conf-available/thredds.conf &lt;&lt; EOF ProxyPass /thredds/ http://localhost:8080/thredds/ ProxyPassReverse /thredds/ http://localhost:8080/thredds/ RedirectMatch permanent ^/thredds$ /thredds/ EOF a2enmod proxy_http a2enconf thredds service apache2 reload 20.5.3 Customize the Thredds server To customize the thredds server for your installation edit the file in /home/carya/thredds/threddsConfig.xml. For example the following file is included in the VM. &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;threddsConfig&gt; &lt;!-- all options are commented out in standard install - meaning use default values --&gt; &lt;!-- see http://www.unidata.ucar.edu/software/thredds/current/tds/reference/ThreddsConfigXMLFile.html --&gt; &lt;serverInformation&gt; &lt;name&gt;PEcAn&lt;/name&gt; &lt;logoUrl&gt;/pecan/images/pecan_small.jpg&lt;/logoUrl&gt; &lt;logoAltText&gt;PEcAn&lt;/logoAltText&gt; &lt;abstract&gt;Scientific Data&lt;/abstract&gt; &lt;keywords&gt;meteorology, atmosphere, climate, ocean, earth science&lt;/keywords&gt; &lt;contact&gt; &lt;name&gt;Rob Kooper&lt;/name&gt; &lt;organization&gt;NCSA&lt;/organization&gt; &lt;email&gt;kooper@illinois.edu&lt;/email&gt; &lt;!--phone&gt;&lt;/phone--&gt; &lt;/contact&gt; &lt;hostInstitution&gt; &lt;name&gt;PEcAn&lt;/name&gt; &lt;webSite&gt;http://www.pecanproject.org/&lt;/webSite&gt; &lt;logoUrl&gt;/pecan/images/pecan_small.jpg&lt;/logoUrl&gt; &lt;logoAltText&gt;PEcAn Project&lt;/logoAltText&gt; &lt;/hostInstitution&gt; &lt;/serverInformation&gt; &lt;!-- The &lt;catalogRoot&gt; element: For catalogs you don&#39;t want visible from the /thredds/catalog.xml chain of catalogs, you can use catalogRoot elements. Each catalog root config catalog is crawled and used in configuring the TDS. &lt;catalogRoot&gt;myExtraCatalog.xml&lt;/catalogRoot&gt; &lt;catalogRoot&gt;myOtherExtraCatalog.xml&lt;/catalogRoot&gt; --&gt; &lt;!-- * Setup for generated HTML pages. * * NOTE: URLs may be absolute or relative, relative URLs must be relative * to the webapp URL, i.e., http://server:port/thredds/. --&gt; &lt;htmlSetup&gt; &lt;!-- * CSS documents used in generated HTML pages. * The CSS document given in the &quot;catalogCssUrl&quot; element is used for all pages * that are HTML catalog views. The CSS document given in the &quot;standardCssUrl&quot; * element is used in all other generated HTML pages. * --&gt; &lt;standardCssUrl&gt;tds.css&lt;/standardCssUrl&gt; &lt;catalogCssUrl&gt;tdsCat.css&lt;/catalogCssUrl&gt; &lt;openDapCssUrl&gt;tdsDap.css&lt;/openDapCssUrl&gt; &lt;!-- * The Google Analytics Tracking code you would like to use for the * webpages associated with THREDDS. This will not track WMS or DAP * requests for data, only browsing the catalog. --&gt; &lt;googleTrackingCode&gt;&lt;/googleTrackingCode&gt; &lt;/htmlSetup&gt; &lt;!-- The &lt;TdsUpdateConfig&gt; element controls if and how the TDS checks for updates. The default is for the TDS to check for the current stable and development release versions, and to log that information in the TDS serverStartup.log file as INFO entries. &lt;TdsUpdateConfig&gt; &lt;logVersionInfo&gt;true&lt;/logVersionInfo&gt; &lt;/TdsUpdateConfig&gt; --&gt; &lt;!-- The &lt;CORS&gt; element controls Cross-Origin Resource Sharing (CORS). CORS is a way to allow a website (such as THREDDS) to open up access to resources to web pages and applications running on a different domain. One example would be allowing a web-application to use fonts from a separate host. For TDS, this can allow a javascript app running on a different site to access data on a THREDDS server. For more information see: https://en.wikipedia.org/wiki/Cross-origin_resource_sharing The elements below represent defaults. Only the &lt;enabled&gt; tag is required to enable CORS. The default allowed origin is &#39;*&#39;, which allows sharing to any domain. &lt;CORS&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;maxAge&gt;1728000&lt;/maxAge&gt; &lt;allowedMethods&gt;GET&lt;/allowedMethods&gt; &lt;allowedHeaders&gt;Authorization&lt;/allowedHeaders&gt; &lt;allowedOrigin&gt;*&lt;/allowedOrigin&gt; &lt;/CORS&gt; --&gt; &lt;!-- The &lt;CatalogServices&gt; element: - Services on local TDS served catalogs are always on. - Services on remote catalogs are set with the allowRemote element below. They are off by default (recommended). --&gt; &lt;CatalogServices&gt; &lt;allowRemote&gt;false&lt;/allowRemote&gt; &lt;/CatalogServices&gt; &lt;!-- Configuring the CDM (netcdf-java library) see http://www.unidata.ucar.edu/software/netcdf-java/reference/RuntimeLoading.html &lt;nj22Config&gt; &lt;ioServiceProvider class=&quot;edu.univ.ny.stuff.FooFiles&quot;/&gt; &lt;coordSysBuilder convention=&quot;foo&quot; class=&quot;test.Foo&quot;/&gt; &lt;coordTransBuilder name=&quot;atmos_ln_sigma_coordinates&quot; type=&quot;vertical&quot; class=&quot;my.stuff.atmosSigmaLog&quot;/&gt; &lt;typedDatasetFactory datatype=&quot;Point&quot; class=&quot;gov.noaa.obscure.file.Flabulate&quot;/&gt; &lt;/nj22Config&gt; --&gt; &lt;!-- CDM uses the DiskCache directory to store temporary files, like uncompressed files. &lt;DiskCache&gt; &lt;alwaysUse&gt;false&lt;/alwaysUse&gt; &lt;scour&gt;1 hour&lt;/scour&gt; &lt;maxSize&gt;1 Gb&lt;/maxSize&gt; &lt;/DiskCache&gt; --&gt; &lt;!-- Caching open NetcdfFile objects. default is to allow 50 - 100 open files, cleanup every 11 minutes &lt;NetcdfFileCache&gt; &lt;minFiles&gt;50&lt;/minFiles&gt; &lt;maxFiles&gt;100&lt;/maxFiles&gt; &lt;scour&gt;11 min&lt;/scour&gt; &lt;/NetcdfFileCache&gt; --&gt; &lt;!-- The &lt;HTTPFileCache&gt; element: allow 10 - 20 open datasets, cleanup every 17 minutes used by HTTP Range requests. &lt;HTTPFileCache&gt; &lt;minFiles&gt;10&lt;/minFiles&gt; &lt;maxFiles&gt;20&lt;/maxFiles&gt; &lt;scour&gt;17 min&lt;/scour&gt; &lt;/HTTPFileCache&gt; --&gt; &lt;!-- Writing GRIB indexes. &lt;GribIndexing&gt; &lt;setExtendIndex&gt;false&lt;/setExtendIndex&gt; &lt;alwaysUseCache&gt;false&lt;/alwaysUseCache&gt; &lt;/GribIndexing&gt; --&gt; &lt;!-- Persist joinNew aggregations to named directory. scour every 24 hours, delete stuff older than 90 days &lt;AggregationCache&gt; &lt;scour&gt;24 hours&lt;/scour&gt; &lt;maxAge&gt;90 days&lt;/maxAge&gt; &lt;cachePathPolicy&gt;NestedDirectory&lt;/cachePathPolicy&gt; &lt;/AggregationCache&gt; --&gt; &lt;!-- How to choose the template dataset for an aggregation. latest, random, or penultimate &lt;Aggregation&gt; &lt;typicalDataset&gt;penultimate&lt;/typicalDataset&gt; &lt;/Aggregation&gt; --&gt; &lt;!-- The Netcdf Subset Service is off by default. &lt;NetcdfSubsetService&gt; &lt;allow&gt;false&lt;/allow&gt; &lt;scour&gt;10 min&lt;/scour&gt; &lt;maxAge&gt;-1 min&lt;/maxAge&gt; &lt;/NetcdfSubsetService&gt; --&gt; &lt;!-- &lt;Opendap&gt; &lt;ascLimit&gt;50&lt;/ascLimit&gt; &lt;binLimit&gt;500&lt;/binLimit&gt; &lt;serverVersion&gt;opendap/3.7&lt;/serverVersion&gt; &lt;/Opendap&gt; --&gt; &lt;!-- The WCS Service is off by default. Also, off by default (and encouraged) is operating on a remote dataset. &lt;WCS&gt; &lt;allow&gt;false&lt;/allow&gt; &lt;allowRemote&gt;false&lt;/allowRemote&gt; &lt;scour&gt;15 min&lt;/scour&gt; &lt;maxAge&gt;30 min&lt;/maxAge&gt; &lt;/WCS&gt; --&gt; &lt;!-- &lt;WMS&gt; &lt;allow&gt;false&lt;/allow&gt; &lt;allowRemote&gt;false&lt;/allowRemote&gt; &lt;maxImageWidth&gt;2048&lt;/maxImageWidth&gt; &lt;maxImageHeight&gt;2048&lt;/maxImageHeight&gt; &lt;/WMS&gt; --&gt; &lt;!-- &lt;NCISO&gt; &lt;ncmlAllow&gt;false&lt;/ncmlAllow&gt; &lt;uddcAllow&gt;false&lt;/uddcAllow&gt; &lt;isoAllow&gt;false&lt;/isoAllow&gt; &lt;/NCISO&gt; --&gt; &lt;!-- CatalogGen service is off by default. &lt;CatalogGen&gt; &lt;allow&gt;false&lt;/allow&gt; &lt;/CatalogGen&gt; --&gt; &lt;!-- DLwriter service is off by default. As is support for operating on remote catalogs. &lt;DLwriter&gt; &lt;allow&gt;false&lt;/allow&gt; &lt;allowRemote&gt;false&lt;/allowRemote&gt; &lt;/DLwriter&gt; --&gt; &lt;!-- DqcService is off by default. &lt;DqcService&gt; &lt;allow&gt;false&lt;/allow&gt; &lt;/DqcService&gt; --&gt; &lt;!-- Link to a Viewer application on the HTML page: &lt;Viewer&gt;my.package.MyViewer&lt;/Viewer&gt; --&gt; &lt;!-- Add a DataSource - essentially an IOSP with access to Servlet request parameters &lt;datasetSource&gt;my.package.DatsetSourceImpl&lt;/datasetSource&gt; --&gt; &lt;!-- set FeatureCollection logging &lt;FeatureCollection&gt; &lt;RollingFileAppender&gt; &lt;MaxFileSize&gt;1 MB&lt;/MaxFileSize&gt; &lt;MaxBackups&gt;5&lt;/MaxBackups&gt; &lt;Level&gt;INFO&lt;/Level&gt; &lt;/RollingFileAppender&gt; &lt;/FeatureCollection&gt; --&gt; &lt;!-- Configure how the NetCDF-4 C library is discovered and used. libraryPath: The directory in which the native library is installed. libraryName: The name of the native library. This will be used to locate the proper .DLL, .SO, or .DYLIB file within the libraryPath directory. useForReading: By default, the native library is only used for writing NetCDF-4 files; a pure-Java layer is responsible for reading them. However, if this property is set to true, then it will be used for reading NetCDF-4 (and HDF5) files as well. --&gt; &lt;!-- &lt;Netcdf4Clibrary&gt; &lt;libraryPath&gt;/usr/local/lib&lt;/libraryPath&gt; &lt;libraryName&gt;netcdf&lt;/libraryName&gt; &lt;useForReading&gt;false&lt;/useForReading&gt; &lt;/Netcdf4Clibrary&gt; --&gt; &lt;/threddsConfig&gt; 20.5.4 Update the catalog For example to update the catalog with the latest data, run the following command from the root crontab. This cronjob will also synchronize the database with remote servers and dump your database (by default in /home/carya/dump) 0 * * * * /home/carya/pecan/scripts/cron.sh -o /home/carya/dump 20.5.5 Troubleshooting Refer to the log files for Tomcat (/var/log/tomcat8/*) and Thredds (/home/carya/thredds/logs). 20.5.6 Further reading Thredds reference "],
["maintaining-pecan.html", "21 Maintaining PEcAn 21.1 Updating PEcAn Code and Bety Database 21.2 Updating BETY 21.3 Database synchronization", " 21 Maintaining PEcAn This section will cover topics to keep your PEcAn instance up to date with the latest code and database. It will hopefully answer the question: How to do I keep my PEcAN up to date? Updating your code Updating BETY database Database-Synchronization Troubleshooting PEcAn 21.1 Updating PEcAn Code and Bety Database Release notes for all releases can be found here. This page will only list any steps you have to do to upgrade an existing system. When updating PEcAn it is highly encouraged to update BETY. You can find instructions on how to do this, as well on how to update the database in the Updating BETYdb gitbook page. 21.1.1 Updating PEcAn The latest version of PEcAn code can be obtained from the PEcAn repository on GitHub: cd pecan # If you are not already in the PEcAn directory git pull The PEcAn build system is based on GNU Make. The simplest way to install is to run make from inside the PEcAn directory. This will update the documentation for all packages and install them, as well as all required dependencies. For more control, the following make commands are available: make document – Use devtools::document to update the documentation for all package. Under the hood, this uses the roxygen2 documentation system. make install – Install all packages and their dependnencies using devtools::install. By default, this only installs packages that have had their code changed and any dependent packages. make check – Perform a rigorous check of packages using devtools::check make test – Run all unit tests (based on testthat package) for all packages, using devtools::test make clean – Remove the make build cache, which is used to track which packages have changed. Cache files are stored in the .doc, .install, .check, and .test subdirectories in the PEcAn main directory. Running make clean will force the next invocation of make commands to operate on all PEcAn packages, regardless of changes. The following are some additional make tricks that may be useful: Install, check, document, or test a specific package – make .&lt;cmd&gt;/&lt;pkg-dir&gt;; e.g. make .install/utils or make .check/modules/rtm Force make to run, even if package has not changed – make -B &lt;command&gt; Run make commands in parallel – make -j&lt;ncores&gt;; e.g. make -j4 install to install packages using four parallel processes. All instructions for the make build system are contained in the Makefile in the PEcAn root directory. For full documentation on make, see the man pages by running man make from a terminal. Point of contact: Alexey Shiklomanov GitHub/Gitter: @ashiklom email: ashiklom@bu.edu 21.2 Updating BETY Moved to: https://pecan.gitbooks.io/betydb-documentation/content/updating_betydb_when_new_versions_are_released.html 21.3 Database synchronization The database synchronization consists of 2 parts: - Getting the data from the remote servers to your server - Sharing your data with everybody else 21.3.1 How does it work? Each server that runs the BETY database will have a unique machine_id and a sequence of ID’s associated. Whenever the user creates a new row in BETY it will receive an ID in the sequence. This allows us to uniquely identify where a row came from. This is information is crucial for the code that works with the synchronization since we can now copy those rows that have an ID in the sequence specified. If you have not asked for a unique ID your ID will be 99. The synchronization code itself is split into two parts, loading data with the load.bety.sh script and exporting data using dump.bety.sh. If you do not plan to share data, you only need to use load.bety.sh to update your database. 21.3.2 Set up Requests for new machine ID’s is currently handled manually. To request a machine ID contact Rob Kooper kooper@illinois.edu. In the examples below this ID is referred to as MYSITE. To setup the database to use this ID you need to call load.bety in ‘CREATE’ mode sudo -u postgres {$PECAN}/scripts/load.bety.sh -c -u -m X WARNING: At the momment running CREATE deletes all current records in the database. If you are running from the VM this includes both all runs you have done and all information that the database is prepopulated with (e.g. input and model records). Remote records can be fetched (see below), but local records will be lost (we’re working on improving this!) 21.3.3 Fetch latest data When logged into the machine you can fetch the latest data using the load.bety.sh script. The script will check what site you want to get the data for and will remove all data in the database associated with that id. It will then reinsert all the data from the remote database. The script is configured using environment variables. The following variables are recognized: - DATABASE: the database where the script should write the results. The default is bety. - OWNER: the owner of the database (if it is to be created). The default is bety. - PG_OPT: additional options to be added to psql (default is nothing). - MYSITE: the (numerical) ID of your site. If you have not requested an ID, use 99; this is used for all sites that do not want to share their data (i.e. VM). 99 is in fact the default. - REMOTESITE: the ID of the site you want to fetch the data from. The default is 0 (EBI). - CREATE: If ‘YES’, this indicates that the existing database (bety, or the one specified by DATABASE) should be removed. Set to YES (in caps) to remove the database. THIS WILL REMOVE ALL DATA in DATABASE. The default is NO. - KEEPTMP: indicates whether the downloaded file should be preserved. Set to YES (in caps) to keep downloaded files; the default is NO. - USERS: determines if default users should be created. Set to YES (in caps) to create default users with default passwords. The default is NO. All of these variables can be specified as command line arguments, to see the options use -h. load.bety.sh -h ./scripts/load.bety.sh [-c YES|NO] [-d database] [-h] [-m my siteid] [-o owner] [-p psql options] [-r remote siteid] [-t YES|NO] [-u YES|NO] -c create database, THIS WILL ERASE THE CURRENT DATABASE, default is NO -d database, default is bety -h this help page -m site id, default is 99 (VM) -o owner of the database, default is bety -p additional psql command line options, default is empty -r remote site id, default is 0 (EBI) -t keep temp folder, default is NO -u create carya users, this will create some default users dump.bety.sh -h ./scripts/dump.bety.sh [-a YES|NO] [-d database] [-h] [-l 0,1,2,3,4] [-m my siteid] [-o folder] [-p psql options] [-u YES|NO] -a use anonymous user, default is YES -d database, default is bety -h this help page -l level of data that can be dumped, default is 3 -m site id, default is 99 (VM) -o output folder where dumped data is written, default is dump -p additional psql command line options, default is -U bety -u should unchecked data be dumped, default is NO 21.3.4 Sharing data Sharing your data requires a few steps. First, before entering any data, you will need to request an ID from the PEcAn developers. Simply open an issue at github and we will generate an ID for you. If possible, add the URL of your data host. You will now need to synchronize the database again and use your ID. For example if you are given ID=42 you can use the following command: MYID=42 REMOTEID=0 ./scripts/load.bety.sh. This will load the EBI database and set the ID’s such that any data you insert will have the right ID. To share your data you can now run the dump.bey.sh. The script is configured using environment variables, the following variables are recognized: - DATABASE: the database where the script should write the results. The default is bety. - PG_OPT: additional options to be added to psql (default is nothing). - MYSITE: the ID of your site. If you have not requested an ID, use 99, which is used for all sites that do not want to share their data (i.e. VM). 99 is the default. - LEVEL: the minimum access-protection level of the data to be dumped (0=private, 1=restricted, 2=internal collaborators, 3=external collaborators, 4=public). The default level for exported data is level 3. - note that currently only the traits and yields tables have restrictions on sharing. If you share data, records from other (meta-data) tables will be shared. If you wish to extend the access_level to other tables please submit a feature request. - UNCHECKED: specifies whether unchecked traits and yields be dumped. Set to YES (all caps) to dump unchecked data. The default is NO. - ANONYMOUS: specifies whether all users be anonymized. Set to YES (all caps) to keep the original users (INCLUDING PASSWORD) in the dump file. The default is NO. - OUTPUT: the location of where on disk to write the result file. The default is ${PWD}/dump. NOTE: If you want your dumps to be accessible to other PEcAn servers you need to perform the following additional steps Open pecan/scripts/load.bety.sh In the DUMPURL section of the code add a new record indicating where you are dumping your data. Below is the example for SITE number 1 (Boston University) elif [ &quot;${REMOTESITE}&quot; == &quot;1&quot; ]; then DUMPURL=&quot;http://psql-pecan.bu.edu/sync/dump/bety.tar.gz&quot; Check your Apache settings to make sure this location is public Commit this code and submit a Pull Request From the URL in the Pull Request, PEcAn administrators will update the machines table, the status map, and notify other users to update their cron jobs (see Automation below) Plans to simplify this process are in the works 21.3.5 Automation Below is an example of a script to synchronize PEcAn database instances across the network. db.sync.sh #!/bin/bash ## make sure psql is in PATH export PATH=/usr/pgsql-9.3/bin/:$PATH ## move to export directory cd /fs/data3/sync ## Dump Data MYSITE=1 /home/dietze/pecan/scripts/dump.bety.sh ## Load Data from other sites MYSITE=1 REMOTESITE=2 /home/dietze/pecan/scripts/load.bety.sh MYSITE=1 REMOTESITE=5 /home/dietze/pecan/scripts/load.bety.sh MYSITE=1 REMOTESITE=0 /home/dietze/pecan/scripts/load.bety.sh ## Timestamp sync log echo $(date +%c) &gt;&gt; /home/dietze/db.sync.log Typically such a script is set up to run as a cron job. Make sure to schedule this job (crontab -e) as the user that has database privledges (typically postgres). The example below is a cron table that runs the sync every hour at 12 min after the hour. MAILTO=user@yourUniversity.edu 12 * * * * /home/dietze/db.sync.sh 21.3.6 Network Status Map https://pecan2.bu.edu/pecan/status.php Nodes: red = down, yellow = out-of-date schema, green = good Edges: red = fail, yellow = out-of-date sync, green = good 21.3.7 Tasks Following is a list of tasks we plan on working on to improve these scripts: - pecanproject/bety#368 allow site-specific customization of information and UI elements including title, contacts, logo, color scheme. "],
["git-and-github-workflow.html", "22 Git and GitHub Workflow 22.1 Using Git 22.2 References: 22.3 GitHub use with PEcAn", " 22 Git and GitHub Workflow 22.1 Using Git This document describes the steps required to download PEcAn, make changes to code, and submit your changes. For asking questions, reporting bugs, and requesting features, see our documentation for reporting issues on Redmine and GitHub If you are new to GitHub or to PEcAn, start with the one-time set-up instructions under Before any work is done. Also see the excellent tutorials and references in the References section at the the bottom of this page. To make trivial changes, see Quick and Easy To make changes to the code, start with the basic workflow. If you want to submit changes that you’ve made to be part of PEcAn you’ll want to follow Committing Changes Using Pull Requests To update your local branch cd pecan git pull upstream master ./scripts/build.sh 22.1.1 Git Git is a version control software; GitHub is a project-hosting website that is similar to Redmine but easier to use for open and collaborative development. Git is a free &amp; open source, distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Every Git clone is a full-fledged repository with complete history and full revision tracking capabilities, not dependent on network access or a central server. Branching and merging are fast and easy to do. A good place to start is the GitHub 5 minute illustrated tutorial. In addition, there are three fun tutorials for learning git: Learn Git is a great web-based interactive tutorial. LearnGitBranching TryGit. For additional tutorials and background see References URLs In the rest of the document will use specific URL’s to clone the code. There a few URL’s you can use to clone a project, using https, ssh and git. You can use either https or git to clone a repository and write to it. The git protocol is read-only. 22.1.1.1 PEcAnProject on GitHub: https://github.com/organizations/PecanProject PEcAn source code: https://github.com/PecanProject/pecan.git git@github.com:PecanProject/pecan.git BETYdb source code: https://github.com/PecanProject/bety.git git@github.com:PecanProject/bety.git These instructions apply to other repositories too. 22.1.1.2 Milestones, Issues, Tasks The Milestones, issues, and tasks can be used to organize specific features or research projects. In general, there is a heirarchy: milestones (Big picture, “Epic”): contains many issues issues (Specific features / bugs, “Story”): may contain a list of tasks; represent task list (to do list, “Tasks”): list of steps required to close an issue, e.g.: ``` [ ] first do this [ ] then this [ ] completed when x and y ``` 22.1.1.3 Quick and Easy The easiest approach is to use GitHub’s browser based workflow. This is useful when your change is a few lines, if you are editing a wiki, or if the edit is trivial (and won’t break the code). The GitHub documentation is here but it is simple: finding the page or file you want to edit, click “edit” and then the GitHub web application will automatically forking and branch, then allow you to submit a pull request. However, it should be noted that unless you are a member of the PEcAn project that the “edit” button will not be active and you’ll want to follow the workflow described below for forking and then submitting a pull request. 22.1.2 Recommended Git Workflow Each feature should be in it’s own branch (for example each redmine issue is a branch, names of branches are often the issue in a bug tracking system). Commit and Push Frequency On your branch, commit at minimum once a day before you push changes: even better: every time you reach a stopping point and move to a new issue. best: any time that you have done work that you do not want to re-do. Remember, pushing changes to your branch is like saving a draft. Submit a pull request when you are done. 22.1.2.1 Before any work is done: The first step below only needs to be done once when you first start working on the PEcAn code. The steps below that need to be done to set up PEcAn on your computer, and would need to be repeated if you move to a new computer. If you are working from the PEcAn VM, you can skip the “git clone” since the PEcAn code is already installed. Most people will not be able to work in the PEcAn repository directly and will need to create a fork of the PEcAn source code in their own folder. To fork PEcAn into your own github space (github help: “fork a repo”). This forked repository will allow you to create branches and commit changes back to GitHub and create pull requests to the master of PEcAn. The forked repository is the only way for external people to commit code back to PEcAn and BETY. The pull request will start a review process that will eventually result in the code being merged into the main copy of the codebase. See https://help.github.com/articles/fork-a-repo for more information, especially on how to keep your fork up to date with respect to the original. (Rstudio users should also see Git + Rstudio, below) You can setup SSH keys to make it easier to commit cod back to GitHub. This might especially be true if you are working from a cluster, see set up ssh keys Introduce yourself to GIT git config --global user.name &quot;FULLNAME&quot; git config --global user.email you@yourdomain.example.com Fork PEcAn on GitHub. Goto the PEcAn source code and click on the Fork button in the upper right. This will create a copy of PEcAn in your personal space. Clone to your local machine git clone git@github.com:&lt;username&gt;/pecan.git Define upstream repository cd pecan git remote add upstream git@github.com:PecanProject/pecan.git 22.1.3 During development: commit often; each commit can address 0 or 1 issue; many commits can reference an issue (see [[Link commits to issue|Using-Git#link-commits-to-issues]]) ensure that all tests are passing before anything is pushed into master. 22.1.3.1 Basic Workflow PLEASE DO NOT USE THIS, SEE ADVANCED WORKFLOW! Get the latest code from the main repository git pull upstream master Do some coding Commit after each chunk of code (multiple times a day) git commit -m &quot;&lt;some descriptive information about what was done; references/fixes gh-X&gt;&quot; Push to YOUR Github (when a feature is working, a set of bugs are fixed, or you need to share progress with others) git push origin &lt;branchname&gt; Before submitting code back to the main repository, make sure that code compiles ./scripts/build.sh -c submit pull request with a reference to related issue (see [[Link commits to issue|Using-Git#link-commits-to-issues]]); also see github documentation 22.1.3.2 Advanced Workflow: A new branch for each change Make sure you start in master git checkout master Make sure master is up to date git pull upstream master Run any tests / make sure that code compiles For PEcAn: Build most recent versions of R packages (./scripts/build.sh -h for help)) ./scripts/build.sh For BETYdb (see BETY wiki) rspec Create a branch and switch to it git checkout -b &lt;branchname&gt; work/commit/etc git commit -m &quot;&lt;some descriptive information about what was done&gt;&quot; Run any tests / make sure that code compiles For PEcAn: ./scripts/build.sh -c Push this branch to your github space git push origin &lt;branchname&gt; submit pull request with [[link commits to issues|Using-Git#link-commits-to-issuess]]; also see explanation in this PecanProject/bety issue and github documentation 22.1.3.3 After pull request is merged Make sure you start in master git checkout master delete branch remotely git push origin --delete &lt;branchname&gt; delete branch locally git branch -D &lt;branchname&gt; 22.1.3.4 Link commits to issues You can reference and close issues from comments, pull requests, and commit messages. This should be done when you commit code that is related to or will close/fix an existing issue. There are two ways to do this. One easy way is to include the following text in your commit message: Github to close: “closes gh-xxx” (or syn. close, closed, fixes, fix, fixed) to reference: just the issue number (e.g. “gh-xxx”) avoid “closes #xxx” which will cross-reference Redmine issues Redmine: to close: “fixes redmine #xxx” (or closes etc.) to reference: “redmine #xxx” Bitbucket: to close: reference and use web interface! to reference: “re #xxx” Another way is to just add the url to the issue that you are updating. For example, if you are resolving an issue in Redmine, you can simply write the text “resolved by pull request https://github.com/PecanProject/pecan/pull/1” in the comments. 22.1.4 For PEcAn ./scripts/build.sh -c 22.1.4.1 Committing Changes Using Pull Requests GitHub provides a useful overview of how to submit changes to a project, Using Pull Requests. Once you have added a feature on your local fork of the project that you would like to contribute, these are the steps: Submit a Pull Request Pull request is reviewed by a member of the PEcAn core group Any comments should be addressed Additional commits are added to the pull request When ready, changes are merged 22.1.4.2 Other Useful Git Commands: GIT encourages branching “early and often” First pull from master Branch before working on feature One branch per feature You can switch easily between branches Merge feature into main line when branch done git branch &lt;name of branch&gt; git checkout &lt;name of branch&gt; repeat write some code commit until done git checkout master git merge &lt;name of brach&gt; git push If during above process you want to work on something else, commit all your code, create a new branch, and work on new branch. Delete a branch: git branch -d &lt;name of branch&gt; To push a branch git: push -u origin` To check out a branch: git fetch origin git checkout --track origin/&lt;name of branch&gt; Get the remote repository locally: git clone URL To update local repository to latest: git pull To add new files to the local repository: git add &lt;file&gt; To commit changes git commit &lt;file|folder&gt; To update remote repository: git push Show graph of commits: git log --graph --oneline --all 22.1.4.3 Tags Git supports two types of tags: lightweight and annotated. For more information see the Tagging Chapter in the Git documentation. Lightweight tags are useful, but here we discuss the annotated tags that are used for marking stable versions, major releases, and versions associated with published results. The basic command is git tag. The -a flag means ‘annotated’ and -m is used before a message. Here is an example: git tag -a v0.6 -m &quot;stable version with foo and bar features, used in the foobar publication by Bob&quot; Adding a tag to the a remote repository must be done explicitly with a push, e.g. git push v0.6 To use a tagged version, just checkout: git checkout v0.6 To tag an earlier commit, just append the commit SHA to the command, e.g. git tag -a v0.99 -m &quot;last version before 1.0&quot; 9fceb02 Using GitHub The easiest way to get working with GitHub is by installing the GitHub client. For instructions for your specific OS and download of the GitHub client, see https://help.github.com/articles/set-up-git. This will help you set up an SSH key to push code back to GitHub. To check out a project you do not need to have an ssh key and you can use the https or git url to check out the code. 22.1.5 Git + Rstudio Rstudio is nicely integrated with many development tools, including git and GitHub. It is quite easy to check out source code from within the Rstudio program or browser. The Rstudio documentation includes useful overviews of version control and R package development. Once you have git installed on your computer (see the Rstudio version control documentation for instructions), you can use the following steps to install the PEcAn source code in Rstudio. 22.1.5.1 Creating a Read-only version: This is a fast way to clone the repository that does not support contributing new changes (this can be done with further modification). install Rstudio (www.rstudio.com) click (upper right) project create project version control Git - clone a project from a Git Repository paste https://www.github.com/PecanProject/pecan choose working dir. for repo 22.1.6 For development: create account on github create a fork of the PEcAn repository to your own account https://www.github.com/pecanproject/pecan install Rstudio (www.rstudio.com) generate an ssh key in Rstudio: Tools -&gt; Options -&gt; Git/SVN -&gt; &quot;create RSA key&quot; View public key -&gt; ctrl+C to copy in GitHub go to ssh settings -&gt; 'add ssh key' -&gt; ctrl+V to paste -&gt; 'add key' Create project in Rstudio project (upper right) -&gt; create project -&gt; version control -&gt; Git - clone a project from a Git Repository paste repository url git@github.com:&lt;username&gt;/pecan.git&gt; choose working dir. for repository 22.2 References: 22.2.0.0.1 Git Documentation Scott Chacon, ‘Pro Git book’, http://git-scm.com/book GitHub help pages, https://help.github.com/ Main GIT page, http://git-scm.com/documentation Information about branches, http://nvie.com/posts/a-successful-git-branching-model/ Another set of pages about branching, http://sandofsky.com/blog/git-workflow.html Stackoverflow highest voted questions tagged “git” 22.2.0.0.2 GitHub Documentation When in doubt, the first step is to click the “Help” button at the top of the page. GitHub Flow by Scott Chacon (Git evangelist and Ruby developer working on GitHub.com) GitHub FAQ Using Pull Requests SSH Keys 22.3 GitHub use with PEcAn In this section, development topics are introduced and discussed. PEcAn code lives within the If you are looking for an issue to work on, take a look through issues labled “good first issue”. To get started you will want to review We use GitHub to track development. To learn about GitHub, it is worth taking some time to read through the FAQ. When in doubt, the first step is to click the “Help” button at the top of the page. To address specific people, use a github feature called @mentions e.g. write @dlebauer, @robkooper, @mdietze, or @serbinsh … in the issue to alert the user as described in the GitHub documentation on notifications 22.3.1 Bugs, Issues, Features, etc. 22.3.1.1 Reporting a bug (For developers) work through debugging. Once you have identified a problem, that you can not resolve, you can write a bug report Write a bug report submit the bug report If you do find the answer, explain the resolution (in the issue) and close the issue 22.3.1.2 Required content Note: a bug is only a bug if it is reproducible clear bug reports save time Clear, specific title Description - What you did What you expected to happen What actually happened What does work, under what conditions does it fail? Reproduction steps - minimum steps required to reproduce the bug additional materials that could help identify the cause: screen shots stack traces, logs, scripts, output specific code and data / settings / configuration files required to reproduce the bug environment (operating system, browser, hardware) 22.3.2 Requesting a feature (from The Pragmatic Programmer, available as ebook through UI libraries, hardcopy on David’s bookshelf) * focus on “user stories”, e.g. specific use cases * Be as specific as possible, Here is an example: Bob is at www.mysite.edu/maps map of the the region (based on user location, e.g. US, Asia, etc) option to “use current location” is provided, if clicked, map zooms in to, e.g. state or county level for site run: option to select existing site or specify point by lat/lon option to specify a bounding box and grid resolution in either lat/lon or polar stereographic. asked to specify start and end times in terms of year, month, day, hour, minute. Time is recorded in UTC not local time, this should be indicated 22.3.3 Closing an issue Definition of “Done” test documentation when issue is resolved: status is changed to “resolved” assignee is changed to original author if original author agrees that issue has been resolved original author changes status to “closed” except for trivial issues, issues are only closed by the author 22.3.4 When to submit an issue? 22.3.4.1 Ideally, non-trivial code changes will be linked to an issue and a commit. This requires creating issues for each task, making small commits, and referencing the issue within your commit message. Issues can be created on GitHub. These issues can be linked to commits by adding text such as fixes gh-5 (see [[Using Git During Development|Using-Git#during-development]]). Rationale: This workflow is a small upfront investment that reduces error and time spent re-creating and debugging errors. Associating issues and commits, makes it easier to identify why a change was made, and potential bugs that could arise when the code is changed. In addition, knowing which issue you are working on clarifies the scope and objectives of your current task. "],
["coding-practices.html", "23 Coding Practices 23.1 Coding Style 23.2 Logging 23.3 Package Data 23.4 Packages used in development 23.5 Roxygen2 23.6 Testing", " 23 Coding Practices 23.1 Coding Style Consistent coding style improves readability and reduces errors in shared code. R does not have an official style guide, but Hadley Wickham provides one that is well thought out and widely adopted. Advanced R: Coding Style. Both the Wickham text and this page are derived from Google’s R Style Guide. 23.1.1 Use Roxygen2 documentation This is the standard method of documentation used in PEcAn development, it provides inline documentation similar to doxygen. Even trivial functions should be documented. See Roxygen2 Wiki page 23.1.2 Write your name at the top Any function that you create or make a meaningful contribution to should have your name listed after the author tag in the function documentation. 23.1.3 Use testthat testing package See Unit_Testing wiki for instructions, and Advanced R: Tests. tests provide support for documentation - they define what a function is (and is not) expected to do all functions need tests to ensure basic functionality is maintained during development. all bugs should have a test that reproduces the bug, and the test should pass before bug is closed 23.1.4 Don’t use shortcuts R provides many shortcuts that are useful when coding interactively, or for writing scripts. However, these can make code more difficult to read and can cause problems when written into packages. 23.1.4.1 Function Names (verb.noun) Following convention established in PEcAn 0.1, we use the all lowercase with periods to separate words. They should generally have a verb.noun format, such as query.traits, get.samples, etc. 23.1.4.2 File Names File names should end in .R, .Rdata, .Rscript and should be meaningful, e.g. named after the primary functions that they contain. There should be a separate file for each major high-level function to aid in identifying the contents of files in a directory. 23.1.4.3 Use “&lt;-” as an assignment operator Because most R code uses &lt;- (except where = is required), we will use &lt;- “=” is used for function arguments 23.1.4.4 Use Spaces around all binary operators (=, +, -, &lt;-, etc.). after but not before a comma 23.1.4.5 Use curly braces The option to omit curly braces is another shortcut that makes code easier to write but harder to read and more prone to error. 23.1.5 Package Dependencies: 23.1.5.1 library vs require When another package is required by a function or script, it can be called in the following ways: (As a package dependency loads with the package, these should be the default approaches when writing functions in a package. There can be some exceptions, such as when a rarely-used or non-essential function requires an esoteric package.) 1. When using library, if dependency is not met, it will print an error and stop 2. When using require, it will print a warning and continue (but will throw an error when a function from the required package is called) Reference: Stack Overflow “What is the difference between require and library?” 23.1.5.2 DEPENDS, SUGGESTS, IMPORTS It is considered best practice to use DEPENDS and SUGGESTS in DESCRIPTION; SUGGESTS should be used for packages that are called infrequently, or only in examples and vignettes; suggested packages are called by require inside a function. Consider using IMPORTS instead of depends in the DESCRIPTION files. This will make loading packages faster by allowing it to have functions available without loading the hierarchy of dependencies, dependencies of dependencies, ad infinitum … From p. 6 of the “R extensions manual”:http://cran.r-project.org/doc/manuals/R-exts.html The Suggests field uses the same syntax as Depends and lists packages that are not necessarily needed. This includes packages used only in examples, tests or vignettes (see Section 1.4 [Writing package vignettes], page 26), and packages loaded in the body of functions. E.g., suppose an example from package foo uses a dataset from package bar. Then it is not necessary to have bar use foo unless one wants to execute all the examples/tests/vignettes: it is useful to have bar, but not necessary. Version requirements can be specified, and will be used by R CMD check. 23.2 Logging During development we often add many print statements to check to see how the code is doing, what is happening, what intermediate results there are etc. When done with the development it would be nice to turn this additional code off, but have the ability to quickly turn it back on if we discover a problem. This is where logging comes into play. Logging allows us to use “rules” to say what information should be shown. For example when I am working on the code to create graphs, I do not have to see any debugging information about the SQL command being sent, however trying to figure out what goes wrong during a SQL statement it would be nice to show the SQL statements without adding any additional code. 23.2.1 PEcAn logging functions These logger family of functions are more sophisticated, and can be used in place of stop, warn, print, and similar functions. The logger functions make it easier to print to a system log file. 23.2.1.1 Examples The file test.logger.R provides descriptive examples This query provides an current overview of functions that use logging logger functions (in order of increasing level): logger.debug logger.info logger.warn logger.error the logger.setLevel function sets the level at which a message will be printed logger.setLevel(&quot;DEBUG&quot;) will print messages from all logger functions logger.setLevel(&quot;ERROR&quot;) will only print messages from logger.error logger.setLevel(&quot;INFO&quot;) and logger.setLevel(&quot;WARN&quot;) shows messages fromlogger.and higher functions, e.g.logger.setLevel(“WARN”)shows messages fromlogger.warnandlogger.error` logger.setLevel(&quot;OFF&quot;) suppresses all logger messages To print all messages to console, use logger.setUseConsole(TRUE) 23.2.1.2 Related Issues (requires Redmine developer account) #1071 How to handle errors? #1222 Ignore warnings You can use @logger.setLevel(“ERROR”)@ to only show error messages. All the code that does not use logger will not be filtered. 23.2.2 Other R logging packages This section is for reference - these functions should not be used in PEcAn, as they are redundant with the logger.* functions described above R does provide a basic logging capability using stop, warning and message. These allow to print message (and stop execution in case of stop). However there is not an easy method to redirect the logging information to a file, or turn the logging information on and off. This is where one of the following packages comes into play. The packages themselves are very similar since they try to emulate log4j. Both of the following packages use a hierarchic loggers, meaning that if you change the level of displayed level of logging at one level all levels below it will update their logging. 23.2.2.1 logging The logging development is done at http://logging.r-forge.r-project.org/ and more information is located at http://cran.r-project.org/web/packages/logging/index.html . To install use the following command: install.packages(\"logging\", repos=\"http://R-Forge.R-project.org\") This has my preference pure based on documentation. 23.2.2.2 futile The second logging package is http://cran.r-project.org/web/packages/futile.logger/ and is eerily similar to logging (as a matter of fact logging is based on futile). 23.2.3 Example Usage To be able to use the loggers there needs to be some initialization done. Neither package allows to read it from a configuration file, so we might want to use the pecan.xml file to set it up. The setup will always be somewhat the same: # load library library(logging) logReset() # add handlers, responsible for actually printing/saving the messages addHandler(writeToConsole) addHandler(writeToFile, file=&quot;file.log&quot;) # setup root logger with INFO setLevel(&#39;INFO&#39;) # make all of PEcAn print debug messages setLevel(&#39;DEBUG&#39;, getLogger(&#39;PEcAn&#39;)) # only print info and above for the SQL part of PEcAn setLevel(&#39;INFO&#39;, getLogger(&#39;PEcAn.SQL&#39;)) To now use logging in the code you can use the following code: pl &lt;- getLogger(&#39;PEcAn.MetaAnalysis.function1&#39;) pl$info(&quot;This is an INFO message.&quot;) pl$debug(&quot;The value for x=%d&quot;, x) pl$error(&quot;Something bad happened and I am scared now.&quot;) or loginfo(&quot;This is an INFO message.&quot;, logger=&quot;PEcAn.MetaAnalysis.function1&quot;) logdebug(&quot;The value for x=%d&quot;, x, logger=&quot;PEcAn.MetaAnalysis.function1&quot;) logerror(&quot;Something bad happened and I am scared now.&quot;, logger=&quot;PEcAn.MetaAnalysis.function1&quot;) 23.3 Package Data 23.3.1 Summary: Files with the following extensions will be read by R as data: plain R code in .R and .r files are sourced using source() text tables in .tab, .txt, .csv files are read using read() ** objects in R image files: .RData, .rda are loaded using load() capitalization matters all objects in foo.RData are loaded into environment pro: easiset way to store objects in R format con: format is application (R) specific (discussed in #318) Details are in ?data, which is mostly a copy of Data section of Writing R Extensions. 23.3.2 Accessing data Data in the [data] directory will be accessed in the following ways, efficient way: (especially for large data sets) using the data function: data(foo) # accesses data with, e.g. load(foo.RData), read(foo.csv), or source(foo.R) easy way: by adding the following line to the package DESCRIPTION: note: this should be used with caution or it can cause difficulty as discussed in redmine issue #1118 LazyData: TRUE From the R help page: Currently, a limited number of data formats can be accessed using the data function by placing one of the following filetypes in a packages’ data directory: * files ending .R or .r are source()d in, with the R working directory changed temporarily to the directory containing the respective file. (data ensures that the utils package is attached, in case it had been run via utils::data.) * files ending .RData or .rda are load()ed. * files ending .tab, .txt or .TXT are read using read.table(..., header = TRUE), and hence result in a data frame. * files ending .csv or .CSV are read using read.table(..., header = TRUE, sep = ';'), and also result in a data frame. If your data does not fall in those 4 categories, or you can use the system.file function to get access to the data: system.file(&quot;data&quot;, &quot;ed.trait.dictionary.csv&quot;, package=&quot;PEcAn.utils&quot;) [1] &quot;/home/kooper/R/x86_64-pc-linux-gnu-library/2.15/PEcAn.utils/data/ed.trait.dictionary.csv&quot; The arguments are folder, filename(s) and then package. It will return the fully qualified path name to a file in a package, in this case it points to the trait data. This is almost the same as the data function, however we can now use any function to read the file, such as read.csv instead of read.csv2 which seems to be the default of data. This also allows us to store arbitrary files in the data folder, such as the the bug file and load it when we need it. 23.3.2.0.1 Examples of data in PEcAn packages Redmine issue #1060 added time constants in source:utils/data/time.constants.RData outputs: [/modules/uncertainties/data/output.RData] parameter samples [/modules/uncertainties/data/samples.RData] 23.4 Packages used in development 23.4.0.1 roxygen2 Used to document code. See instructions under [[R#Coding_Style|Coding Style]] 23.4.0.2 devtools Provides functions to simplify development Documentation: The R devtools packate load_all(&quot;pkg&quot;) document(&quot;pkg&quot;) test(&quot;pkg&quot;) install(&quot;pkg&quot;) build(&quot;pkg&quot;) other tips for devtools (from the documentation): Adding the following to your ~/.Rprofile will load devtools when running R in interactive mode: # load devtools by default if (interactive()) { suppressMessages(require(devtools)) } Adding the following to your .Rpackages will allow devtools to recognize package by folder name, rather than directory path # in this example, devhome is the pecan trunk directory devhome &lt;- &quot;/home/dlebauer/R-dev/pecandev/&quot; list( default = function(x) { file.path(devhome, x, x) }, &quot;utils&quot; = paste(devhome, &quot;pecandev/utils&quot;, sep = &quot;&quot;) &quot;common&quot; = paste(devhome, &quot;pecandev/common&quot;, sep = &quot;&quot;) &quot;all&quot; = paste(devhome, &quot;pecandev/all&quot;, sep = &quot;&quot;) &quot;ed&quot; = paste(devhome, &quot;pecandev/models/ed&quot;, sep = &quot;&quot;) &quot;uncertainty&quot; = paste(devhome, &quot;modules/uncertainty&quot;, sep = &quot;&quot;) &quot;meta.analysis&quot; = paste(devhome, &quot;modules/meta.analysis&quot;, sep = &quot;&quot;) &quot;db&quot; = paste(devhome, &quot;db&quot;, sep = &quot;&quot;) ) Now, devtools can take pkg as an argument instead of /path/to/pkg/, e.g. so you can use build(&quot;pkg&quot;) instead of build(&quot;/path/to/pkg/&quot;) 23.5 Roxygen2 This is the standard method of documentation used in PEcAn development, it provides inline documentation similar to doxygen. 23.5.1 Canonical references: Must Read: R package development by Hadley Wickham: Object Documentation Package Metadata Roxygen2 Documentation Roxygen2 Package Documentation GitHub 23.5.2 Basic Roxygen2 instructions: Section headers link to “Writing R extensions” which provides in-depth documentation. This is provided as an overview and quick reference. 23.5.2.1 Tags tags are preceeded by ##' tags required by R: ** title tag is required, along with actual title ** param one for each parameter, should be defined ** return must state what function returns (or nothing, if something occurs as a side effect tags strongly suggested for most functions: ** author ** examples can be similar to test cases. optional tags: ** export required if function is used by another package ** import can import a required function from another package (if package is not loaded or other function is not exported) ** seealso suggests related functions. These can be linked using \\code{link{}} 23.5.2.2 Text markup 23.5.2.2.1 Formatting \\bold{} \\emph{} italics 23.5.2.2.2 Links \\url{www.url.com} or \\href{url}{text} for links \\code{\\link{thisfn}} links to function “thisfn” in the same package \\code{\\link{foo::thatfn}} links to function “thatfn” in package “foo” \\pkg{package_name} 23.5.2.2.3 Math \\eqn{a+b=c} uses LaTex to format an inline equation \\deqn{a+b=c} uses LaTex to format displayed equation \\deqn{latex}{ascii} and \\eqn{latex}{ascii} can be used to provide different versions in latex and ascii. 23.5.2.2.4 Lists \\enumerate{ \\item A database consists of one or more records, each with one or more named fields. \\item Regular lines start with a non-whitespace character. \\item Records are separated by one or more empty lines. } \\itemize and \\enumerate commands may be nested. 23.5.2.2.5 “Tables”:http://cran.r-project.org/doc/manuals/R-exts.html#Lists-and-tables \\tabular{rlll}{ [,1] \\tab Ozone \\tab numeric \\tab Ozone (ppb)\\cr [,2] \\tab Solar.R \\tab numeric \\tab Solar R (lang)\\cr [,3] \\tab Wind \\tab numeric \\tab Wind (mph)\\cr [,4] \\tab Temp \\tab numeric \\tab Temperature (degrees F)\\cr [,5] \\tab Month \\tab numeric \\tab Month (1--12)\\cr [,6] \\tab Day \\tab numeric \\tab Day of month (1--31) } 23.5.3 Example Here is an example documented function, myfun ##&#39; My function adds three numbers ##&#39; ##&#39; A great function for demonstrating Roxygen documentation ##&#39; @param a numeric ##&#39; @param b numeric ##&#39; @param c numeric ##&#39; @return d, numeric sum of a + b + c ##&#39; @export ##&#39; @author David LeBauer ##&#39; @examples ##&#39; myfun(1,2,3) ##&#39; \\dontrun{myfun(NULL)} myfun &lt;- function(a, b, c){ d &lt;- a + b + c return(d) } In emacs, with the cursor inside the function, the keybinding C-x O will generate an outline or update the Roxygen2 documentation. 23.5.4 Updating documentation After adding documentation run the following command (replacing common with the name of the folder you want to update): ** In R using devtools to call roxygenize: require(devtools) document(&quot;common&quot;) 23.6 Testing PEcAn uses the testthat package developed by Hadley Wickham. Hadley has written instructions for using this package in his Testing chapter. 23.6.1 Rationale makes development easier provides working documentation of expected functionality saves time by allowing computer to take over error checking once a test has been made improves code quality Further reading: Aruliah et al 2012 Best Practices for Scientific Computing 23.6.2 Tests makes development easier and less error prone Testing makes it easier to develop by organizing everything you are already doing anyway - but integrating it into the testing and documentation. With a codebase like PEcAn, it is often difficult to get started. You have to figure out what was I doing yesterday? what do I want to do today? what existing functions do I need to edit? what are the arguments to these functions (and what are examples of valid arguments) what packages are affected where is a logical place to put files used in testing 23.6.3 Quick Start: decide what you want to do today identify the issue in github (if none exists, create one) to work on issue 99, create a new branch called “github99” or some descriptive name… Today we will enable an existing function, make.cheas to make goat.cheddar. We will know that we are done by the color and taste. git branch goat-cheddar git checkout goat-cheddar open existing (or create new) file in inst/tests/. If working on code in “myfunction” or a set of functions in “R/myfile.R”, the file should be named accordingly, e.g. “inst/tests/test.myfile.R” if you are lucky, the function has already been tested and has some examples. if not, you may need to create a minimal example, often requiring a settings file. The default settings file can be obtained in this way: r settings &lt;- read.settings(system.file(&quot;extdata/test.settings.xml&quot;, package = &quot;PEcAn.utils&quot;)) write what you want to do test_that(&quot;make.cheas can make cheese&quot;,{ goat.cheddar &lt;- make.cheas(source = 'goat', style = 'cheddar') expect_equal(color(goat.cheddar), &quot;orange&quot;) expect_is(object = goat.cheddar, class = &quot;cheese&quot;) expect_true(all(c(&quot;sharp&quot;, &quot;creamy&quot;) %in% taste(goat.cheddar))) } now edit the goat.cheddar function until it makes savory, creamy, orange cheese. commit often update documentation and test r library(devtools) document(&quot;mypkg&quot;) test(&quot;mypkg&quot;) * commit again * when complete, merge, and push bash git commit -m &quot;make.cheas makes goat.cheddar now&quot; git checkout master git merge goat-cheddar git push 23.6.4 Test files Many of PEcAn’s functions require inputs that are provided as data. These can be in the /data or the /inst/extdata folders of a package. Data that are not package specific should be placed in the PEcAn.all or PEcAn.utils files. Some useful conventions: 23.6.4.1 Settings A generic settings can be found in the PEcAn.all package settings.xml &lt;- system.file(&quot;pecan.biocro.xml&quot;, package = &quot;PEcAn.BIOCRO&quot;) settings &lt;- read.settings(settings.xml) database settings can be specified, and tests run only if a connection is available We currently use the following database to run tests against; tests that require access to a database should check db.exists() and be skipped if it returns FALSE to avoid failed tests on systems that do not have the database installed. settings$database &lt;- list(userid = &quot;bety&quot;, passwd = &quot;bety&quot;, name = &quot;bety&quot;, # database name host = &quot;localhost&quot; # server name) test_that(..., { skip_if_not(db.exists(settings$database)) ## write tests here }) instructions for installing this are available on the VM creation wiki examples can be found in the PEcAn.DB package (base/db/tests/testthat/). Model specific settings can go in the model-specific module, for example: settings.xml &lt;- system.file(&quot;extdata/pecan.biocro.xml&quot;, package = &quot;PEcAn.BIOCRO&quot;) settings &lt;- read.settings(settings.xml) test-specific settings: settings text can be specified inline: settings.text &lt;- &quot; &lt;pecan&gt; &lt;nocheck&gt;nope&lt;/nocheck&gt; ## allows bypass of checks in the read.settings functions &lt;pfts&gt; &lt;pft&gt; &lt;name&gt;ebifarm.pavi&lt;/name&gt; &lt;outdir&gt;test/&lt;/outdir&gt; &lt;/pft&gt; &lt;/pfts&gt; &lt;outdir&gt;test/&lt;/outdir&gt; &lt;database&gt; &lt;userid&gt;bety&lt;/userid&gt; &lt;passwd&gt;bety&lt;/passwd&gt; &lt;location&gt;localhost&lt;/location&gt; &lt;name&gt;bety&lt;/name&gt; &lt;/database&gt; &lt;/pecan&gt;&quot; settings &lt;- read.settings(settings.text) values in settings can be updated: settings &lt;- read.settings(settings.text) settings$outdir &lt;- &quot;/tmp&quot; ## or any other settings 23.6.4.2 Helper functions created to make testing easier tryl returns FALSE if function gives error temp.settings creates temporary settings file test.remote returns TRUE if remote connection is available db.exists returns TRUE if connection to database is available 23.6.4.3 When should I test? A test should be written for each of the following situations: Each bug should get a regression test. The first step in handling a bug is to write code that reproduces the error This code becomes the test most important when error could re-appear essential when error silently produces invalid results Every time a (non-trivial) function is created or edited Write tests that indicate how the function should perform example: expect_equal(sum(1,1), 2) indicates that the sum function should take the sum of its arguments Write tests for cases under which the function should throw an error example: expect_error(sum(&quot;foo&quot;)) better : expect_error(sum(&quot;foo&quot;), &quot;invalid 'type' (character)&quot;) 23.6.4.4 What types of testing are important to understand? 23.6.4.5 Unit Testing / Test Driven Development Tests are only as good as the test write test write code 23.6.4.6 Regression Testing When a bug is found, write a test that finds the bug (the minimum test required to make the test fail) fix the bug bug is fixed when test passes 23.6.4.7 How should I test in R? The testthat package. tests are found in ~/pecan/&lt;packagename&gt;/inst/tests, for example utils/inst/tests/ See attached file and http://r-pkgs.had.co.nz/tests.html for details on how to use the testthat package. 23.6.4.7.1 List of Expectations Full Abbreviation expect_that(x, is_true()) expect_true(x) expect_that(x, is_false()) expect_false(x) expect_that(x, is_a(y)) expect_is(x, y) expect_that(x, equals(y)) expect_equal(x, y) expect_that(x, is_equivalent_to(y)) expect_equivalent(x, y) expect_that(x, is_identical_to(y)) expect_identical(x, y) expect_that(x, matches(y)) expect_matches(x, y) expect_that(x, prints_text(y)) expect_output(x, y) expect_that(x, shows_message(y)) expect_message(x, y) expect_that(x, gives_warning(y)) expect_warning(x, y) expect_that(x, throws_error(y)) expect_error(x, y) 23.6.4.7.2 How to run tests add the following to “pecan/tests/testthat.R” library(testthat) library(mypackage) test_check(&quot;mypackage&quot;) 23.6.4.8 basic use of the testthat package Here is an example of tests (these should be placed in &lt;packagename&gt;/tests/testthat/test-&lt;sourcefilename&gt;.R: test_that(&quot;mathematical operators plus and minus work as expected&quot;,{ expect_equal(sum(1,1), 2) expect_equal(sum(-1,-1), -2) expect_equal(sum(1,NA), NA) expect_error(sum(&quot;cat&quot;)) set.seed(0) expect_equal(sum(matrix(1:100)), sum(data.frame(1:100))) }) test_that(&quot;different testing functions work, giving excuse to demonstrate&quot;,{ expect_identical(1, 1) expect_identical(numeric(1), integer(1)) expect_equivalent(numeric(1), integer(1)) expect_warning(mean(&#39;1&#39;)) expect_that(mean(&#39;1&#39;), gives_warning(&quot;argument is not numeric or logical: returning NA&quot;)) expect_warning(mean(&#39;1&#39;), &quot;argument is not numeric or logical: returning NA&quot;) expect_message(message(&quot;a&quot;), &quot;a&quot;) }) 23.6.4.8.1 Script testing It is useful to add tests to a script during development. This allows you to test that the code is doing what you expect it to do. * here is a fake script using the iris data set test_that(&quot;the iris data set has the same basic features as before&quot;,{ expect_equal(dim(iris), c(150,5)) expect_that(iris$Sepal.Length, is_a(&quot;numeric&quot;)) expect_is(iris$Sepal.Length, &quot;numeric&quot;)#equivalent to prev. line expect_is(iris$Species, &quot;factor&quot;) }) iris.color &lt;- data.frame(Species = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;), color = c(&quot;pink&quot;, &quot;blue&quot;, &quot;orange&quot;)) newiris &lt;- merge(iris, iris.color) iris.model &lt;- lm(Petal.Length ~ color, data = newiris) test_that(&quot;changes to Iris code occurred as expected&quot;,{ expect_that(dim(newiris), equals(c(150, 6))) expect_that(unique(newiris$color), is_identical_to(unique(iris.color$color))) expect_equivalent(iris.model$coefficients[&quot;(Intercept)&quot;], 4.26) }) 23.6.4.8.2 Function testing Testing of a new function, as.sequence. The function and documentation are in source:R/utils.R and the tests are in source:tests/test.utils.R. Recently, I made the function as.sequence to turn any vector into a sequence, with custom handling of NA’s: function(x, na.rm = TRUE){ x2 &lt;- as.integer(factor(x, unique(x))) if(all(is.na(x2))){ x2 &lt;- rep(1, length(x2)) } if(na.rm == TRUE){ x2[is.na(x2)] &lt;- max(x2, na.rm = TRUE) + 1 } return(x2) } The next step was to add documentation and test. Many people find it more efficient to write tests before writing the function. This is true, but it also requires more discipline. I wrote these tests to handle the variety of cases that I had observed. As currently used, the function is exposed to a fairly restricted set of options - results of downloads from the database and transformations. test_that(“as.sequence works”;{ expect_identical(as.sequence(c(“a”, “b”)), 1:2) expect_identical(as.sequence(c(“a”, NA)), 1:2) expect_equal(as.sequence(c(“a”, NA), na.rm = FALSE), c(1,NA)) expect_equal(as.sequence(c(NA,NA)), c(1,1)) }) 23.6.5 Testing the Shiny Server Shiny can be difficult to debug because, when run as a web service, the R output is hidden in system log files that are hard to find and read. One useful approach to debugging is to use port forwarding, as follows. First, on the remote machine (including the VM), make sure R’s working directory is set to the directory of the Shiny app (e.g., setwd(/path/to/pecan/shiny/WorkflowPlots), or just open the app as an RStudio project). Then, in the R console, run the app as: shiny::runApp(port = XXXX) # E.g. shiny::runApp(port = 5638) Then, on your local machine, open a terminal and run the following command, matching XXXX to the port above and YYYY to any unused port on your local machine (any 4-digit number should work). ssh -L YYYY:localhost:XXXX &lt;remote connection&gt; # E.g., for the PEcAn VM, given the above port: # ssh -L 5639:localhost:5638 carya@localhost -p 6422 Now, in a web browser on your local machine, browse to localhost:YYYY (e.g., localhost:5639) to run whatever app you started with shiny::runApp in the previous step. All of the output should display in the R console where the shiny::runApp command was executed. Note that this includes any print, message, logger.*, etc. statements in your Shiny app. If the Shiny app hits an R error, the backtrace should include a line like Hit error at of server.R#LXX – that XX being a line number that you can use to track down the error. To return from the error to a normal R prompt, hit &lt;Control&gt;-C (alternatively, the “Stop” button in RStudio). To restart the app, run shiny::runApp(port = XXXX) again (keeping the same port). Note that Shiny runs any code in the pecan/shiny/&lt;app&gt; directory at the moment the app is launched. So, any changes you make to the code in server.R and ui.R or scripts loaded therein will take effect the next time the app is started. If for whatever reason this doesn’t work with RStudio, you can always run R from the command line. Also, note that the ability to forward ports (ssh -L) may depend on the ssh configuration of your remote machine. These instructions have been tested on the PEcAn VM (v.1.5.2+). "],
["directory-structure.html", "24 Directory structure 24.1 Overview of PEcAn repository as of PEcAn 1.5.3 24.2 Generic R package structure:", " 24 Directory structure 24.1 Overview of PEcAn repository as of PEcAn 1.5.3 pecan/ +- base/ # Core functions +- all # Dummy package to load all PEcAn R packages +- db # Modules for querying the database +- logger # Report warnings without killing workflows +- qaqc # Model skill testing and integration testing +- remote # Communicate with and execute models on local and remote hosts +- settings # Functions to read and manipulate PEcAn settings files +- utils # Misc. utility functions +- visualization # Advanced PEcAn visualization module +- workflow # functions to coordinate analysis steps +- book_source/ # Main documentation and developer&#39;s guide +- CHANGELOG.md # Running log of changes in each version of PEcAn +- docker/ # Experimental replacement for PEcAn virtual machine +- documentation # index_vm.html, references, other misc. +- models/ # Wrappers to run models within PEcAn +- ed/ # Wrapper scripts for running ED within PEcAn +- sipnet/ # Wrapper scripts for running SIPNET within PEcAn +- ... # Wrapper scripts for running [...] within PEcAn +- template/ # Sample wrappers to copy and modify when adding a new model +- modules # Core modules +- allometry +- data.atmosphere +- data.hydrology +- data.land +- meta.analysis +- priors +- rtm +- uncertainty +- ... +- scripts # R and Shell scripts for use with PEcAn +- shiny/ # Interactive visualization of model results +- tests/ # Settings files for host-specific integration tests +- web # Main PEcAn website files 24.2 Generic R package structure: see the R development wiki for more information on writing code and adding data. +- DESCRIPTION # short description of the PEcAn library +- R/ # location of R source code +- man/ # Documentation (automatically compiled by Roxygen) +- inst/ # files to be installed with package that aren&#39;t R functions +- extdata/ # misc. data files (in misc. formats) +- data/ # data used in testing and examples (saved as *.RData or *.rda files) +- NAMESPACE # declaration of package imports and exports (automatically compiled by Roxygen) +- tests/ # PEcAn testing scripts +- testthat/ # nearly all tests should use the testthat framework and live here "],
["models-in-pecan.html", "25 Models in PEcAn 25.1 BioCro Configuration 25.2 BioCro 25.3 CLM 25.4 DALEC 25.5 ED 25.6 ED2 Configuration 25.7 GDAY 25.8 LINKAGES 25.9 LPJ-GUESS 25.10 MAESPA", " 25 Models in PEcAn 25.1 BioCro Configuration BioCro uses a config.xml file similar to ED2. At this time, no other template files are required. 25.2 BioCro Model Information Home Page Source Code License Authors PEcAn Integration Introduction Introduction about model PEcAn configuration file additions Should list the model specific additions to the PEcAn file here Model specific input files List of inputs required by model, such as met, etc. Model configuration files BioCro uses a config.xml file similar to ED2. At this time, no other template files are required. Installation notes This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. VM 25.3 CLM Model Information Home Page Source Code License Authors PEcAn Integration Introduction Introduction about model PEcAn configuration file additions Should list the model specific additions to the PEcAn file here Model specific input files List of inputs required by model, such as met, etc. Model configuration files MODEL is configured using 3 files which are placed in the run folder, as well as a symbolic link to the met file. file1 : template for this file is located at models/MODEL/inst/file1 and is not modified. file2 : template for this file is located at models/MODEL/inst/file2 and is not modified. file3 : template for this file is in models/MODEL/inst/file3 or it is specified in the &lt;model&gt; section as &lt;template&gt;. The values in this template are replaced by those computed in the earlier stages of PEcAN. Installation notes This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. VM 25.4 DALEC Model Information Home Page Source Code License Authors PEcAn Integration Introduction Introduction about model PEcAn configuration file additions Should list the model specific additions to the PEcAn file here Model specific input files List of inputs required by model, such as met, etc. Model configuration files MODEL is configured using 3 files which are placed in the run folder, as well as a symbolic link to the met file. file1 : template for this file is located at models/MODEL/inst/file1 and is not modified. file2 : template for this file is located at models/MODEL/inst/file2 and is not modified. file3 : template for this file is in models/MODEL/inst/file3 or it is specified in the &lt;model&gt; section as &lt;template&gt;. The values in this template are replaced by those computed in the earlier stages of PEcAN. Installation notes This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. VM 25.5 ED Model Information Home Page http://moorcroftlab.oeb.harvard.edu/ Source Code https://github.com/EDmodel/ED2 License Authors Paul Moorcroft, … PEcAn Integration Michael Dietze, Rob Kooper Introduction Introduction about ED model PEcAn configuration file additions Should list the model specific additions to the PEcAn file here Model specific input files List of inputs required by model, such as met, etc. Model configuration files ED2 is configured using 2 files which are placed in the run folder. ED2IN : template for this file is located at models/ed/inst/ED2IN.&lt;revision&gt;. The values in this template that need to be modified are described below and are surrounded with @ symbols. config.xml : this file is generated by PEcAn. Some values are stored in the pecan.xml in &lt;pfts&gt;&lt;pft&gt;&lt;constants&gt; section as well as in &lt;model&gt; section. An example of the template can be found in ED2IN.r82 The ED2IN template can contain the following variables. These will be replaced with actual values when the model configuration is written. **@ENSNAME@** : run id of the simulation, used in template for NL%EXPNME **@START_MONTH@** : start of simulation UTC time, from &lt;run&gt;&lt;start.date&gt;, used in template for NL%IMONTHA **@START_DAY@** : start of simulation UTC time, from &lt;run&gt;&lt;start.date&gt;, used in template for NL%IDATEA **@START_YEAR@** : start of simulation UTC time, from &lt;run&gt;&lt;start.date&gt;, used in template for NL%IYEARA **@END_MONTH@** : end of simulation UTC time, from &lt;run&gt;&lt;end.date&gt;, used in template for NL%IMONTHZ **@END_DAY@** : end of simulation UTC time, from &lt;run&gt;&lt;end.date&gt;, used in template for NL%IDATEZ **@END_YEAR@** : end of simulation UTC time, from &lt;run&gt;&lt;end.date&gt;, used in template for NL%IYEARZ **@SITE_LAT@** : site latitude location, from &lt;run&gt;&lt;site&gt;&lt;lat&gt;, used in template for NL%POI_LAT **@SITE_LON@** : site longitude location, from &lt;run&gt;&lt;site&gt;&lt;lon&gt;, used in template for NL%POI_LON **@SITE_MET@** : met header location, from &lt;run&gt;&lt;site&gt;&lt;met&gt;, used in template for NL%ED_MET_DRIVER_DB **@MET_START@** : first year of met data, from &lt;run&gt;&lt;site&gt;&lt;met.start&gt;, used in template for NL%METCYC1 **@MET_END@** : last year of met data, from &lt;run&gt;&lt;site&gt;&lt;met.end&gt;, used in template for NL%METCYCF **@PHENOL_SCHEME@** : phenology scheme, if this variabe is 1 the following 3 fields will be used, otherwise they will be set to empty strings, from &lt;model&gt;&lt;phenol.scheme&gt;, used in template for NL%IPHEN_SCHEME **@PHENOL_START@** : first year for phenology, from &lt;model&gt;&lt;phenol.start&gt;, used in template for NL%IPHENYS1 and NL%IPHENYF1 **@PHENOL_END@** : last year for phenology, from &lt;model&gt;&lt;phenol.end&gt;, used in template for NL%IPHENYSF and NL%IPHENYFF **@PHENOL@** : path and prefix of the prescribed phenology data, from * &lt;phenol&gt;, used in template for NL%PHENPATH **@SITE_PSSCSS@** : path and prefix of the previous ecosystem state, from &lt;model&gt;&lt;psscss&gt;, used in template for NL%SFILIN **@ED_VEG@** : path and prefix of the vegetation database, used only to determine the land/water mask, from &lt;model&gt;&lt;veg&gt;, used in template for NL%VEG_DATABASE **@ED_SOIL@** : path and prefix of the soil database, used to determine the soil type, from &lt;model&gt;&lt;soil&gt;, used in template for NL%SOIL_DATABASE **@ED_INPUTS@** : input directory with dataset to initialise chilling degrees and growing degree days, which is used to drive the cold-deciduous phenology, from &lt;model&gt;&lt;inputs&gt;, used in template for NL%THSUMS_DATABASE **@FFILOUT@** : path and prefix for analysis files, generated from &lt;run&gt;&lt;host&gt;&lt;outdir&gt;/run.id/analysis, used in template for NL%FFILOUT **@SFILOUT@** : path and prefix for history files, generated from &lt;run&gt;&lt;host&gt;&lt;outdir&gt;/run.id/history, used in template for NL%SFILOUT **@CONFIGFILE@** : XML file containing additional parameter settings, this is always “config.xml”, used in template for NL%IEDCNFGF **@OUTDIR@** : location where output files are written (without the runid), from &lt;run&gt;&lt;host&gt;&lt;outdir&gt;, should not be used. **@SCRATCH@** : local scratch space for outputs, generated /scratch/&lt;username&gt;/run$scratch, should not be used right now since it only works on ebi-cluster Installation notes This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. VM ** BU geo** TACC lonestar module load hdf5 curl -o ED.r82.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/ED.r82.tgz tar zxf ED.r82.tgz rm ED.r82.tgz cd ED.r82/ED/build/bin curl -o include.mk.lonestar http://isda.ncsa.illinois.edu/~kooper/EBI/include.mk.lonestar make OPT=lonestar TACC stampede module load hdf5 curl -o ED.r82.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/ED.r82.tgz tar zxf ED.r82.tgz rm ED.r82.tgz cd ED.r82/ED/build/bin curl -o include.mk.stampede http://isda.ncsa.illinois.edu/~kooper/EBI/include.mk.stampede make OPT=stampede 25.6 ED2 Configuration ED2 is configured using 2 files which are placed in the run folder. ED2IN : template for this file is located at models/ed/inst/ED2IN.&lt;revision&gt;. The values in this template that need to be modified are described below and are surrounded with @ symbols. config.xml : this file is generated by PEcAn. Some values are stored in the pecan.xml in &lt;pfts&gt;&lt;pft&gt;&lt;constants&gt; section as well as in &lt;model&gt; section. An example of the template can be found in ED2IN.r82 25.6.1 ED2IN configuration variables **@ENSNAME@** : run id of the simulation, used in template for NL%EXPNME **@START_MONTH@** : start of simulation UTC time, from &lt;run&gt;&lt;start.date&gt;, used in template for NL%IMONTHA **@START_DAY@** : start of simulation UTC time, from &lt;run&gt;&lt;start.date&gt;, used in template for NL%IDATEA **@START_YEAR@** : start of simulation UTC time, from &lt;run&gt;&lt;start.date&gt;, used in template for NL%IYEARA **@END_MONTH@** : end of simulation UTC time, from &lt;run&gt;&lt;end.date&gt;, used in template for NL%IMONTHZ **@END_DAY@** : end of simulation UTC time, from &lt;run&gt;&lt;end.date&gt;, used in template for NL%IDATEZ **@END_YEAR@** : end of simulation UTC time, from &lt;run&gt;&lt;end.date&gt;, used in template for NL%IYEARZ **@SITE_LAT@** : site latitude location, from &lt;run&gt;&lt;site&gt;&lt;lat&gt;, used in template for NL%POI_LAT **@SITE_LON@** : site longitude location, from &lt;run&gt;&lt;site&gt;&lt;lon&gt;, used in template for NL%POI_LON **@SITE_MET@** : met header location, from &lt;run&gt;&lt;site&gt;&lt;met&gt;, used in template for NL%ED_MET_DRIVER_DB **@MET_START@** : first year of met data, from &lt;run&gt;&lt;site&gt;&lt;met.start&gt;, used in template for NL%METCYC1 **@MET_END@** : last year of met data, from &lt;run&gt;&lt;site&gt;&lt;met.end&gt;, used in template for NL%METCYCF **@PHENOL_SCHEME@** : phenology scheme, if this variabe is 1 the following 3 fields will be used, otherwise they will be set to empty strings, from &lt;model&gt;&lt;phenol.scheme&gt;, used in template for NL%IPHEN_SCHEME **@PHENOL_START@** : first year for phenology, from &lt;model&gt;&lt;phenol.start&gt;, used in template for NL%IPHENYS1 and NL%IPHENYF1 **@PHENOL_END@** : last year for phenology, from &lt;model&gt;&lt;phenol.end&gt;, used in template for NL%IPHENYSF and NL%IPHENYFF **@PHENOL@** : path and prefix of the prescribed phenology data, from &lt;model&gt;&lt;phenol&gt;, used in template for NL%PHENPATH **@SITE_PSSCSS@** : path and prefix of the previous ecosystem state, from &lt;model&gt;&lt;psscss&gt;, used in template for NL%SFILIN **@ED_VEG@** : path and prefix of the vegetation database, used only to determine the land/water mask, from &lt;model&gt;&lt;veg&gt;, used in template for NL%VEG_DATABASE **@ED_SOIL@** : path and prefix of the soil database, used to determine the soil type, from &lt;model&gt;&lt;soil&gt;, used in template for NL%SOIL_DATABASE **@ED_INPUTS@** : input directory with dataset to initialise chilling degrees and growing degree days, which is used to drive the cold-deciduous phenology, from &lt;model&gt;&lt;inputs&gt;, used in template for NL%THSUMS_DATABASE **@FFILOUT@** : path and prefix for analysis files, generated from &lt;run&gt;&lt;host&gt;&lt;outdir&gt;/run.id/analysis, used in template for NL%FFILOUT **@SFILOUT@** : path and prefix for history files, generated from &lt;run&gt;&lt;host&gt;&lt;outdir&gt;/run.id/history, used in template for NL%SFILOUT **@CONFIGFILE@** : XML file containing additional parameter settings, this is always “config.xml”, used in template for NL%IEDCNFGF **@OUTDIR@** : location where output files are written (without the runid), from &lt;run&gt;&lt;host&gt;&lt;outdir&gt;, should not be used. **@SCRATCH@** : local scratch space for outputs, generated /scratch/\\&lt;username\\&gt;/run$scratch, should not be used right now since it only works on ebi-cluster ED Computation HPC TACC lonestar module load hdf5 curl -o ED.r82.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/ED.r82.tgz tar zxf ED.r82.tgz rm ED.r82.tgz cd ED.r82/ED/build/bin curl -o include.mk.lonestar http://isda.ncsa.illinois.edu/~kooper/EBI/include.mk.lonestar make OPT=lonestar TACC stampede module load hdf5 curl -o ED.r82.tgz http://isda.ncsa.illinois.edu/~kooper/EBI/ED.r82.tgz tar zxf ED.r82.tgz rm ED.r82.tgz cd ED.r82/ED/build/bin curl -o include.mk.stampede http://isda.ncsa.illinois.edu/~kooper/EBI/include.mk.stampede make OPT=stampede 25.7 GDAY Model Information Home Page Source Code License Authors PEcAn Integration Introduction Introduction about model PEcAn configuration file additions Should list the model specific additions to the PEcAn file here Model specific input files List of inputs required by model, such as met, etc. Model configuration files MODEL is configured using 3 files which are placed in the run folder, as well as a symbolic link to the met file. file1 : template for this file is located at models/MODEL/inst/file1 and is not modified. file2 : template for this file is located at models/MODEL/inst/file2 and is not modified. file3 : template for this file is in models/MODEL/inst/file3 or it is specified in the &lt;model&gt; section as &lt;template&gt;. The values in this template are replaced by those computed in the earlier stages of PEcAN. Installation notes This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. VM 25.8 LINKAGES Model Information Home Page Source Code License Authors PEcAn Integration Introduction Introduction about model PEcAn configuration file additions Should list the model specific additions to the PEcAn file here Model specific input files List of inputs required by model, such as met, etc. Model configuration files MODEL is configured using 3 files which are placed in the run folder, as well as a symbolic link to the met file. file1 : template for this file is located at models/MODEL/inst/file1 and is not modified. file2 : template for this file is located at models/MODEL/inst/file2 and is not modified. file3 : template for this file is in models/MODEL/inst/file3 or it is specified in the &lt;model&gt; section as &lt;template&gt;. The values in this template are replaced by those computed in the earlier stages of PEcAN. Installation notes This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. VM 25.9 LPJ-GUESS Model Information Home Page Source Code License Authors PEcAn Integration Introduction Introduction about model PEcAn configuration file additions Should list the model specific additions to the PEcAn file here Model specific input files List of inputs required by model, such as met, etc. Model configuration files MODEL is configured using 3 files which are placed in the run folder, as well as a symbolic link to the met file. file1 : template for this file is located at models/MODEL/inst/file1 and is not modified. file2 : template for this file is located at models/MODEL/inst/file2 and is not modified. file3 : template for this file is in models/MODEL/inst/file3 or it is specified in the &lt;model&gt; section as &lt;template&gt;. The values in this template are replaced by those computed in the earlier stages of PEcAN. Installation notes This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. VM 25.10 MAESPA Model Information Home Page http://maespa.github.io/ Source Code http://maespa.github.io/download.html License Authors Belinda Medlyn and Remko Duursma PEcAn Integration Tony Gardella, Martim DeKauwe, Remki Duursma Introduction PEcAn configuration file additions Model specific input files Model configuration files MODEL is configured using 3 files which are placed in the run folder, as well as a symbolic link to the met file. file1 : template for this file is located at models/MODEL/inst/file1 and is not modified. file2 : template for this file is located at models/MODEL/inst/file2 and is not modified. file3 : template for this file is in models/MODEL/inst/file3 or it is specified in the &lt;model&gt; section as &lt;template&gt;. The values in this template are replaced by those computed in the earlier stages of PEcAN. Installation notes Installing the MAESPA model requires cloning the MAESPA Bitbucket Repository, executing the makefile, and ensuring that the Maeswarp R package is correctly installed. To clone and compile the model, execute this code at the command line git clone https://bitbucket.org/remkoduursma/maespa.git cd maespa make clean make maespa.out is your executable. Example input files can be found in the inputfiles directory. Executing measpa.out from within one of the example directories will produce output. MAESPA developers have also developed a wrapper package called Maeswrap. The usual R package installation method install.packages may present issues with downloading an unpacking a dependency package called rgl. Here are a couple of solutions: Solution 1 From the Command Line sudo apt-get install r-cran-rgl then from within R install.packages(&quot;Maeswrap&quot;) Solution 2 From the Command line sudo apt-get install libglu1-mesa-dev then from within R install.packages(&quot;Maeswrap&quot;) This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. VM "],
["pecan-models.html", "26 PEcAn Models 26.1 PRELES 26.2 SIPNET configuration 26.3 Sipnet", " 26 PEcAn Models This section will contain information about all models and output variables that are supported by PEcAn. Model Name Available in the VM Prescribed Inputs Input Functions/Values Restart Function BioCro Yes Yes Yes No CLM No No No No DALEC Yes Yes Yes No ED Yes Yes Yes No FATES No Yes No GDAY No No No No LINKAGES Yes Yes Yes Yes LPJ-GUESS Yes Yes No No MAESPA Yes Yes No No PRELES Yes Yes Partially No Sipnet Yes Yes Yes No Output Variables PEcAn converts all model outputs to a single standard. This standard evolved out of MsTMIP project, which is itself based on NACP, LBA, and other model-intercomparison projects. This standard was expanded for the PalEON MIP and the needs of the PEcAn modeling community to support variables not in these standards. Model developers: do not add variables to your PEcAn output without first adding them to the PEcAn standard table! Also, do not create new variables equivalent to existing variables but just with different names or units. 26.1 PRELES Model Information Home Page Source Code License Authors PEcAn Integration Introduction Introduction about model PEcAn configuration file additions Should list the model specific additions to the PEcAn file here Model specific input files List of inputs required by model, such as met, etc. Model configuration files MODEL is configured using 3 files which are placed in the run folder, as well as a symbolic link to the met file. file1 : template for this file is located at models/MODEL/inst/file1 and is not modified. file2 : template for this file is located at models/MODEL/inst/file2 and is not modified. file3 : template for this file is in models/MODEL/inst/file3 or it is specified in the &lt;model&gt; section as &lt;template&gt;. The values in this template are replaced by those computed in the earlier stages of PEcAN. Installation notes This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. VM 26.2 SIPNET configuration SIPNET is configured using 3 files which are placed in the run folder, as well as a symbolic link to the met file. sipnet.in : template for this file is located at models/sipnet/inst/sipnet.in and is not modified. sipnet.param-spatial : template for this file is located at models/sipnet/inst/template.param-spatial and is not modified. sipnet.param : template for this file is in models/sipnet/inst/template.param or it is specified in the &lt;model&gt; section as &lt;default.param&gt;. The values in this template are replaced by those computed in the earlier stages of PEcAN. 26.3 Sipnet Model Information Home Page Source Code License Authors PEcAn Integration Michael Dietze, Rob Kooper Introduction Introduction about model PEcAn configuration file additions Should list the model specific additions to the PEcAn file here Model specific input files List of inputs required by model, such as met, etc. Model configuration files SIPNET is configured using 3 files which are placed in the run folder, as well as a symbolic link to the met file. sipnet.in : template for this file is located at models/sipnet/inst/sipnet.in and is not modified. sipnet.param-spatial : template for this file is located at models/sipnet/inst/template.param-spatial and is not modified. sipnet.param : template for this file is in models/sipnet/inst/template.param or it is specified in the &lt;model&gt; section as &lt;default.param&gt;. The values in this template are replaced by those computed in the earlier stages of PEcAN. Installation notes This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. SIPNET version unk: if [ ! -e ${HOME}/sipnet_unk ]; then cd curl -o sipnet_unk.tar.gz http://isda.ncsa.illinois.edu/~kooper/PEcAn/models/sipnet_unk.tar.gz tar zxf sipnet_unk.tar.gz rm sipnet_unk.tar.gz fi cd ${HOME}/sipnet_unk/ make clean make sudo cp sipnet /usr/local/bin/sipnet.runk make clean SIPNET version 136: if [ ! -e ${HOME}/sipnet_r136 ]; then cd curl -o sipnet_r136.tar.gz http://isda.ncsa.illinois.edu/~kooper/EBI/sipnet_r136.tar.gz tar zxf sipnet_r136.tar.gz rm sipnet_r136.tar.gz sed -i &#39;s#$(LD) $(LIBLINKS) \\(.*\\)#$(LD) \\1 $(LIBLINKS)#&#39; ${HOME}/sipnet_r136/Makefile fi cd ${HOME}/sipnet_r136/ make clean make sudo cp sipnet /usr/local/bin/sipnet.r136 make clean VM "],
["template-model-page.html", "27 Template MODEL page 27.1 Model Output Variables", " 27 Template MODEL page Model Information Home Page Source Code License Authors PEcAn Integration Introduction Introduction about model PEcAn configuration file additions Should list the model specific additions to the PEcAn file here Model specific input files List of inputs required by model, such as met, etc. Model configuration files MODEL is configured using 3 files which are placed in the run folder, as well as a symbolic link to the met file. file1 : template for this file is located at models/MODEL/inst/file1 and is not modified. file2 : template for this file is located at models/MODEL/inst/file2 and is not modified. file3 : template for this file is in models/MODEL/inst/file3 or it is specified in the &lt;model&gt; section as &lt;template&gt;. The values in this template are replaced by those computed in the earlier stages of PEcAN. Installation notes This section contains notes on how to compile the model. The notes for the VM might work on other machines or configurations as well. VM 27.1 Model Output Variables Note: initial table derived from MsTMIP Num Group order Saveit Variable.Name standard_name Units Long.name Priority Category Hourly Monthly var_type ndim dim1 dim2 dim3 dim4 Description 1 1 1 Yes lon longitude degrees_east Longitude 0 Grid Yes Yes real 1 lon na na na longitude at center of each grid cell 2 1 2 Yes lat latitude degrees_north Latitude 0 Grid Yes Yes real 1 lat na na na latitude at center of each grid cell 3 1 3 Yes lon_bnds degrees_east Longitude west-east bounds 0 Grid Yes Yes real 2 nbnds lon na na (west boundary of grid cell, east boundary of grid cell) 4 1 4 Yes lat_bnds degrees_north Latitude south-north bounds 0 Grid Yes Yes real 2 nbnds lat na na (south boundary of grid cell, north boundary of grid cell) 5 2 1 Yes time time days since 1700-01-01 00:00:00 UTC Time middle averaging period 0 Time Yes Yes double 1 time na na na julian days days since 1700-01-01 00:00:00 UTC for middle of time averaging period Proleptic_Gregorianc calendar 6 2 2 Yes time_bnds days since 1700-01-01 00:00:00 UTC Time beginning-end bounds 0 Time Yes Yes double 2 nbnds time na na (julian days days since 1700-01-01 beginning time ave period, julian days days since 1700-01-01 end time ave period) 7 2 3 Yes dec_date yr Decimal date middle averaging period 0 Time Yes Yes double 1 time na na na decimal date in fractional years for middle of time averaging period 8 2 4 Yes dec_date_bnds yr Decimal date beginning-end bounds 0 Time Yes Yes double 2 nbnds time na na (decimal date beginning time ave period, decimal date end time ave period) 9 2 5 Yes cal_date_mid yr, mon, day, hr, min, sec Calender date middle averaging period 0 Time Yes Yes integer 2 ncal time na na calender date middle of time ave period: year, month, day, hour, minute, second for UTC time zone 10 2 6 Yes cal_date_beg yr, mon, day, hr, min, sec Calender date beginning averaging period 0 Time Yes Yes integer 2 ncal time na na calender date beginning of time ave period: year, month, day, hour, minute, second for UTC time zone 11 2 7 Yes cal_date_end yr, mon, day, hr, min, sec Calender date end averaging period 0 Time Yes Yes integer 2 ncal time na na calender date end of time ave period: year, month, day, hour, minute, second for UTC time zone 12 3 1 Yes GPP kg C m-2 s-1 Gross Primary Productivity 1 Carbon Fluxes Yes Yes real 3 lon lat time na Rate of photosynthesis (always positive) 13 3 2 Yes NPP kg C m-2 s-1 Net Primary Productivity 1 Carbon Fluxes Yes Yes real 3 lon lat time na Net Primary Productivity (NPP=GPP-AutoResp, positive into plants) 14 3 3 Yes TotalResp kg C m-2 s-1 Total Respiration 1 Carbon Fluxes Yes Yes real 3 lon lat time na Total respiration (TotalResp=AutoResp+heteroResp, always positive) 15 3 4 Yes AutoResp kg C m-2 s-1 Autotrophic Respiration 1 Carbon Fluxes Yes Yes real 3 lon lat time na Autotrophic respiration rate (always positive) 16 3 5 Yes HeteroResp kg C m-2 s-1 Heterotrophic Respiration 1 Carbon Fluxes Yes Yes real 3 lon lat time na Heterotrophic respiration rate (always positive) 17 3 6 Yes DOC_flux kg C m-2 s-1 Dissolved Organic Carbon flux 1 Carbon Fluxes Yes Yes real 3 lon lat time na Loss of organic carbon dissolved in ground water or rivers (positive out of grid cell) 18 3 7 Yes Fire_flux kg C m-2 s-1 Fire emissions 1 Carbon Fluxes Yes Yes real 3 lon lat time na Flux of carbon due to fires (always positive) 19 3 8 Yes NEE kg C m-2 s-1 Net Ecosystem Exchange 1 Carbon Fluxes Yes Yes real 3 lon lat time na Net Ecosystem Exchange (NEE=HeteroResp+AutoResp-GPP, positive into atmosphere) 21 4 2 Yes poolname (-) Name of each Carbon Pool 1 Carbon Pools No Yes character 2 nchar npool na na Name of each carbon pool (i.e., &quot;wood,&quot; or &quot;Coarse Woody Debris&quot;) 22 4 3 Yes CarbPools kg C m-2 Size of each carbon pool 1 Carbon Pools No Yes real 4 lon lat npool time Total size of each carbon pool vertically integrated over the entire soil column 23 4 4 Yes AbvGrndWood kg C m-2 Above ground woody biomass 1 Carbon Pools No Yes real 3 lon lat time na Total above ground wood biomass 24 4 5 Yes TotLivBiom kg C m-2 Total living biomass 1 Carbon Pools No Yes real 3 lon lat time na Total carbon content of the living biomass (leaves+roots+wood) 25 4 6 Yes TotSoilCarb kg C m-2 Total Soil Carbon 1 Carbon Pools No Yes real 3 lon lat time na Total soil and litter carbon content vertically integrated over the enire soil column 26 4 7 Yes LAI m2 m-2 Leaf Area Index 1 Carbon Pools No Yes real 3 lon lat time na Area of leaves per area ground 27 5 1 Yes Qh W m-2 Sensible heat 1 Energy Fluxes Yes Yes real 3 lon lat time na Sensible heat flux into the boundary layer (positive into atmosphere) 28 5 2 Yes Qle W m-2 Latent heat 1 Energy Fluxes Yes Yes real 3 lon lat time na Latent heat flux into the boundary layer (positive into atmosphere) 29 5 3 Yes Evap kg m-2 s-1 Total Evaporation 1 Energy Fluxes No Yes real 3 lon lat time na Sum of all evaporation sources (positive into atmosphere) 30 5 4 Yes TVeg kg m-2 s-1 Transpiration 1 Energy Fluxes No Yes real 3 lon lat time na Total Plant transpiration (always positive) 31 5 5 Yes LW_albedo (-) Longwave Albedo 1 Energy Fluxes No Yes real 3 lon lat time na Longwave Albedo 32 5 6 Yes SW_albedo (-) Shortwave Albedo 1 Energy Fluxes No Yes real 3 lon lat time na Shortwave albedo 33 5 7 Yes Lwnet W m-2 Net Longwave Radiation 1 Energy Fluxes No Yes real 3 lon lat time na Incident longwave radiation minus simulated outgoing longwave radiation (positive into grnd) 34 5 8 Yes SWnet W m-2 Net shortwave radiation 1 Energy Fluxes No Yes real 3 lon lat time na Incident shortwave radiation minus simulated outgoing shortwave radiation (positive into grnd) 35 5 9 Yes fPAR (-) Absorbed fraction incoming PAR 1 Energy Fluxes No Yes real 3 lon lat time na absorbed fraction incoming photosyntetically active radiation 37 6 2 Yes z_top m Soil Layer Top Depth 1 Physical Variables No Yes real 1 nsoil na na na Depth from soil surface to top of soil layer 38 6 3 Yes z_node m Soil Layer Node Depth 1 Physical Variables No Yes real 1 nsoil na na na Depth from soil surface to layer prognostic variables; typically center of soil layer 39 6 4 Yes z_bottom m Soil Layer Bottom Depth 1 Physical Variables No Yes real 1 nsoil na na na Depth from soil surface to bottom of soil layer 40 6 5 Yes SoilMoist kg m-2 Average Layer Soil Moisture 1 Physical Variables No Yes real 4 lon lat nsoil time Soil water content in each soil layer, including liquid, vapor and ice 41 6 5 Yes SoilMoistFrac (-) Average Layer Fraction of Saturation 1 Physical Variables No Yes real 4 lon lat nsoil time Fraction of saturation of soil water in each soil layer, including liquid and ice 42 6 6 Yes SoilWet (-) Total Soil Wetness 1 Physical Variables No Yes real 3 lon lat time na Vertically integrated soil moisture divided by maximum allowable soil moisture above wilting point 43 6 7 Yes Qs kg m-2 s-1 Surface runoff 1 Physical Variables No Yes real 3 lon lat time na Runoff from the landsurface and/or subsurface stormflow 44 6 8 Yes Qsb kg m-2 s-1 Subsurface runoff 1 Physical Variables No Yes real 3 lon lat time na Gravity soil water drainage and/or soil water lateral flow 45 6 9 Yes SoilTemp K Average Layer Soil Temperature 1 Physical Variables No Yes real 4 lon lat nsoil time Average soil temperature in each soil layer 46 6 10 Yes Tdepth m Active Layer Thickness 1 Physical Variables No Yes real 3 lon lat time na Thaw depth; depth to zero centigrade isotherm in permafrost 47 6 11 Yes Fdepth m Frozen Layer Thickness 1 Physical Variables No Yes real 3 lon lat time na Freeze depth; depth to zero centigrade isotherm in non-permafrost 48 6 12 Yes Tcan K Canopy Temperature 1 Physical Variables No Yes real 3 lon lat time na Canopy or vegetation temperature (or temperature used in photosynthesis calculations) 49 6 13 Yes SWE kg m-2 Snow Water Equivalent 1 Physical Variables No Yes real 3 lon lat time na Total water mass of snow pack, including ice and liquid water 50 6 14 Yes SnowDen kg m-3 Bulk Snow Density 1 Physical Variables No Yes real 3 lon lat time na Overall bulk density of the snow pack, including ice and liquid water 51 6 15 Yes SnowDepth m Total snow depth 1 Physical Variables No Yes real 3 lon lat time na Total snow depth 52 7 1 Yes CO2air micromol mol-1 Near surface CO2 concentration 1 Driver No Yes real 3 lon lat time na Near surface dry air CO2 mole fraction 53 7 2 Yes LWdown surface_downwelling_longwave_flux_in_air W/m2 Surface incident longwave radiation 1 Driver No Yes real 3 lon lat time na Surface incident longwave radiation 54 7 3 Yes Psurf air_pressure Pa Surface pressure 1 Driver No Yes real 3 lon lat time na Surface pressure 55 7 4 Yes Qair specific_humidity kg kg-1 Near surface specific humidity 1 Driver No Yes real 3 lon lat time na Near surface specific humidity 56 7 5 Yes Rainf precipitation_flux kg m-2 s-1 Rainfall rate 1 Driver No Yes real 3 lon lat time na Rainfall rate 57 7 6 Yes SWdown surface_downwelling_shortwave_flux_in_air W m-2 Surface incident shortwave radiation 1 Driver No Yes real 3 lon lat time na Surface incident shortwave radiation 58 7 7 Yes Tair air_temperature K Near surface air temperature 1 Driver No Yes real 3 lon lat time na Near surface air temperature 59 7 8 Yes Wind wind_speed m s-1 Near surface module of the wind 1 Driver No Yes real 3 lon lat time na Near surface wind magnitude 60 NA NA Tmin air_temperature_max K Daily Maximum Temperature 1 Driver No Yes real 3 lon lat time na Daily Maximum Temperature 61 NA NA Tmax air_temperature_min K Daily Minimum Temperature 1 Driver No Yes real 3 lon lat time na Daily Minimum Temperature 62 NA NA Uwind northward_wind m s-1 Northward Component of Wind 1 Driver No Yes real 3 lon lat time na Northward Component of Wind 63 NA NA Vwind eastward_wind m s-1 Eastward Component of Wind 1 Driver No Yes real 3 lon lat time na Eastward Component of Wind 64 NA NA RH relative_humidity % Relative Humidity 1 Driver No Yes real 3 lon lat time na Relative Humidity 65 NA NA PAR surface_downwelling_photosynthetic_photon_flux_in_air mol m-2 s-1 Photosynthetically Active Radiation 1 Driver No Yes real 3 lon lat time na Photosynthetically Active Radiation "],
["debugging.html", "28 Debugging", " 28 Debugging How to identify the source of a problem. 28.0.1 Using tests/workflow.R This script, along with model-specific settings files in the tests folder, provide a working example. From inside the tests folder, R CMD --vanilla -- --settings pecan.&lt;model&gt;.xml &lt; workflow.R should work. The next step is to add debugonce(&lt;broken.function.name&gt;) before running the test workflow. This allows you can step through the function and evaluate the different objects as they are created and/or transformed. See tests README for more information. 28.0.2 Useful scripts The following scripts (in qaqc/vignettes identify, respectively: relationships among functions across packages function inputs and outputs (e.g. that will identify which functions and outputs are used in a workflow). 28.0.3 Debugging Shiny Apps When developing shiny apps you can run the application from rstudio and place breakpoints int he code. To do this you will need to do the following steps first (already done on the VM) before starting rstudio: - echo “options(shiny.port = 6438)” &gt;&gt; ${HOME}/.Rprofile - echo “options(shiny.launch.browser = ‘FALSE’)” &gt;&gt; ${HOME}/.Rprofile Next you will need to create a tunnel for port 6438 to the VM, which will be used to open the shiny app, the following command will creat this tunnel: ssh -l carya -p 6422 -L 6438:localhost:6438 localhost. Now you can from rstudio run your application using shiny::runApp() and it will show the output from the application in your console. You can now place breakpoints and evaluate the output. 28.0.4 Download GFDL The Downlad.GFDL function assimilates 3 hour frequency CMIP5 outputs generated by multiple GFDL models. GFDL developed several distinct modeling streams on the timescale of CMIP5 and AR5. These models include CM3, ESM2M and ESM2G with a spatial resolution of 2 degrees latitude by 2.5 degrees longitude. Each model has future outputs for the AR5 Representative Concentration Pathways ranging from 2006-2100. 28.0.5 CM3 GFDL’s CMIP5 experiments with CM3 included many of the integrations found in the long-term CMIP5 experimental design. The focus of this physical climate model is on the role of aerosols, aerosol-cloud interactions, and atmospheric chemistry in climate variability and climate change. 28.0.6 ESM2M &amp; ESM2G Two new models representing ocean physics with alternative numerical frameworks to explore the implications of some of the fundamental assumptions embedded in these models. Both ESM2M and ESM2G utilize a more advanced land model, LM3, than was available in ESM2.1 including a variety of enhancements (Milly et al., in prep). GFDL’s CMIP5 experiments with Earth System Models included many of the integrations found in the long-term CMIP5 experimental design. The ESMs, by design, close the carbon cycle and are used to study the impact of climate change on ecosystems, ecosystem changes on climate and human activities on ecosystems. For more information please navigate here &lt;th&gt;&lt;/th&gt; &lt;th&gt;CM#&lt;/th&gt; &lt;th&gt;ESM2M&lt;/th&gt; &lt;th&gt;ESM2G&lt;/th&gt; &lt;tr&gt; &lt;td&gt;rcp26&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;r1i1p1&lt;/td&gt; &lt;td&gt;r1i1p1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;rcp45&lt;/td&gt; &lt;td&gt;r1i1p1, r3i1p1,r5i1p1&lt;/td&gt; &lt;td&gt;r1i1p1&lt;/td&gt; &lt;td&gt;r1i1p1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;rcp60&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;r1i1p1&lt;/td&gt; &lt;td&gt;r1i1p1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;rcp85&lt;/td&gt; &lt;td&gt;r1i1p1&lt;/td&gt; &lt;td&gt;r1i1p1&lt;/td&gt; &lt;td&gt;r1i1p1&lt;/td&gt; &lt;/tr&gt; "],
["part-demos-and-vignettes.html", "(Part) Demos and Vignettes", " (Part) Demos and Vignettes "],
["pecan-hands-on-demo-01-basic-run.html", "29 PEcAn Hands-On Demo 01: Basic Run 29.1 Objective 29.2 Basic Run 29.3 Next steps", " 29 PEcAn Hands-On Demo 01: Basic Run 29.1 Objective We will begin by exploring a set of web-based tools that are designed to run single-site model runs. A lot of the detail about what’s going on under the hood, and all the outputs that PEcAn produces, are left to Demo 2. This demo will also demonstrate how to use PEcAn outputs in additional analyses outside of PEcAn. 29.1.1 PEcAn URL In the following demo, URL is the web address of a PEcAn server and will refer to one of the following: If you are doing a live demo with the PEcAn team, URL was provided If you followed instructions in Download the PEcAn VM , URL = localhost:6480 If you running PEcAn [Using AmazonWeb Services], URL is the Public IP If you followed instructions found in Install PEcAn by hand, URL is your server’s IP 29.2 Basic Run 29.2.1 Start PEcAn: Enter URL in your web browser Click “Run Models” Click the ‘Next’ button to move to the “Site Selection” page. 29.2.2 Site Selection 29.2.3 Host Select the local machine “pecan”. Other options exist if you’ve read and followed instructions found in Remote execution with PEcAn. 29.2.4 Mode Select SIPNET (r136) from the available models because it is quick &amp; simple. Reference material can be found in Models in PEcAn 29.2.5 Site Group To filter sites, you can select a specific group of sites. For this tutorial we will use Ameriflux. 29.2.6 Conversion: Select the conversion check box, to show all sites that PEcAn is capable of generating model drivers for automatically. By default (unchecked), PEcAn only displays sites where model drivers already exist in the system database 29.2.7 Site: For this tutorial, type US-NR1 in the search box to display the Niwot Ridge Ameriflux site (US-NR1), and then click on the pin icon. When you click on a site’s flag on the map, it will give you the name and location of the site and put that site in the “Site:” box on the left hand site, indicating your current selection. Once you are finished with the above steps, click “Next”. 29.2.7.1 Run Specification Next we will specify settings required to run the model. Be aware that the inputs required for any particular model may vary somewhat so there may be addition optional or required input selections available for other models. 29.2.7.2 PFT (Plant Functional Type): Niwot Ridge is temperate coniferous. Available PFTs will vary by model and some models allow multiple competing PFTs to be selected. Also select soil to control the soil parameters 29.2.7.3 Start/End Date: Select 2003/01/01 to 2006/12/31. In general, be careful to select dates for which there is available driver data. 29.2.7.4 Weather Data: Select “Use AmerifluxLBL” from the Available Meteorological Drivers. 29.2.7.5 Optional Settings: Leave all blank for demo run Email sends a message when the run is complete. Use Brown Dog will use the Brown Dog web services in order to do input file conversions. (Note: Required if you select Use NARR for Weather Data) Edit pecan.xml allows you to configure advanced settings via the PEcAn settings file Edit model config pauses the workflow after PEcAn has written all model specific settings but before the model runs are called and allows users to configure any additional settings internal to the model. Advanced Setup controls ensemble and sensitivity run settings discussed in Demo 2. Finally, click “Next” to start the model run. 29.2.7.6 Data Use Policies You will see a data policy statement if you selected a data source with a policy. Agreeing to the policy is required prior to starting the run. 29.2.7.7 If you get an error in your run If you get an error in your run as part of a live demo or class activity, it is probably simplest to start over and try changing options and re-running (e.g. with a different site or PFT), as time does not permit detailed debugging. If the source of the error is not immediately obvious, you may want to take a look at the workflow.Rout to see the log of the PEcAn workflow or the logfile.txt to see the model execution output log and then refer to the Documentation or the Chat Room for help. 29.2.8 Model Run Workflow 29.2.8.1 MET Process: First, PEcAn will download meteorological data based on the type of the Weather Data you chose, and process it into the specific format for the chosen model 29.2.8.2 TRAIT / META: PEcAn then estimates model parameters by performing a meta-analysis of the available trait data for a PFT. TRAIT will extract relevant trait data from the database. META performs a hierarchical Bayes meta-analysis of available trait data. The output of this analysis is a probability distribution for each model parameter. PEcAn selects the median value of this parameter as the default, but in Demo 2 we will see how PEcAn can use this parameter uncertainty to make probabilistic forecasts and assess model sensitivity and uncertainty. Errors at this stage usually indicate errors in the trait database or incorrectly specified PFTs (e.g. defining a variable twice). 29.2.8.3 CONFIG: writes model-specific settings and parameter files 29.2.8.4 MODEL: runs model. 29.2.8.5 OUTPUT: All model outputs are converted to standard netCDF format 29.2.8.6 ENSEMBLE &amp; SENSITIVITY: If enabled post-process output for these analyses If at any point a Stage Name has the Status “ERROR” please notify the PEcAn team member that is administering the demo or feel free to do any of the following: Refer to the PEcAn Documentation for documentation Post the end of your workflow log on our Gitter chat Post an issue on Github. The entire PEcAn team welcomes any questions you may have! If the Finished Stage has a Status of “DONE”, congratulations! If you got this far, you have managed to run an ecosystem model without ever touching a line of code! Now it’s time to look at the results click Finished. FYI, adding a new model to PEcAn does not require modification of the model’s code, just the implementation of a wrapper function. 29.2.8.7 Output and Visualization For now focus on graphs, we will explore all of PEcAn’s outputs in more detail in Demo 02. 29.2.8.8 Graphs Select a Year and Y-axis Variable, and then click ‘Plot run/year/variable’. Initially leave the X-axis as time. Within this figure the points indicate the daily mean for the variable while the envelope encompasses the diurnal variability (max and min). Variable names and units are based on a standard netCDF format. Try looking at a number of different output variables over different years. Try changing the X-axis to look at bivariate plots of how different output variables are related to one another. Be aware that PEcAn currently runs a moving min/mean/max through bivariate plots, just as it does with time series plots. In some cases this makes more sense than others. 29.2.8.9 Alternative Visualization: R Shiny Click on Open SHINY, which will open a new browser window. The shiny app will automatically access your run’s output files and allow you to visualize all output variables as a function of time. Use the pull down menu under Variable Name to choose whichever output variable you wish to plot. 29.2.8.10 Model Run Archive Return to the output window and Click on the HISTORY button. Click on any previous run in the “ID” column to go to the current state of that run’s execution – you can always return to old runs and runs in-progress this way. The run you just did should be the more recent entry in the table. For the next analysis, make note of the ID number from your run. 29.3 Next steps 29.3.0.0.1 Analyzing model output Follow this tutorial, [Analyze Output] to learn how to open model output in R and compare to observed data 29.3.0.1 DEMO 02 Demo 02: Sensitivity and Uncertainty Analysis will show how to perform Ensemble &amp; Sensitivity Analyses through the web interface and explore the PEcAn outputs in greater detail, including the trait meta-analysis "],
["demo-02-sensitivity-and-uncertainty-analysis.html", "30 Demo 02: Sensitivity and Uncertainty Analysis 30.1 Run Specification 30.2 Global Sensitivity: Shiny", " 30 Demo 02: Sensitivity and Uncertainty Analysis In Demo 2 we will be looking at how PEcAn can use information about parameter uncertainty to perform three automated analyses: Ensemble Analysis: Repeat numerous model runs, each sampling from the parameter uncertainty, to generate a probability distribution of model projections. Allows us to put a confidence interval on the model Sensitivity Analysis: Repeats numerous model runs to assess how changes in model parameters will affect model outputs. Allows us to identify which parameters the model is most sensitive to. Uncertainty Analysis: Combines information about model sensitivity with information about parameter uncertainty to determine the contribution of each model parameter to the uncertainty in model outputs. Allow us to identify which parameters are driving model uncertainty. 30.1 Run Specification Return to the main menu for the PEcAn web interface: URL &gt; Run Models Repeat the steps for site selection and run specification from Demo 01, but also click on “Advanced setup”, then click Next. By clicking Advanced setup, PEcAn will first show an Analysis Menu, where we are going to specify new settings. For an ensemble analysis, increase the number of runs in the ensemble, in this case set Runs to 50. In practice you would want to use a larger ensemble size (100-5000) than we are using in the demo. The ensemble analysis samples parameters from their posterior distributions to propagate this uncertainty into the model output. PEcAn’s sensitivity analysis holds all parameters at their median value and then varies each parameter one-at-a-time based on the quantiles of the posterior distribution. PEcAn also includes a handy shortcut, which is the default behavior for the web interface, that converts a specified standard deviation into its Normal quantile equivalent (e.g. 1 and -1 are converted to 0.157 and 0.841). In this example set Sensitivity to -2,-1,1,2 (the median value, 0, occurs by default). We also can tell PEcAn which variable to run the sensitivity on. Here, set Variables to NEE, so we can compare against flux tower NEE observations. Click Next 30.1.1 Additional Outputs: The PEcAn workflow will take considerably longer to complete since we have just asked for over a hundred model runs. Once the runs are complete you will return to the output visualization page were there will be a few new outputs to explore, as well as outputs that were present earlier that we’ll explore in greater details: 30.1.2 1. Run ID: While the sensitivity and ensemble analyses synthesize across runs, you can also select individual runs from the Run ID menu. You can use the Graphs menu to visualize each individual run, or open individual runs in Shiny 30.1.3 2. Inputs: This menu shows the contents of /run which lets you look at and download: A summary file (README.txt) describing each run: location, run ID, model, dates, whether it was in the sensitivity or ensemble analysis, variables modifed, etc. The model-specific input files fed into the model The jobs.sh file used to submit the model run 30.1.4 3. Outputs: This menu shows the contents of /out. A number of files generated by the underlying ecosystem model are archived and available for download. These include: Output files in the standardized netCDF ([year].nc) that can be downloaded for visualization and analysis (R, Matlab, ncview, panoply, etc) Raw model output in model-specific format (e.g. sipnet.out). Logfile.txt contains job.sh &amp; model error, warning, and informational messages 30.1.5 4. PFTs: This menu shows the contents of /pft. There is a wide array of outputs available that are related to the process of estimating the model parameters and running sensitivity/uncertainty analyses for a specific Plant Functional Type. TRAITS: The Rdata files trait.data.Rdata and madata.Rdata are, respectively, the available trait data extracted from the database that was used to estimate the model parameters and that same data cleaned and formatted for the statistical code. The list of variables that are queried is determined by what variables have priors associated with them in the definition of the PFTs. Priors are output into prior.distns.Rdata. Likewise, the list of species that are associated with a PFT determines what subset of data is extracted out of all data matching a given variable name. Demo 3 will demonstrate how a PFT can be created or modified. To look at these files in RStudio click on these files to load them into your workspace. You can further examine them in the Environment window or accessing them at the command line. For example, try typing names(trait.data) as this will tell you what variables were extracted, names(trait.data$Amax) will tell you the names of the columns in the Amax table, and summary(trait.data$Amax) will give you summary data about the Amax values. META-ANALYSIS: *.bug: The evaluation of the meta-analysis is done using a Bayesian statistical software package called JAGS that is called by the R code. For each trait, the R code will generate a [trait].model.bug file that is the JAGS code for the meta-analysis itself. This code is generated on the fly, with PEcAn adding or subtracting the site, treatment, and greenhouse terms depending upon the presence of these effects in the data itself. If the tag is set to FALSE then all random effects will be turned off even if there are multiple sites. meta-analysis.log contains a number of diagnostics, including the summary statistics of the model, an assessment of whether the posterior is consistent with the prior, and the status of the Gelman-Brooks-Rubin convergence statistic (which is ideally 1.0 but should be less than 1.1). ma.summaryplots.*.pdf are collections of diagnostic plots produced in R after the above JAGS code is run that are useful in assessing the statistical model. Open up one of these pdfs to evaluate the shape of the posterior distributions (they should generally be unimodal), the convergence of the MCMC chains (all chains should be mixing well from the same distribution), and the autocorrelation of the samples (should be low). traits.mcmc.Rdata contains the raw output from the statistical code. This includes samples from all of the parameters in the meta-analysis model, not just those that feed forward to the ecosystem, but also the variances, fixed effects, and random effects. post.distns.Rdata stores a simple tables of the posterior distributions for all model parameters in terms of the name of the distribution and its parameters. posteriors.pdf provides graphics showing, for each model parameter, the prior distribution, the data, the smoothed histogram of the posterior distribution (labeled post), and the best-fit analytical approximation to that smoothed histogram (labeled approx). Open posteriors.pdf and compare the posteriors to the priors and data SENSITIVITY ANALYSIS sensitivity.analysis.[RunID].[Variable].[StartYear].[EndYear].pdf shows the raw data points from univariate one-at-a-time analyses and spline fits through the points. Open this file to determine which parameters are most and least sensitive UNCERTAINTY ANALYSIS variance.decomposition.[RunID].[Variable].[StartYear].[EndYear].pdf, contains three columns, the coefficient of variation (normalized posterior variance), the elasticity (normalized sensitivity), and the partial standard deviation of each model parameter. Open this file for BOTH the soil and conifer PFTS and answer the following questions: The Variance Decomposition graph is sorted by the variable explaining the largest amount of variability in the model output (right hand column). From this graph identify the top-tier parameters that you would target for future constraint. A parameter can be important because it is highly sensitive, because it is highly uncertain, or both. Identify parameters in your output that meet each of these criteria. Additionally, identify parameters that are highly uncertain but unimportant (due to low sensitivity) and those that are highly sensitive but unimportant (due to low uncertainty). Parameter constraints could come from further literature synthesis, from direct measurement of the trait, or from data assimilation. Choose the parameter that you think provides the most efficient means of reducing model uncertainty and propose how you might best reduce uncertainty in this process. In making this choice remember that not all processes in models can be directly observed, and that the cost-per-sample for different measurements can vary tremendously (and thus the parameter you measure next is not always the one contributing the most to model variability). Also consider the role of parameter uncertainty versus model sensitivity in justifying your choice of what parameters to constrain. 30.1.6 PEcAn Files: This menu shows the contents of the root workflow folder that are not in one of the folders indicated above. It mostly contains log files from the PEcAn workflow that are useful if the workflow generates an error, and as metadata &amp; provenance (a detailed record of how data was generated). STATUS gives a summary of the steps of the workflow, the time they took, and whether they were successful pecan.*.xml are PEcAn settings files workflow.R is the workflow script workflow.Rout is the corresponding log file samples.Rdata contains the parameter values used in the runs. This file contains two data objects, sa.samples and ensemble.samples, that are the parameter values for the sensitivity analysis and ensemble runs respectively sensitivity.output.[RunID].[Variable].[StartYear].[EndYear].Rdata contains the object sensitivity.output which is the model outputs corresponding to the parameter values in sa.samples. ENSEMBLE ANALYSIS ensemble.Rdata contains contains the object ensemble.output, which is the model predictions at the parameter values given in ensemble.samples. ensemble.analysis.[RunID].[Variable].[StarYear].[EndYear].pdf contains the ensemble prediction as both a histogram and a boxplot. ensemble.ts.[RunID].[Variable].[StartYear].[EndYear].pdf contains a time-series plot of the ensemble mean, median, and 95% CI 30.2 Global Sensitivity: Shiny Navigate to URL/shiny/global-sensitivity. This app uses the output from the ENSEMBLE runs to perform a global Monte Carlo sensitivity analysis. There are three modes controlled by Output type: Pairwise looks at the relationship between a specific parameter (X) and output (Y) All parameters looks at how all parameters affect a specific output (Y) All variables looks at how all outputs are affected by a specific parameter(X) In all of these analyses, the app also fits a linear regression to these scatterplots and reports a number of summary statistics. Among these, the slope is an indicator of global sensitivity and the R2 is an indicator of the contribution to global uncertainty 30.2.1 Next Steps The next set of tutorials will focus on the process of data assimilation and parameter estimation. The next two steps are in “.Rmd” files which can be viewed online. 30.2.2 Assimilation ‘by hand’ Explore how model error changes as a function of parameter value (i.e. data assimilation ‘by hand’) 30.2.3 MCMC Concepts Explore Bayesian MCMC concepts using the photosynthesis module 30.2.4 More info about tools, analyses, and specific tasks… Additional information about specific tasks (adding sites, models, data; software updates; etc.) and analyses (e.g. data assimilation) can be found in the PEcAn documentation If you encounter a problem with PEcAn that’s not covered in the documentation, or if PEcAn is missing functionality you need, please search known bugs and issues, submit a bug report, or ask a question in our chat room. Additional questions can be directed to the project manager "],
["pecan-testing-the-sensitivity-analysis-against-observations.html", "31 PEcAn: Testing the Sensitivity Analysis Against Observations&quot; 31.1 Flux Measurements and Modeling Course, Tutorial Part 2 31.2 Introduction", " 31 PEcAn: Testing the Sensitivity Analysis Against Observations&quot; 31.0.1 Author: “Ankur Desai” 31.1 Flux Measurements and Modeling Course, Tutorial Part 2 This tutorial assumes you have successfully completed the Demo01, Demo02 and the modelVSdata tutorial. 31.2 Introduction Now that you have successfully run PEcAn through the web interface and have learned how to do a simple comparison of flux tower observations to model output, let’s start looking at how data assimilation and parameter estimation would work with an ecosystem model. Before we start a full data assimilation exercise, let’s try something simple – single parameter selection by hand. Open http://localhost:3280/rstudio or http://localhost:6480/rstudio or the Amazon URL/rstudio if running on the cloud. In Demo02, you have ran a sensitivity analysis of SIPNET model runs at Niwot Ridge sampling across quantiles of a parameter prior, while holding all others to the median value. The pecan.xml file told PEcAn to run an sensitivity analysis, which simply meant SIPNET was run multiple times with the same driver, but varying parameter values one at a time (while holding all others to their median), and the parameter range also specified in the pecan.xml file (as quantiles, which were then sampled against the BETY database of observed variation in the parameter for species within the specific plant functional type). Let’s try to compare Ameriflux NEE to SIPNET NEE across all these runs to make a plot of parameter vs. goodness-of-fit. We’ll start with root mean square error (RMSE), but then discuss some other tests, too. 31.2.0.1 A. Read in settings object from a run xml Open up a connection to the bety database and create a settings object from your xml, just like in the modelVSdata tutorial. We read in the pecan.CONFIG.xml instead of the pecan.xml because the pecan.CONFIG.xml has already incorperated some of the information from the database we would have to query again if we used the pecan.xml. Back to the files pane, within the run/ folder, find a folder called pft/ and within that a folder with the pft name (such as temprature.coniferous). Within that is a PDF file that starts sensitivity.analysis. In Rstudio, just click on the PDF to view it. You discussed this PDF last tutorial, through the web interface. Here, we see how the model NEE in SIPNET changes with each parameter. Let’s read that sensitivity output. Navigate back up (..) to the ~/output/RUNDIR/ folder. Find a series of files that end in “.RData”. These files contain the R variables used to make these plots. In particular, there is sensitivity.output..RData which contains the annual NEE as a function of each parameter quantile. Click on it to load a variable into your environment. There is sensitivity.results..RData which contains plotting functions and variance decomposition output, which we don’t need in this tutorial. And finally, there is **sensitivity.samples.*.RData** which contains the actual parameter values and the RunIDs associated with each sensitivity run. Click on sensitivity.samples..RData* to load it into your environment, or run the {r}load() script below. You should see a set of five new variables (pft.names, trait.names, sa.ensemble.id, sa.run.ids, sa.samples). Let’s extract a parameter and it’s sensitivity NEE output from the list sa.samples, which is organized by PFT, and then by parameter. First, let’s look at a list of PFTs and parameters available: Now to see the actual parameter values used by the runs, just pick a parameter and type: Let’s store that value for future use: Now, to see the annual NEE output from the model for a particular PFT and parameter range, try You could even plot the two: What do you notice? Let’s try to read the output from a single run id as you did in the earlier tutorial. 31.2.0.2 B. Now let’s bring in the actual observations Recall reading Ameriflux NEE in the modelVSdata tutorial. 31.2.0.3 C. Finally, we can finally compare model to data In the modelVSdata, you also compared NEE to the ensemble model run. Here we will do the same except we include each sensitivity run. And remember the formula for RMSE: All we need to do to go beyond this is to make a loop that reads in each sensitivity run NEE based on runids, calculates RMSE against the observations, and stores it in an array, by combining the steps above in a for loop. Make sure you change the directory names and year to your specific run. Let’s plot that array Can you identify a minimum (if there is one)? If so, is there any reason to believe this is the “best” parameter? Why or why not? Think about all the other parameters. Now that you have the hang of it, here are a few more things to try: Try a different error functions, given actual NEE uncertainty. You learned earlier that uncertainty in half-hourly observed NEE is not Gaussian. This makes RMSE not the correct measure for goodness-of-fit. Go to ~/pecan/modules/uncertainty/R, open flux_uncertainty.R, and click on the source button in the program editing pane. Alternatively, you can source the function from the console using: Then you can run: The figure shows you uncertainty (err) as a function of NEE magnitude (mag). How might you use this information to change the RMSE calculation? Try a few other parameters. Repeat the above steps but with a different parameter. You might want to select one from the sensitivity PDF that has a large sensitivity or from the variance decomposition that is also poorly constrained. "],
["simple-model-data-comparisons.html", "32 Simple Model-Data Comparisons 32.1 Starting RStudio Server 32.2 Read in settings From an XML file 32.3 Read in model output from specific variables 32.4 Compare model to flux observations", " 32 Simple Model-Data Comparisons 32.0.1 Author: Istem Fer, Tess McCabe In this tutorial we will compare model outputs to data outside of the PEcAn web interface. The goal of this is to demonstrate how to perform additional analyses using PEcAn’s outputs. To do this you can download each of the Output files, and then perform the analyses using whatever software you prefer, or you can perform analyses directly on the PEcAn server itself. Here we’ll be analyzing model outputs in R using a browser-based version of RStudio that’s installed on the server 32.1 Starting RStudio Server Open RStudio Server in a new window at URL/rstudio The username is carya and the password is illinois. To open a new R script click File &gt; New File &gt; R Script Use the Files browser on the lower right pane to find where your run(s) are located All PEcAn outputs are stored in the output folder. Click on this to open it up. Within the outputs folder, there will be one folder for each workflow execution. For example, click to open the folder PEcAn_99000000001 if that’s your workflow ID A workflow folder will have a few log and settings files (e.g. pecan.xml) and the following subfolders run contains all the inputs for each run out contains all the outputs from each run pft contains the parameter information for each PFT Within both the run and out folders there will be one folder for each unique model run, where the folder name is the run ID. Click to open the out folder. For our simple case we only did one run so there should be only one folder (e.g. 99000000001). Click to open this folder. Within this folder you will find, among other things, files of the format .nc. Each of these files contains one year of model output in the standard PEcAn netCDF format. This is the model output that we will use to compare to data. 32.2 Read in settings From an XML file 32.3 Read in model output from specific variables The arguments to read.output are the run ID, the folder where the run is located, the start year, the end year, and the variables being read. The README file in the Input file dropdown menu of any successful run lists the run ID, the output folder, and the start and end year. 32.4 Compare model to flux observations First load up the observations and take a look at the contents of the file File_Path refers to where you stored your observational data. In this example the default file path is an Ameriflux dataset from Niwot Ridge. File_format queries the database for the format your file is in. The defualt format ID “5000000002” is for csv files downloaded from the Ameriflux website. You could query for diffent kinds of formats that exist in bety or make your own. Here 772 is the database site ID for Niwot Ridge Forest, which tells pecan where the data is from and what time zone to assign any time data read in. Second apply a conservative u* filter to observations Third align model output and observations When we aligned the data, we got a dataframe with the variables we requested in a \\(NEE.m\\) and a \\(NEE.o\\) format. The \\(.o\\) is for observations, and the \\(.m\\) is for model. The posix column allows for easy plotting along a timeseries. Fourth, plot model predictions vs. observations and compare this to a 1:1 line Fifth, calculate the Root Mean Square Error (RMSE) between the model and the data na.rm makes sure we don’t include missing or screened values in either time series. Finally, plot time-series of both the model and data together Bonus How would you compare aggregated data? Try RMSE against monthly NEE instead of half-hourly. In this case, first average the values up to monthly in the observations. Then, use align_data to match the monthly timestep in model output. NOTE: Align_data uses two seperate alignment function, match_timestep and mean_over_larger_timestep. Match_timestep will use only that data that is present in both the model and the observation. This is helpful for sparse observations. Mean_over_larger_timestep aggregates the values over the largest timestep present. If you were to look at averaged monthly data, you would use mean_over_larger_timestep. "],
["parameter-data-assimilation-1.html", "33 Parameter Data Assimilation 33.1 Objectives 33.2 Larger Context 33.3 Connect to Rstudio 33.4 Defining variables 33.5 Initial Ensemble Analysis 33.6 Choosing Parameters 33.7 Editing PEcAn settings 33.8 Investigating PEcAn function pda.emulator (optional) 33.9 Running a demo PDA 33.10 Outputs from PEcAn’s Parameter Data Assimilation 33.11 Post-PDA analyses", " 33 Parameter Data Assimilation 33.1 Objectives Gain hands-on experience in using Bayesian MCMC techniques to calibrate a simple ecosystem model using parameter data assimilation (PDA) Set up and run a PDA in PEcAn using model emulation technique, assimilating NEE data from Niwot Ridge Examine outputs from a PDA for the SIPNET model and evaluation of the calibrated model against i) data used to constrain model, ii) additional data for the same site 33.2 Larger Context Parameter data assimilation (PDA) occurs as one step in the larger process of model calibration, validation, and application. The goal of PDA is to update our estimates of the posterior distributions of the model parameters using data that correspond to model outputs. This differs from our previous use of PEcAn to constrain a simple model using data that map directly to the model parameters. Briefly, the recommended overall approach for model calibration and validation consists of the following steps: Assemble and process data sets required by the model as drivers Perform an initial test-run of the model as a basic sanity check Were there errors in drivers? (return to 1) Is the model in the same ballpark as the data? Construct priors for model parameters Collect/assemble the data that can be used to constrain model parameters and outputs Meta-analysis Sensitivity analysis (SA) Variance Decomposition (VD) Determine what parameters need further constraint Does this data exist in the literature? (repeat 4-8) Can I collect this data in the field? (repeat 4-8) Ensemble Analysis Is reality within the range of the uncertainty in the model? Evaluate/estimate uncertainties in the data Parameter Data Assimilation: Propose new parameter values Evaluate L(data | param) &amp; prior(param) Accept or reject the proposed parameter values Repeat many times until a histogram of accepted parameter values approximates the true posterior distribution. Model evaluation [preferably ensemble based] Against data used to constrain model Against additional data for this site Same variable, different time Different variables Against data at a new site Do I need more data? Repeat 4-9 (direct data constraint) or 6-11 (parameter data assimilation). Application [preferably ensemble forecast or hindcast] 33.3 Connect to Rstudio Today, we’re again going to work mostly in Rstudio, in order to easily edit advanced PEcAn settings and browse files. So if you haven’t already, connect now to the Rstudio server on your VM ([URL]/rstudio). This tutorial assumes you have successfully completed an ensemble and a sensitivity analysis (Demo 2) before. 33.4 Defining variables The following variables need to be set specific to the site being run and the workflow being run 33.5 Initial Ensemble Analysis A good place to start when thinking about a new PDA analysis is to look at the current model fit to observed data. In fact, we want to compare data to a full ensemble prediction from the model. This is important because our current parameter distributions will be the priors for PDA. While the analysis will translate these priors into more optimal (in terms of producing model output that matches observations) and more confident (i.e. narrower) posterior distributions, these results are inherently constrained by the current parameter distributions. Thus, if reality falls far outside the prior ensemble confidence interval (which reflects the current uncertainty of all model parameters), data assimilation will not be able to fix this. In such cases, the prior parameter estimates must already be over-constrained, or there are structural errors in the model itself that need fixing. To begin, let’s load up some NEE observations so that we can plot them along with our ensemble predictions. In the code below the elements in bold may vary depending on site and your previous runs. Now let’s load up our ensemble outputs from the previous ensemble analysis (Demo 2) and plot our ensemble predictions against our NEE observations. When interpreting your results it is important to remember the difference between a confidence interval, which just includes parameter uncertainties, and a predictive interval, which includes parameter and residual uncertainties. Your ensemble analysis plot illustrates the former—i.e., the confidence in the mean NEE. By contrast, the data reflect both changes in mean NEE, and random variability. As such, we can’t expect all the data to fall within the CI; in fact, if we had unlimited data to constrain mean NEE, the CI would collapse to a single line and none of the data would be contained! However, your plot will give you an idea of how much uncertainty there is in your model currently, and help to identify systematic errors like bias (values consistently too high or low) or poorly represented seasonal patterns. 33.5.1 Questions: Does your ensemble agree well with the data? If so, how much room for improvement is there, in terms of tightening the CI? If not, what are the greatest discrepancies? What are some of the problems (with model, data, and/or PEcAn) that might explain the data-model disparity you see? 33.6 Choosing Parameters Beyond exploratory exercises, the first step of PDA analysis is to choose the model parameters you will target for optimization. PDA is computationally expensive (even when using an emulator), and the cost increases exponentially with the number of parameters targeted. The number you can handle in any given analysis completely depends on the complexity of the model and your available computational resources, but in practice it’s going to be rather small (~1–10) relative to the large number of parameters in a mechanistic ecosystem model (~10–100). Given this limitation, it is important to target parameters that can contribute substantially to improving model fit. If you recall, identifying those parameters was the goal of the uncertainty analysis you conducted previously, in the second PEcAn demo. Let’s revisit that analysis now. Open your variance decomposition graph from Demo 2 From this figure decide which variables you will target with PDA. As noted, an obvious criterion is that the parameter should be contributing a large amount of uncertainty to the current model, because otherwise it simply can’t change the model output much no matter how much you try to optimize it. But there are other considerations too. For example, if two parameters have similar or competing roles in the model, you may have trouble optimizing both simultaneously. In practice, there will likely be some guess-and-testing involved, though a good understanding of how the model works will help. It may also help to look at the shape of the Sensitivity responses and details of model fit to data (your ensemble analysis from the previous section). For the purposes of this demo, choose eight to ten parameters (in total, if you have more than one PFT) that contribute high uncertainty to model output and/or seem like good choices for some other rational reason. 33.6.1 Questions: Which parameters did you choose, and why? 33.7 Editing PEcAn settings Now let’s add settings to tell PEcAn how to run the PDA with emulator, we will come to the details of model emulation later. Open up the pecan.CONFIGS.xml file you located previously, and choose File &gt; Save as... from the menu to save a new copy as pecan.PDA.xml. Now add the block of XML listed below to the file, immediately after the line. Check and fill in the parts corresponding to your run when necessary. In this block, use the &lt;param.names&gt;&lt;param&gt; tags to identify the parameters you’ve chosen for PDA (it’s up to you to choose the number of parameters you want to constrain, then you can set the &lt;n.knot&gt; to be &gt;= 10 per parameter you choose, e.g. 200 knots for 10 parameters). Here, you need to use PEcAn’s standard parameter names, which are generally not the same as what’s printed on your variance decomposition graph. To find your parameters look at the row names in the prior.distns.csv file for each PFT under the PFT pulldown menu. Insert the variable name (exactly, and case sensitive) into the &lt;param&gt; tags of the XML code below. In addition, you may need to edit &lt;inputs&gt;&lt;file&gt;&lt;path&gt;, depending on the site and year you ran previously. The rest of the settings control options for the PDA analysis (how long to run, etc.), and also identify the data to be used for assimilation. For more details, see the assim.batch vignette on the PEcAn GitHub page (https://goo.gl/9hYVPQ). &lt;?xml version=&quot;1.0&quot;?&gt; &lt;-- These lines are already in there. Don&#39;t duplicate them, &lt;pecan&gt; &lt;-- just paste the &lt;assim.batch&gt; block below right after them. &lt;assim.batch&gt; &lt;method&gt;emulator&lt;/method&gt; &lt;n.knot&gt;160&lt;/n.knot&gt; &lt;-- FILL IN &lt;iter&gt;25000&lt;/iter&gt; &lt;chain&gt;3&lt;/chain&gt; &lt;param.names&gt; &lt;soil&gt; &lt;param&gt;YOUR_PFT_1_PARAM_1&lt;/param&gt; &lt;-- FILL IN &lt;param&gt;YOUR_PFT_1_PARAM_2&lt;/param&gt; &lt;-- FILL IN &lt;/soil&gt; &lt;temperate.coniferous&gt; &lt;param&gt;YOUR_PFT_2_PARAM_1&lt;/param&gt; &lt;-- FILL IN &lt;param&gt;YOUR_PFT_2_PARAM_2&lt;/param&gt; &lt;-- FILL IN &lt;param&gt;YOUR_PFT_2_PARAM_3&lt;/param&gt; &lt;-- FILL IN &lt;param&gt;YOUR_PFT_2_PARAM_4&lt;/param&gt; &lt;-- FILL IN &lt;param&gt;YOUR_PFT_2_PARAM_5&lt;/param&gt; &lt;-- FILL IN &lt;param&gt;YOUR_PFT_2_PARAM_6&lt;/param&gt; &lt;-- FILL IN &lt;/temperate.coniferous&gt; &lt;/param.names&gt; &lt;jump&gt; &lt;adapt&gt;100&lt;/adapt&gt; &lt;adj.min&gt;0.1&lt;/adj.min&gt; &lt;ar.target&gt;0.3&lt;/ar.target&gt; &lt;/jump&gt; &lt;inputs&gt; &lt;file&gt; &lt;path&gt; &lt;path&gt;/home/carya/output/dbfiles/AmerifluxLBL_site_0-772/AMF_US-NR1_BASE_HH_9-1.csv&lt;/path&gt; &lt;/path&gt; &lt;format&gt;5000000002&lt;/format&gt; &lt;input.id&gt;1000011238&lt;/input.id&gt; &lt;-- FILL IN, from BETY inputs table, this is *NOT* the workflow ID &lt;likelihood&gt;Laplace&lt;/likelihood&gt; &lt;variable.name&gt; &lt;variable.name&gt;NEE&lt;/variable.name&gt; &lt;variable.name&gt;UST&lt;/variable.name&gt; &lt;/variable.name&gt; &lt;variable.id&gt;297&lt;/variable.id&gt; &lt;/file&gt; &lt;/inputs&gt; &lt;/assim.batch&gt; Once you’ve made and saved the changes to your XML, load the file and check that it contains the new settings: If the printed list contains everything you just added to pecan.PDA.xml, you’re ready to proceed. 33.8 Investigating PEcAn function pda.emulator (optional) Before we run the data assimilation, let’s take a high-level look at the organization of the code. Use the Rstudio file browser to open up ~/pecan/modules/assim.batch/R/pda.emulator.R. This code works in much the same way as the pure statistical models that we learned about earlier in the week, except that the model being fit is a statistical model that emulates a complicated process-based computer simulation (i.e., an ecosystem model). We could have directly used the ecosystem model (indeed PEcAn’s other PDA functions perform MCMC by actually running the ecosystem model at each iteration, see pda.mcmc.R script as an example), however, this would require a lot more computational time than we have today. Instead here we will use a technique called model emulation. This technique allows us to run the model for a relatively smaller number of times with parameter values that have been carefully chosen to give a good coverage of parameter space. Then we can interpolate the likelihood calculated for each of those runs to get a surface that “emulates” the true likelihood and perform regular MCMC, except instead of actually running the model on every iteration to get a likelihood, this time we will just get an approximation from the likelihood emulator. The general algorithm of this method can be further expressed as: Propose initial parameter set sampling design Run full model for each parameter set Evaluate the likelihoods Construct emulator of multivariate likelihood surface Use emulator to estimate posterior parameter distributions (Optional) Refine emulator by proposing new design points, goto 2) For now, we just want you to get a glimpse at the overall structure of the code, which is laid out in the comment headers in pda.emulator(). Most of the real work gets done by the functions this code calls, which are all located in the file ~/pecan/modules/assim.batch/R/pda.utils.R and the MCMC will be performed by the mcmc.GP() function in ~/pecan/modules/emulator/R/minimize.GP.R. To delve deeper into how the code works, take a look at these files when you have the time. 33.9 Running a demo PDA Now we can go ahead and run a data assimilation MCMC with emulator. Since you’ve already loaded the settings containing your modified XML block, all you need to do to start the PDA is run pda.emulator(settings). But, in the emulator workflow, there is a bit of a time consuming step where we calculate the effective sample size of the input data, and we have already done this step for you. You could load it up and pass it to the function explicitly in order to skip this step: After executing the code above, you will see print-outs to the console. The code begins with loading the prior values which in this case are the posterior distributions coming from your previous meta analysis. Then, normally, it loads the observational data and carries out necessary conversions and formatting to align it with model outputs, as we did separately above, but today it will skip this step as we are passing data externally. After this step, you will see a progress bar where the actual model is run n.knot times with the proposed parameter sets and then the outputs from these runs are read. Next, this model output is compared to the specified observational data, and the likelihood is calculated using the heteroskedastic Laplacian discussed previously. Once we calculate the likelihoods, we fit an emulator which interpolates the model output in parameter space between the points where the model has actually been run. Now we can put this emulator in the MCMC algorithm instead of the model itself. Within the MCMC loop the code proposes new parameter value from a multivariate normal jump distribution. The corresponding likelihood will be approximated by the emulator and the new parameter value is accepted or rejected based on its posterior probability relative to the current value. 33.10 Outputs from PEcAn’s Parameter Data Assimilation When the PDA is finished, a number of outputs are automatically produced that are either the same as or similar to posterior outputs that we’ve seen before. These are located in the PEcAn_[workflow_id]/pft/* output directory and are identified by pda.[PFT]_[workflow_id] in the filenames: posteriors.pda.[PFT]*.pdf shows the posterior distributions resulting from your PDA trait.mcmc.pda.[PFT]*.Rdata contains all the parameter samples contained in the PDA posterior mcmc.pda.[PFT]*.Rdata is essentially the same thing in a different format mcmc.diagnostics.pda.[PFT]*.pdf shows trace plots and posterior densities for each assimilated parameter, as well as pairs plots showing all pairwise parameter correlations. Together, these files allow you to evaluate whether a completed PDA analysis has converged and how the posterior distributions compare to the priors, and to use the posterior samples in further analyses, including additional PDA. If you haven’t done so already, take a look at all of the outputs described here. 33.10.1 Questions: Do the diagnostic figures indicate that your likelihood at least improved over the course of the analysis? Does the MCMC appear to have converged? Are the posterior distributions well resolved? 33.11 Post-PDA analyses In addition to the outputs of the PDA itself, you may want to conduct ensemble and/or sensitivity analyses based on the posteriors of the data assimilation, in order to check progress towards improved model fit and/or changing sensitivity. For this, you need to generate new model runs based on parameters sampled from the updated (by PDA) posterior, which is a simple matter of rerunning several steps of the PEcAn workflow. The PDA you ran has automatically produced an updated XML file (pecan.pda***.xml) that includes the posterior id to be used in the next round of runs. Locate this file in your run directory and load the file for the post-pda ensemble/sensitivity analysis (if you already have the settings list in your working environment you don’t need to re-read the settings): Now you can check the new figures produced by your analyses under PEcAn_[workflow_id]/pft/*/variance.decomposition.*.pdf and PEcAn_[workflow_id]/pft/*/sensitivity.analysis.*.pdf, and compare them to the previous ones. Also, take a look at the comparison of model outputs to data when we run SIPNET with pre- and post-PDA parameter (mean) values under PEcAn_[workflow_id]/model.data.comparison.pdf. 33.11.1 Questions: Looking at the ensemble analysis outputs in order (i.e., in order of increasing ID in the filenames), qualitatively how did the model fit to data change over the course of the analysis? Based on the final ensemble analysis, what are the major remaining discrepancies between model and data? Can you think of the processes / parameters that are likely contributing to the differences? What would be your next steps towards evaluating or improving model performance? "],
["state-variable-data-assimilation.html", "34 State-Variable Data Assimilation 34.1 Objectives: 34.2 Overview: 34.3 Allometric equations 34.4 Estimate stand-level NPP 34.5 Build Initial Conditions 34.6 Load Priors 34.7 Ensemble Kalman Filter 34.8 Finishing up", " 34 State-Variable Data Assimilation 34.1 Objectives: Assimilate tree ring estimated NPP &amp; inventory AGB within the SIPNET model in order to: Reconcile data and model NPP &amp; AGB estimates Constrain inferences about other ecosystem responses. 34.2 Overview: Initial Run Settings Load and match plot and increment data Estimate tree-level data uncertainties Estimate allometric relationships Estimate stand-level NPP Sample initial conditions and parameters Run Ensemble Kalman Filter 34.2.1 Initial Run Perform a site-level SIPNET run using the following settings Site = UNDERC Start = 01/01/1979 End = 12/31/2015 Met = NARR Check Brown Dog When the run is complete, open the pecan.xml and cut-and-paste the outdir for later use 34.2.2 Settings: Open the PEcAn RStudio environment back up. Set your working directory to the outdir from above setwd(outdir) and shift the file browser to that location (Files &gt; More &gt; Go To Working Directory) Open up the latest settings file pecan.CONFIG.xml. At the top of the file add the following tags to set the ensemble size &lt;state.data.assimilation&gt; &lt;n.ensemble&gt;35&lt;/n.ensemble&gt; &lt;process.variance&gt;FALSE&lt;/process.variance&gt; &lt;sample.parameters&gt;TRUE&lt;/sample.parameters&gt; &lt;data&gt; &lt;format_id&gt;1000000040&lt;/format_id&gt; &lt;input.id&gt;1000013298&lt;/input.id&gt; &lt;/data&gt; &lt;state.variables&gt; &lt;variable&gt; &lt;variable.name&gt;NPP&lt;/variable.name&gt; &lt;unit&gt;MgC/ha/yr&lt;/unit&gt; &lt;min_value&gt;-9999&lt;/min_value&gt; &lt;max_value&gt;9999&lt;/max_value&gt; &lt;/variable&gt; &lt;variable&gt; &lt;variable.name&gt;AbvGrndWood&lt;/variable.name&gt; &lt;unit&gt;KgC/m^2&lt;/unit&gt; &lt;min_value&gt;0&lt;/min_value&gt; &lt;max_value&gt;9999&lt;/max_value&gt; &lt;/variable&gt; &lt;variable&gt; &lt;variable.name&gt;TotSoilCarb&lt;/variable.name&gt; &lt;unit&gt;KgC/m^2&lt;/unit&gt; &lt;min_value&gt;0&lt;/min_value&gt; &lt;max_value&gt;9999&lt;/max_value&gt; &lt;/variable&gt; &lt;variable&gt; &lt;variable.name&gt;LeafC&lt;/variable.name&gt; &lt;unit&gt;m^2/m^2&lt;/unit&gt; &lt;min_value&gt;0&lt;/min_value&gt; &lt;max_value&gt;9999&lt;/max_value&gt; &lt;/variable&gt; &lt;variable&gt; &lt;variable.name&gt;SoilMoistFrac&lt;/variable.name&gt; &lt;unit&gt;&lt;/unit&gt; &lt;min_value&gt;0&lt;/min_value&gt; &lt;max_value&gt;9999&lt;/max_value&gt; &lt;/variable&gt; &lt;variable&gt; &lt;variable.name&gt;SWE&lt;/variable.name&gt; &lt;unit&gt;cm&lt;/unit&gt; &lt;min_value&gt;0&lt;/min_value&gt; &lt;max_value&gt;9999&lt;/max_value&gt; &lt;/variable&gt; &lt;variable&gt; &lt;variable.name&gt;Litter&lt;/variable.name&gt; &lt;unit&gt;gC/m^2&lt;/unit&gt; &lt;min_value&gt;0&lt;/min_value&gt; &lt;max_value&gt;9999&lt;/max_value&gt; &lt;/variable&gt; &lt;/state.variables&gt; &lt;forecast.time.step&gt;year&lt;/forecast.time.step&gt; &lt;start.date&gt;1980/01/01&lt;/start.date&gt; &lt;end.date&gt;2015/12/31&lt;/end.date&gt; &lt;/state.data.assimilation&gt; Delete the `&lt;pfts&gt; block from the settings In the PEcAn History, go to your PDA run and open pecan.pda[UNIQUEID].xml (the one PEcAn saved for you AFTER you finished the PDA) Cut-and-paste the PDA &lt;pfts&gt; block into the SDA settings file Save the file as pecan.SDA.xml 34.2.3 Loading data If you have not done so already, clone (new) or pull (update) the PalEON Camp2016 repository Open a shell under Tools &gt; Shell `cd to go to your home directory To clone: git clone git@github.com:PalEON-Project/Camp2016.git To pull: cd Camp2016; git pull https://github.com/PalEON-Project/Camp2016.git master Open the tree-ring data assimilation workflow under Home &gt; pecan &gt; scripts &gt; workflow.treering.R Run the script from the start up through the LOAD DATA section 34.2.4 Estimating tree-level data uncertainties One thing that is critical for data assimilation, whether it is being used to estimate parameters or state variables, is the careful consideration and treatment of the uncertainties in the data itself. For this analysis we will be using a combination of forest plot and tree ring data in order to estimate stand-level productivity. The basic idea is that we will be using the plot-sample of measured DBHs as an estimate of the size structure of the forest, and will use the annual growth increments to project that forest backward in time. Tree biomass is estimated using empirical allometric equations relating DBH to aboveground biomass. There are a number of sources of uncertainty in this approach, and before moving you are encouraged to think about and write down a few: Today we will use a statistical model based on the model developed by Clark et al 2007 that partitions out a number of sources of variability and uncertainty in tree ring and plot data (Fig 1). This model is a Bayesian statespace model that treats the true diameters (D) and increments (X) as latent variables that are connected through a fairly simple mixed effects process model \\[D_{ij,t+1} = D_{ij,t} + \\mu + \\alpha_{i} + \\alpha_t + \\epsilon_{ij,t}\\] where i = individual, j = plot, t = time (year). Each of these terms are represented at normal distributions, where \\(\\mu\\) is a fixed effect (overall mean growth rate) and individual and year are random effects \\[\\mu \\sim N(0.5,0.5)\\] \\[\\alpha_{i} \\sim N(0,\\tau_{i})\\] \\[\\alpha_{t} \\sim N(0,\\tau_{t})\\] \\[\\epsilon_{ij,t} \\sim N(0,\\tau_{e})\\] The connection between the true (latent) variable and the observations is also represented as normal with these variances representing measurement error: \\[D_{ij,t}^O \\sim N( D_{ij,t},\\tau_D)\\] \\[X_{ij,t}^O \\sim N( X_{ij,t},\\tau_r)\\] Finally, there are five gamma priors on the precisions, one for the residual process error (\\(\\tau_{e}\\)), two for the random effects on individual (\\(\\tau_{i}\\)) and time (\\(\\tau_t\\)), and two measurement errors or DBH (\\(\\tau_D\\)) and tree rings (\\(\\tau_r\\)) \\[\\tau_{e} \\sim Gamma(a_e,r_e)\\] \\[\\tau_{i} \\sim Gamma(a_i,r_i)\\] \\[\\tau_{t} \\sim Gamma(a_t,r_t)\\] \\[\\tau_{D} \\sim Gamma(a_D,r_D)\\] \\[\\tau_{r} \\sim Gamma(a_r,r_r)\\] This model is encapsulated in the PEcAn function: InventoryGrowthFusion(combined,n,iter) where the first argument is the combined data set formatted for JAGS and the second is the number of MCMC interations. The model itself is written for JAGS and is embedded in the function. Running the above InventoryGrowthFusion will run a full MCMC algorithm, so it does take a while to run. The code returns the results as an mcmc.list object, and the next line in the script saves this to the outputs directory We then call the function InventoryGrowthFusionDiagnostics to print out a set of MCMC diagnostics and example time-series for growth and DBH. 34.3 Allometric equations Aboveground NPP is estimated as the increment in annual total aboveground biomass. This estimate is imperfect, but not unreasonable for demonstration purposes. As mentioned above, we will take an allometric approach of scaling from diameter to biomass: Biomass = b0 * DBH b1 We will generate the allometric equation on a PFT level using another Bayesian model that synthesizes across a database of species-level allometric equations (Jenkins et al 2004). This model has two steps within the overall MCMC loop. First it simulates data from each equation, including both parameter and residual uncertainties, and then it updated the parameters of a single allometric relationship across all observations. The code also fits a second model, which includes a random site effect, but for simplicity we will not be using output from this version. Prior to running the model we have to first query the species codes for our pfts. Next we pass this PFT list to the model, AllomAve, which saves the results to the output directory in addition to returning a summary of the parameters and covariances. 34.4 Estimate stand-level NPP If we have no uncertainty in our data or allometric equations, we could estimate the stand aboveground biomass (AGB) for every year by summing over the biomass of all the trees in the plot and then divide by the plot area. We would then estimate NPP by the difference in AGB between years. One approach to propagating uncertainties into NPP would be to transform the distribution of DBH for each individual tree and year into a distribution for biomass, then sum over those distributions to get a distribution for AGB and then subtract the distributions to get the distributions of NPP. However, if we do this we will greatly overestimate the uncertainty in NPP because we ignore the fact that our increment data has much lower uncertainty than our diameter data. In essence, if we take a random draw from a distribution of AGB in year and it comes back above average, the AGB is much more likely to also be above average the following year than if we were to do an independent draw from that distribution. Accounting for this covariance requires a fairly simple change in our approach and takes advantage of the nature of the MCMC output. The basic idea is that we are going to take a random draw from the full individual x year diameter matrix, as well as a random draw of allometric parameters, and perform the ‘zero-error’ calculation approach described above. We will then create a distribution of all the NPP estimates that comes out of repeated draws for the full diameter matrix. This approach is encapsulated in the function plot2AGB. The argument unit.conv is a factor that combines both the area of the plot and the unit conversion from tree biomass (kg/tree) to stand AGB (Mg/ha). There are two outputs from plot2AGB: a pdf depicting estimated NPP and AGB (mean and 95% CI) time series, with each page being a plot; and plot2AGB.Rdata, a binary record of the function output that is read into the data assimilation code. The latter is also returned from the fuction and assigned to the variable “state”. Finally, we calculate the mean and standard deviation of NPP and save this as obs. 34.5 Build Initial Conditions The function sample.IC.SIPNET uses the AGB estimate from the previous function in order to initialize the data assimilation routine. Specifically it samples n.ensemble values from the first time step of the AGB estimate. Embedded in this function are also a number of prior distributions for other state variables, which are also samples in order to create a full set of initial conditions for SIPNET. 34.6 Load Priors The function sample.parameters samples values from the most recent posterior parameter distributions. You can also specify a specific set of parameters so sample from by specifying the argument &lt;prior&gt; within &lt;assim.sequential&gt; as the posterior.id you want to use. This is useful to know if you want to go back and run with the Meta-analysis posteriors, or if you end up rerunning the meta-analysis and need to go back and specify the parameter data assimilation posteriors instead of the most recent. 34.7 Ensemble Kalman Filter The function sda.enkf will run SIPNET in Ensemble Kalman Filter mode. The output of this function will be all the of run outputs, a PDF of diagnostics, and an Rdata object that includes three lists: FORECAST will be the ensemble forecasts for each year ANALYSIS will be the updated ensemble sample given the NPP observations enkf.params contains the prior and posterior mean vector and covariance matrix for each time step. If you look within this function you will find that much of the format is similar to the pda.mcmc function, but in general is much simpler. The function begins by setting parameters, opening a database connection, and generating workflow and ensemble ID’s. Next we split the SIPNET clim meteorology file up into individual annual files since we will be running SIPNET for a year at a time between updates. Next we perform an initial set of runs starting from the initial states and parameters we described above. In doing so we create the run and output directories, the README file, and the runs.txt file that is read by start.model.runs. Worth noting is that the README and runs.txt don’t need to be updated within the forecast loop. Given this initial run we then enter the forecast loop. Within this loop over years we perform four basic steps. First, we read the output from the latest runs. Second, we calculate the updated posterior state estimates based on the model ensemble prior and observation likelihood. Third, we resample the state estimates based on these posterior parameters. Finally, we start a new set of runs based on this sample. The sda.enfk function then ends by saving the outputs and generating some diagnostic figures. The first set of these shows the data, forecast, analysis. The second set shows pairs plots of the covariance structure for the Forecast and Analysis steps. The final set shows the time-series plots for the Analysis of the over state variables produced by SIPNET. 34.8 Finishing up The final bit of code in the script will register the workflow as complete in the database. After this is run you should be able to find all of the runs, and all of the outputs generated above, from within the PEcAn webpages. "],
["data-assimilation-concepts.html", "35 Data Assimilation Concepts&quot; 35.1 Fitting the model 35.2 What’s going on 35.3 Evaluating the model output 35.4 Additional information 35.5 Citations", " 35 Data Assimilation Concepts&quot; The goal of this tutorial is to help you gain some hands-on familiarity with some of the concepts, tools, and techniques involved in Bayesian Calibration. As a warm-up to more advanced approaches to model-data fusion involving full ecosystem models, this example will focus on fitting the Farquhar, von Caemmerer, and Berry (1980) photosynthesis model [FvCB model] to leaf-level photosynthetic data. This is a simple, nonlinear model consisting of 3 equations that models net carbon assimilation, \\(A^{(m)}\\), at the scale of an individual leaf as a function of light and CO2. \\[A_j = \\frac{\\alpha Q}{\\sqrt{1+(\\alpha^2 Q^2)/(Jmax^2)}} \\frac{C_i- \\Gamma}{4 C_i + 8 \\Gamma}\\] \\[A_c = V_{cmax} \\frac{C_i - \\Gamma}{C_i+ K_C (1+[O]/K_O) }\\] \\[A^{(m)} = min(A_j,A_c) - r\\] The first equation \\(A_j\\) describes the RUBP-regeneration limited case. In this equation the first fraction is a nonrectangular hyperbola predicting \\(J\\), the electron transport rate, as a function of incident light \\(Q\\), quantum yield \\(\\alpha\\), and the assymptotic saturation of \\(J\\) at high light \\(J_{max}\\). The second equation, \\(A_c\\), describes the Rubisco limited case. The third equation says that the overall net assimilation is determined by whichever of the two above cases is limiting, minus the leaf respiration rate, \\(r\\). To keep things simple, as a Data Model (a.k.a. Likelihood or Cost Function) we’ll assume that the observed leaf-level assimilation \\(A^{(o)}\\) is Normally distributed around the model predictions with observation error \\(\\tau\\). \\[A^{(o)} \\sim N(A^{(m)},\\tau)\\] To fit this model to data we’re going to rely on a piece of statistical software known as JAGS. The above model would be written in JAGS as: model{ ## Priors Jmax ~ dlnorm(4.7,2.7) ## maximum electron transport rate prior alpha~dnorm(0.25,100) ##quantum yield (mol electrons/mole photon) prior vmax ~dlnorm(4.6,2.7) ## maximum rubisco capacity prior r ~ dlnorm(0.75,1.56) ## leaf respiration prior cp ~ dlnorm(1.9,2.7) ## CO2 compensation point prior tau ~ dgamma(0.1,0.1) for(i in 1:n){ ## electron transport limited Aj[i]&lt;-(alpha*q[i]/(sqrt(1+(alpha*alpha*q[i]*q[i])/(Jmax*Jmax))))*(pi[i]-cp)/(4*pi[i]+8*cp) ## maximum rubisco limited without covariates Ac[i]&lt;- vmax*(pi[i]-cp)/(pi[i]+Kc*(1+po/Ko)) Am[i]&lt;-min(Aj[i], Ac[i]) - r ## predicted net photosynthesis Ao[i]~dnorm(Am[i],tau) ## likelihood } } The first chunk of code defines the prior probability distributions. In Bayesian inference every unknown parameter that needs to be estimated is required to have a prior distribution. Priors are the expression of our belief about what values a parameter might take on prior to observing the data. They can arise from many sources of information (literature survey, meta-analysis, expert opinion, etc.) provided that they do not make use of the data that is being used to fit the model. In this particular case, the priors were defined by Feng and Dietze 2013. Most priors are lognormal or gamma, which were choosen because most of these parameters need to be positive. After the priors is the Data Model, which in JAGS needs to be implemented as a loop over every observation. This is simply a codified version of the earlier equations. Table 1: FvCB model parameters in the statistical code, their symbols in equations, and definitions Parameter Symbol Definition alpha0 \\(\\alpha\\) quantum yield (mol electrons/mole photon) Jmax \\(J_{max}\\) maximum electron transport cp \\(\\Gamma\\) CO2 compensation point vmax0 \\(V_{cmax}\\) maximum Rubisco capacity (a.k.a Vcmax) r \\(R_d\\) leaf respiration tau \\(\\tau\\) residual precision q \\(Q\\) PAR pi \\(C_i\\) CO2 concentration 35.1 Fitting the model To begin with we’ll load up an example A-Ci and A-Q curve that was collected during the 2012 edition of the Flux Course at Niwot Ridge. The exact syntax below may be a bit confusing to those unaccustomed to R, but the essence is that the filenames line is looking up where the example data is stored in the PEcAn.photosynthesis package and the dat line is loading up two files (once A-Ci, the other A-Q) and concatanating them together. In PEcAn we’ve written a wrapper function, \\(fitA\\), around the statistical model discussed above, which has a number of other bells and whistles discussed in the PEcAn Photosynthesis Vignette. For today we’ll just use the most basic version, which takes as arguments the data and the number of MCMC iterations we want to run. 35.2 What’s going on Bayesian numerical methods for model calibration are based on sampling parameter values from the posterior distribution. Fundamentally what’s returned is a matrix, with the number of iterations as rows and the number of parameters as columns, which are samples from the posterior distribution, from which we can approximate any quantity of interest (mean, median, variance, CI, etc.). The following plots follow the trajectory of two correlated parameters, Jmax and alpha. In the first figure, arrows show the first 10 iterations. Internally JAGS is choosing between a variety of different Bayesian sampling methods (e.g. Metropolis-Hasting, Gibbs sampling, slice sampling, rejection sampling, etc) to draw a new value for each parameter conditional on the current value. After just 10 steps we don’t have a good picture of the overall posterior, but it should still be clear that the sampling is not a complete random walk. After 100 steps, we can see a cloud start to form, with occassional wanderings around the periphery. After \\(nrow(params)\\) steps what we see is a point cloud of samples from the joint posterior distribution. When viewed sequentially, points are not independent, but we are interested in working with the overall distribution, where the order of samples is not important. 35.3 Evaluating the model output A really important aspect of Bayesian inference is that the output is the joint posterior probability of all the parameters. This is very different from an optimization approach, which tries to find a single best parameter value. It is also different from estimating the independent posterior probabilities of each parameter – Bayesian posteriors frequently have strong correlation among parameters for reasons having to do both with model structure and the underlying data. The model we’re fitting has six free parameters, and therefore the output matrix has 6 columns, of which we’ve only looked at two. Unfortunately it is impossible to visualize a 6 dimensional parameter space on a two dimensional screen, so a very common practice (for models with a small to medium number of parameters) is to look at all pairwise scatterplots. If parameters are uncorrelated we will typically see oval shaped clouds that are oriented in the same directions as the axes. For parameters with linear correlations those clouds will be along a diagonal. For parameters with nonlinear trade-offs the shapes of the parameter clouds can be more complex, such as the banana-shaped or triangular. For the FvCB model we see very few parameters that are uncorrelated or have simple linear correlations, a fact that we should keep in mind when interpreting individual parameters. The three most common outputs that are performed on almost all Bayesian analyses are to look at the MCMC chains themselves, the marginal distributions of each parameter, and the overall summary statistics. The ‘trace’ diagrams below show the history of individual parameters during the MCMC sampling. There are different color lines that represent the fact that JAGS ran the MCMC multiple times, with each run (i.e. each color) being refered to as a different \\(chain\\). It is common to run multiple chains in order to assess whether the model, started from different points, consistently converges on the same answer. The ideal trace plot looks like white noise with all chains in agreement. The ‘density’ figures represent smoothed versions of the marginal distributions of each parameter. The tick marks on the x-axis are the actual samples. You will note that some posteriors will look approximately Normal, while others may be skewed or have clearly defined boundaries. On occassion there will even be posteriors that are multimodal. There is no assumption built into Bayesian statistics that the posteriors need be Normal, so as long as an MCMC has converged this diversity of shapes is valid. [note: the most common cause of multi-modal posteriors is a lack of convergence] Finally, the summary table reports, for each parameter, a mean, standard deviation, two variants of standard error, and standard quantile estimates (95% CI, interquartile, and median). The standard deviation of the posterior is a good summary statistic about how uncertain we are about a parameter. The Naive SE is the traditonal \\(\\frac{SD}{\\sqrt{n}}\\), which is an estimate of the NUMERICAL accuracy in our estimate of the mean. As we run the MCMC longer (i.e. take more samples), we get an answer that is numerically more precise (SE converges to 0) but the uncertainty in the parameter (i.e. SD) stays the same because that’s determined by the sample size of the DATA not the length of the MCMC. Finally, the Time-series SE is a variant of the SE calculation that accounts for the autocorrelation in the MCMC samples. In practice is is therefore more appropriate to use this term to assess numerical accuracy. Assessing the convergence of the MCMC is first done visually, but more generally the use of statistical diagnostics to assess convergence is highly encouraged. There are a number of metrics in the literature, but the most common is the Gelman-Brooks-Rubin statistic, which compare the variance within each chain to the variance across chains. If the chains have converged then this quantity should be 1. Values less than 1.05 are typically considered sufficient by most statisticians, but these are just rules-of-thumb. As with any modeling, whether statistical or process-based, another common diagnostic is a predicted vs observed plot. In a perfect model the data would fall along the 1:1 line. The deviations away from this line are the model residuals. If observations lie along a line other than the 1:1 this indicates that the model is biased in some way. This bias is often assessed by fitting a linear regression to the points, though two important things are noteworthy about this practice. First, the \\(R^2\\) and residual error of this regression are not the appropriate statistics to use to assess model performance (though you will frequently find them reported incorrectly in the literature). The correct \\(R^2\\) and residual error (a.k.a Root Mean Square Error, RMSE) are based on deviations from the 1:1 line, not the regression. The code below shows these two terms calculated by hand. The second thing to note about the regression line is that the standard regression F-test, which assesses deviations from 0, is not the test you are actually interested in, which is whether the line differs from 1:1. Therefore, while the test on the intercept is correct, as this value should be 0 in an unbiased model, the test statistic on the slope is typically of less interest (unless your question really is about whether the model is doing better than random). However, this form of bias can easily be assessed by looking to see if the CI for the slope overlaps with 1. In the final set of plots we look at the actual A-Ci and A-Q curves themselves. Here we’ve added two interval estimates around the curves. The CI captures the uncertainty in the parameters and will asympotically shrink with more and more data. The PI (predictive interval) includes the parameter and residual error. If our fit is good then 95% PI should thus encompass at least 95% of the observations. That said, as with any statistical model we want to look for systematic deviations in the residuals from either the mean or the range of the PI. Note: on the last figure you will get warnings about “No ACi” and “No AQ” which can be ignored. These are occuring because the file that had the ACi curve didn’t have an AQ curve, and the file that had the AQ curve didn’t have an ACi curve. 35.4 Additional information There is a more detailed R Vignette on the use of the PEcAn photosynthesis module available in the PEcAn Repository. 35.5 Citations Dietze, M.C. (2014). Gaps in knowledge and data driving uncertainty in models of photosynthesis. Photosynth. Res., 19, 3–14. Farquhar, G., Caemmerer, S. &amp; Berry, J.A. (1980). A biochemical model of photosynthetic CO2 assimilation in leaves of C3 species. Planta, 149, 78–90. Feng, X. &amp; Dietze, M.C. (2013). Scale dependence in the effects of leaf ecophysiological traits on photosynthesis: Bayesian parameterization of photosynthesis models. New Phytol., 200, 1132–1144. "],
["submitting-workflow-from-command-line.html", "36 Submitting Workflow from Command Line 36.1 Data assimilation with DART 36.2 Troubleshooting PEcAn", " 36 Submitting Workflow from Command Line This is how you can submit a workflow from the command line through the pecan web interface. This will use curl to submit all the requireed parameters to the web interface and trigger a run. # the host where the model should run # never use remote sites since you will need to pass your username/password and that WILL be stored hostname=pecan.vm # the site id where to run the model (NIWOT in this case) siteid=772 # start date and end date, / need to be replaced with %2F or use - (NOT TESTED) start=2004-01-01 end=2004-12-31 # if of model you want to run, rest of section parameters depend on the model selected (SIPNET 136) modelid=5000000002 # PFT selected (we should just use a number here) # NOTE: the square brackets are needed and will need be escaped with a \\ if you call this from command line pft[]=temperate.coniferous # initial pool condition (-1 means nothing is selected) input_poolinitcond=-1 # met data input_met=99000000006 # variables to collect variables=NPP,GPP # ensemble size runs=10 # use sensitivity analysis sensitivity=-1,1 # redirect to the edit pecan.xml file pecan_edit=on # redirect to edit the model configuration files model_edit=on # use browndog browndog=on For example the following will run the above workflow. Using -v in curl will show verbose output (needed) and the grep will make sure it only shows the redirect. This will show the actual workflowid: curl -s -v &#39;http://localhost:6480/pecan/04-runpecan.php?hostname=pecan.vm&amp;siteid=772&amp;start=2004-01-01&amp;end=2004-12-31&amp;modelid=5000000002&amp;pft\\[\\]=temperate.coniferous&amp;input_poolinitcond=-1&amp;input_met=99000000006&#39; 2&gt;&amp;1 | grep &#39;Location:&#39; &lt; Location: 05-running.php?workflowid=99000000004 In this case you can use the browser to see progress, or use the following to see the status: curl -s &#39;http://localhost:6480/pecan/dataset.php?workflowid=99000000004&amp;type=file&amp;name=STATUS&#39; TRAIT 2017-12-13 08:56:56 2017-12-13 08:56:57 DONE META 2017-12-13 08:56:57 2017-12-13 08:57:13 DONE CONFIG 2017-12-13 08:57:13 2017-12-13 08:57:14 DONE MODEL 2017-12-13 08:57:14 2017-12-13 08:57:15 DONE OUTPUT 2017-12-13 08:57:15 2017-12-13 08:57:15 DONE ENSEMBLE 2017-12-13 08:57:15 2017-12-13 08:57:16 DONE FINISHED 2017-12-13 08:57:16 2017-12-13 08:57:16 DONE Or to show the output log: curl -s &#39;http://localhost:6480/pecan/dataset.php?workflowid=99000000004&amp;type=file&amp;name=workflow.Rout&#39; R version 3.4.3 (2017-11-30) -- &quot;Kite-Eating Tree&quot; Copyright (C) 2017 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type &#39;license()&#39; or &#39;licence()&#39; for distribution details. R is a collaborative project with many contributors. .... 36.1 Data assimilation with DART In addition to the state assimilation routines found in the assim.sequential module, another approach for state data assimilation in PEcAn is through the DART workflow created by the DARES group in NCAR. This section gives a straight-forward explanation how to implement DART, focused on the technical aspects of the implementation. If there are any questions, feel free to send @Viskari an email (tt.viskari@gmail.com) or contacting DART support as they are quite awesome in helping people with problems. Also, if there are any suggestions on how to improve the wiki, please let me know. Running with current folders in PEcAn Currently the DART folders in PEcAn are that you can simply copy the structure there over a downloaded DART workflow and it should replace/add relevant files and folders. The most important step after that is to check and change the run paths in the following files: Path_name files in the work folders T_ED2IN file, as it indicates where the run results be written. advance_model.csh, as it indicates where to copy files from/to. Second thing is setting the state vector size. This is explained in more detail below, but essentially this is governed by the variable model_size in model_mod.f90. In addition to this, it should be changed in utils/F2R.f90 and R2F.f90 programs, which are responsible for reading and writing state variable information for the different ensembles. This also will be expanded below. Finally, the new state vector size should be updated for any other executable that runs it. Third thing needed are the initial condition and observation sequence files. They will always follow the same format and are explained in more detail below. Finally the ensemble size, which is the easiest to change. In the work subfolder, there is a file named input.nml. Simply changing the ensemble size there will set it for the run itself. Also remember that initial conditions file should have the equal amount of state vectors as there are ensemble members. Adjusting the workflow The central file for the actual workflow is advance_model.csh. It is a script DART calls to determine how the state vector changes between the two observation times and is essentially the only file one needs to change when changing state models or observations operators. The file itself should be commented to give a good idea of the flow, but beneath is a crude order of events. 1. Create a temporary folder to run the model in and copy/link required files in to it. 2. Read in the state vector values and times from DART. Here it is important to note that the values will be in binary format, which need to be read in by a Fortran program. In my system, there is a program called F2R which reads in the binary values and writes out in ascii form the state vector values as well as which ED2 history files it needs to copy based on the time stamps. 3. Run the observation operator, which writes the state vector state in to the history files and adjusts them if necessary. 4. Run the program. 5. Read the new state vector values from output files. 6. Convert the state vector values to the binary. In my system, this is done by the program R2F. Initial conditions file The initial conditions file, commonly named filter_ics although you can set it to something else in input.nml, is relatively simple in structure. It has one sequence repeating over the number of ensemble members. First line contains two times: Seconds and days. Just use one of them in this situation, but it has to match the starting time given in input.nml. After that each line should contain a value from the state vector in the order you want to treat them. R functions filter_ics.R and B_filter_ics.R in the R folder give good examples of how to create these. Observations files The file which contains the observations is commonly known as obs_seq.out, although again the name of the file can be changed in input.nml. The structure of the file is relatively straight-forward and the R function ObsSeq.R in the R subfolder has the write structure for this. Instead of writing it out here, I want to focus on a few really important details in this file. Each observations will have a time, a value, an uncertainty, a location and a kind. The first four are self-explanatory, but the kind is really important, but also unfortunately really easy to misunderstand. In this file, the kind does not refer to a unit or a type of observation, but which member of the state vector is this observation of. So if the kind was, for example, 5, it would mean that it was of the fifth member of the state vector. However, if the kind value is positive, the system assumes that there is some sort of an operator change in comparing the observation and state vector value which is specified in a subprogram in model_mod.f90. So for an direct identity comparison between the observation and the state vector value, the kind needs to be negative number of the state vector component. Thus, again if the observation is of the fifth state vector value, the kind should be set as -5. Thus it is recommendable that the state vector values have already been altered to be comparable with the observations. As for location, there are many ways to set in DART and the method needs to be chosen when compiling the code by giving the program which of the location mods it is to use. In our examples we used a 1-dimensional location vector with scaled values between 0 and 1. For future it makes sense to switch to a 2 dimensional long- and lat-scale, but for the time being the location does not impact the system a lot. The main impact will be if the covariances will be localized, as that will be decided on their locations. State variable vector in DART Creating/adjusting a state variable vector in DART is relatively straight-forward. Below are listed the steps to specify a state variable vector in DART. I. For each specific model, there should be an own folder within the DART root models folder. In this folder there is a model_mod.f90, which contains the model specific subroutines necessary for a DART run. At the beginning of this file there should be the following line: integer, parameter :: model_size = [number] The number here should be the number of variables in the vector. So for example if there were three state variables, then the line should look like this: integer, parameter :: model_size = 3 This number should also be changed to match with any of the other executables called during the run as indicated by the list above. In the DART root, there should be a folder named obs_kind, which contains a file called DEFAULT_obs_kind_mod.F90. It is important to note that all changes should be done to this file instead of obs_kind_mod.f90, as during compilation DART creates obs_kind_mod.f90 from DEFAULT_obs_kind_mod.F90. This program file contains all the defined observation types used by DART and numbers them for easier reference later. Different types are classified according to observation instrument or relevant observation phenomenon. Adding a new type only requires finding an unused number and starting a new identifying line with the following: integer, parameter, public :: &amp; KIND_… Note that the observation kind should always be easy to understand, so avoid using unnecessary acronyms. For example, when adding an observation type for Leaf Area Index, it would look like below: integer, parameter, public :: &amp; KIND_LEAF_AREA_INDEX = [number] In the DART root, there should be a folder named obs_def, which contains several files starting with obs_def_. There files contain the different available observation kinds classified either according to observation instrument or observable system. Each file starts with the line ! BEGIN DART PREPROCESS KIND LIST And end with line ! END DART PREPROCESS KIND LIST The lines between these two should contain ! The desired observation reference, the observation type, COMMON_CODE. For example, for observations relating to phenology, I have created a file called obs_def_phen_mod.f90. In this file I define the Leaf Area Index observations in the following way. ! BEGIN DART PREPROCESS KIND LIST ! LAI, TYPE_LEAF_AREA_INDEX, COMMON_CODE ! END DART PREPROCESS KIND LIST Note that the exclamation marks are necessary for the file. In the model specific folder, in the work subfolder there is a namelist file input.nml. This contains all the run specific information for DART. In it, there is a subtitle &amp;preprocess, under which there is a line input_files = ‘….’ This input_files sections must be set to refer to the obs_def file created in step III. The input files can contain references to multiple obs_def files if necessary. As an example, the reference to the obs_def_phen_mod.f90 would look like input_files = ‘../../../obs_def/obs_def_phen_mod.f90’ V. Finally, as an optional step, the different values in state vector can be typed. In model_mod, referred to in step I, there is a subroutine get_state_meta_data. In it, there is an input variable index_in, which refers to the vector component. So for instance for the second component of the vector index_in would be 2. If this is done, the variable kind has to be also included at the beginning of the model_mod.f90 file, at the section which begins use obs_kind_mod, only :: The location of the variable can be set, but for a 0-dimensional model we are discussing here, this is not necessary. Here, though, it is possible to set the variable types by including the following line if(index_in .eq. [number]) var_type = [One of the variable kinds set in step II] If the length of the state vector is changed, it is important that the script ran with DART produces a vector of that length. Change appropriately if necessary. After these steps, DART should be able to run with the state vector of interest. 36.2 Troubleshooting PEcAn 36.2.1 Cookies and pecan web pages You may need to disable cookies specifically for the pecan webserver in your browser. This shouldn’t be a problem running from the virtual machine, but your installation of php can include a ‘PHPSESSID’ that is quite long, and this can overflow the params field of the workflows table, depending on how long your hostname, model name, site name, etc are. 36.2.2 Warning: mkdir() [function.mkdir]: No such file or directory If you are seeing: Warning: mkdir() [function.mkdir]: No such file or directory in /path/to/pecan/web/runpecan.php at line 169 it is because you have used a relative path for $output_folder in system.php. 36.2.3 After creating a new PFT the tag for PFT not passed to config.xml in ED This is a result of the rather clunky way we currently have adding PFTs to PEcAn. This is happening because you need to edit the ./pecan/models/ed/data/pftmapping.csv file to include your new PFTs. This is what the file looks like: PEcAn;ED ebifarm.acru;11 ebifarm.acsa3;11 ... You just need to edit this file (in a text editor, no Excel) and add your PFT names and associated number to the end of the file. Once you do this, recompile PEcAn and it should then work for you. We currently need to reference this file in order to properly set the PFT number and maintain internal consistency between PEcAn and ED2. "],
["workflow-modules.html", "37 Workflow modules 37.1 Overview 37.2 Glossary", " 37 Workflow modules NOTE: As of PEcAn 1.2.6 – needs to be updated significantly 37.1 Overview Workflow inputs and outputs (click to open in new page, then zoom). Code used to generate this image is provided in qaqc/vignettes/module_output.Rmd 37.1.1 Load Settings: 37.1.1.1 read.settings(&quot;/home/pecan/pecan.xml&quot;) loads settings create directories generates new xml, put in output folder 37.1.2 Query Database: 37.1.2.1 get.trait.data() Queries the database for both the trait data and prior distributions associated with the PFTs specified in the settings file. The list of variables that are queried is determined by what variables have priors associated with them in the definition of the pft. Likewise, the list of species that are associated with a PFT determines what subset of data is extracted out of all data matching a given variable name. 37.1.3 Meta Analysis: 37.1.3.1 run.meta.analysis() The meta-analysis code begins by distilling the trait.data to just the values needed for the meta-analysis statistical model, with this being stored in madata.Rdata. This reduced form includes the conversion of all error statistics into precision (1/variance), and the indexing of sites, treatments, and greenhouse. In reality, the core meta-analysis code can be run independent of the trait database as long as input data is correctly formatted into the form shown in madata. The evaluation of the meta-analysis is done using a Bayesian statistical software package called JAGS that is called by the R code. For each trait, the R code will generate a [trait].model.bug file that is the JAGS code for the meta-analysis itself. This code is generated on the fly, with PEcAn adding or subtracting the site, treatment, and greenhouse terms depending upon the presence of these effects in the data itself. Meta-analyses are run, and summary plots are produced. 37.1.4 Write Configuration Files 37.1.4.1 write.configs(model) writes out a configuration file for each model run ** writes 500 configuration files for a 500 member ensemble ** for n traits, writes 6 * n + 1 files for running default Sensitivity Analysis (number can be changed in the pecan settings file) 37.1.5 Start Runs: 37.1.5.1 start.runs(model) This code starts the model runs using a model specific run function named start.runs.model. If the ecosystem model is running on a remote server, this module also takes care of all of the communication with the remote server and its run queue. Each of your subdirectories should now have a [run.id].out file in it. One instance of the model is run for each configuration file generated by the previous write configs module. 37.1.6 Get Model Output 37.1.6.1 get.model.output(model) This code first uses a model-specific model2netcdf.model function to convert the model output into a standard output format (MsTMIP). Then it extracts the data for requested variables specified in the settings file as settings$ensemble$variable, averages over the time-period specified as start.date and end.date, and stores the output in a file output.Rdata. The output.Rdata file contains two objects, sensitivity.output and ensemble.output, that is the model prediction for the parameter sets specified in sa.samples and ensemble.samples. In order to save bandwidth, if the model output is stored on a remote system PEcAn will perform these operations on the remote host and only return the output.Rdata object. 37.1.7 Ensemble Analysis 37.1.7.1 run.ensemble.analysis() This module makes some simple graphs of the ensemble output. Open ensemble.analysis.pdf to view the ensemble prediction as both a histogram and a boxplot. ensemble.ts.pdf provides a timeseries plot of the ensemble mean, meadian, and 95% CI 37.1.8 Sensitivity Analysis, Variance Decomposition 37.1.8.1 run.sensitivity.analysis() This function processes the output of the previous module into sensitivity analysis plots, sensitivityanalysis.pdf, and a variance decomposition plot, variancedecomposition.pdf . In the sensitivity plots you will see the parameter values on the x-axis, the model output on the Y, with the dots being the model evaluations and the line being the spline fit. The variance decomposition plot is discussed more below. For your reference, the R list object, sensitivity.results, stored in sensitivity.results.Rdata, contains all the components of the variance decomposition table, as well as the the input parameter space and splines from the sensitivity analysis (reminder: the output parameter space from the sensitivity analysis was in outputs.R). The variance decomposition plot contains three columns, the coefficient of variation (normalized posterior variance), the elasticity (normalized sensitivity), and the partial standard deviation of each model parameter. This graph is sorted by the variable explaining the largest amount of variability in the model output (right hand column). From this graph identify the top-tier parameters that you would target for future constraint. 37.2 Glossary Inputs: data sets that are used, and file paths leading to them Parameters: e.g. info set in settings file Outputs: data sets that are dropped, and the file paths leading to them "],
["available-meteorological-drivers.html", "38 Available Meteorological Drivers 38.1 Ameriflux 38.2 AmerifluxLBL 38.3 Fluxnet2015 38.4 NARR 38.5 CRUNCEP 38.6 CMIP5 38.7 NLDAS 38.8 GLDAS 38.9 PalEON 38.10 FluxnetLaThuile 38.11 Geostreams", " 38 Available Meteorological Drivers 38.1 Ameriflux Scale: site Resolution: 30 or 60 min Availability: varies by site http:\\/\\/ameriflux.lbl.gov\\/data\\/data-availability\\/ Notes: Old ORNL server, use is deprecated 38.2 AmerifluxLBL Scale: site Resolution: 30 or 60 min Availability: varies by site http:\\/\\/ameriflux.lbl.gov\\/data\\/data-availability\\/ Notes: new Lawrence Berkeley Lab server 38.3 Fluxnet2015 Scale: site Resolution: 30 or 60 min Availability: varies by site http://fluxnet.fluxdata.org/sites/site-list-and-pages Notes: Fluxnet 2015 synthesis product. Does not cover all FLUXNET sites 38.4 NARR Scale: North America Resolution: 3 hr, approx. 32km \\(Lambert conical projection\\) Availability: 1979-present 38.5 CRUNCEP Scale: global Resolution: 6hr, 0.5 degree Availability: 1901-2010 38.6 CMIP5 Scale: varies by model Resolution: 3 hr Availability: 2006-2100 Currently only GFDL available. Different scenerios and ensemble members can be set via Advanced Edit. 38.7 NLDAS Scale: Lower 48 + buffer, Resolution: 1 hour, .125 degree Availability: 1980-present 38.8 GLDAS Scale: Global Resolution: 3hr, 1 degree Availability: 1948-2010 38.9 PalEON Scale: -100 to -60 W Lon, 35 to 50 N Latitude \\(US northern hardwoods + buffer\\) Resolution: 6hr, 0.5 degree Availability: 850-2010 38.10 FluxnetLaThuile Scale: site Resolution: 30 or 60 min Availability: varies by site http:\\/\\/www.fluxdata.org\\/DataInfo\\/Dataset%20Doc%20Lib\\/SynthDataSummary.aspx Notes: 2007 synthesis. Fluxnet2015 supercedes this for sites that have been updated 38.11 Geostreams Scale: site Resolution: varies Availability: varies by site Notes: This is a protocol, not a single archive. The PEcAn functions currently default to querying [https://terraref.ncsa.illinois.edu/clowder/api/geostreams], which requires login and contains data from only two sites (Urbana IL and Maricopa AZ). However the interface can be used with any server that supports the Geostreams API. THIS IS EXPERIMENTAL DO NOT USE THIS IN PRODUCTION SUBJECT TO CHANGE "],
["pecan-docker-architecture.html", "39 PEcAn Docker Architecture", " 39 PEcAn Docker Architecture The PEcAn docker architecture consists of many containers (see figure below) that will communicate with each other. The goal of this architecture is to easily expand the PEcAn system by deploying new model containers and registering them with PEcAn. Once this is done the user can now use these new models in their work. The PEcAn framework will setup the configurations for the models, and send a message to the model containers to start execution. Once the execution is finished the PEcAn framework will continue. This is exactly as if the model is running on a HPC machine. Models can be executed in paralel by launching multiple model containers. PEcAn docker containers As can be seen in the figure the architecture leverages of two standard containers (in orange). The first container is postgresql with postgis (mdillon/postgis) which is used to store the database used by both BETY and PEcAn. The second containers is a messagebus, more specifically RabbitMQ (rabbitmq). The BETY app container (pecan/bety) is the front end to the BETY database and is connected to the postgresql container. A http server can be put in front of this container for SSL termination as well to allow for load balancing (by using multiple BETY app containers). The PEcAn framework containers consist of multiple unique ways to interact with the PEcAn system (none of these containers will have any models installed): PEcAn shiny hosts the shiny applications developed and will interact with the database to get all information necessary to display PEcAn rstudio is a rstudio environment with the PEcAn libraries preloaded. This allows for prototyping of new algorithms that can be used as part of the PEcAn framework later. PEcAn web allows the user to create a new PEcAn workflow. The workflow is stored in the database, and the models are exectued by the model containers. PEcAn cli will allow the user to give a pecan.xml file that will be executed by the PEcAn framework. The workflow created from the XML file is stored in the database, and the models are exectued by the model containers. The model containers contain the actual models that are executed as well as small wrappers to make them work in the PEcAn framework. The containers will run the model based on the parameters received from the message bus and convert the outputs back to the standard PEcAn output format. Once the container is finished processing a message it will immediatly get the next message and start processing it. THIS IS EXPERIMENTAL DO NOT USE THIS IN PRODUCTION SUBJECT TO CHANGE "],
["what-is-docker.html", "40 What is Docker?", " 40 What is Docker? Docker is a relativly new technology to encapsulate software as a service. This allow for simple distribution of software since all depedencies are installed as well. Unlike traditional VM’s, in docker each component will only run a single service/process and is build on top of existing services provided by the host OS (such as disk access, networking, memory managment etc.). To create a complex system such as PEcAn, we take multiple of these components and link them together. A common term used in docker is that of an image or container. This refers to all the software needed to run a single service, including the service itself, bundled and installed. The difference between an image and a container is that a container is the execution of an image (in object oriented programming, an image is a class and a container is an instance of the class). When creating docker images a good mental model to follow is to have a single process per container, and have multiple containers together create an application. For example in the case of BETY we will need at least two processes, the database and the BETY web application. When distributing a VM with the BETY application we will need to create VM with a base OS (in our case this would be Ubuntu), a database (PostgreSQL) and the BETY webapp (ruby). This requires us to install the dependencies and applications. In the case of Docker, we will need a container for the database (standard postgis container) as well as the custom BETY container (build on top of the exising Ruby container), both running on a host OS (coreOS for example). When starting BETY we start the postgis container first, and next start the BETY container telling it where it can find the postgis container. To upgrade we stop the BETY container, download the latest version, tell it to upgrade the database, and start the BETY container. There is no need to install new dependencies for BETY since they are all shipped as part of the container. In the case of PEcAn we want to use this ability to ship all dependencies as part of the image to make it easier for the users to download a new model, since the image will contain everything that is needed to run the model as part of PEcAn. If two models depend on different versions of the library we no longer need to worry on how to install these models next to each other and creating issues with the libraries used. Each image will contain only a single model and all libraries needed by that model. The next section, architecture will go in more detail on the containers that make up the PEcAn framework and how these containers interact with each other. THIS IS EXPERIMENTAL DO NOT USE THIS IN PRODUCTION SUBJECT TO CHANGE "],
["dockerfiles-for-models.html", "41 Dockerfiles for Models 41.1 SIPNET 41.2 PEcAn Project use to teach Ecological model-data synthesis", " 41 Dockerfiles for Models Each model will be disitrbuted as a docker container. These docker containers will contain the model, as well as some additional code to convert the output from the model to the standard PEcAn output, and code to connect to the message bus and receive messages. Most of these dockerfiles will be the same and will compile the model in one container, copy the resulting binary and any additional files needed to a second container and add all PEcAn pieces needed. This process will reduce the size of the final image. Each of these docker images will be ready to run the model given an input file and produce outputs that conform to PEcAn. Each container will contain a small script that will execute the binary, check the model exit code, convert the model output to PEcAn output and quit. The conversion from the model specific output to the PEcAn output is done by calling the model2netcdf. This code will assume the site is located at lat=0, lon=0. To set this you can start the docker process with -e &quot;SITE_LAT=35.9782&quot; -e &quot;SITE_LON=-79.0942&quot;. The following variables can be set: SITE_LAT : Latitutude of the site, written in output files, default is 0 SITE_LON : Longitude of the site, written in output files, default is 0 START_DATE : Start date of the model to be executed. END_DATE : End date of the model to be executed. DELETE_RAW : Should the model output be deleted, default is no. OVERWRITE : Should any results be overwritten Following environment variables are only for information purpose and should not be changed: - MODEL : The name of the model, used in the script to run the model - BINARY : Location of the acutual binary, used in the script to run the model - OUTDIR : Location where data is, in this case /work - PECAN_VERSION : Version of PEcAn used to compile, default is develop 41.1 SIPNET The folllowing command will build sipnet v136 for PEcAn using the branch that is currently checked out. docker build \\ --build-arg MODEL_VERSION=136 \\ --build-arg PECAN_VERSION=$(git rev-parse --abbrev-ref HEAD) \\ --tag pecan/pecan-sipnet:136 \\ --file docker/Dockerfile.sipnet \\ . Once the process is finished you can push (upload) the created image to docker hub. It will use the tag to place the image. In this case the image will be placed in the pecan project as the pecan-sipnet repository and tagged with 136. To do this you can use: # do this only once docker login # push image docker push pecan/pecan-sipnet:136 Once the image is pushed to dockerhub anybody can run the model. If you have not already downloaded the docker container the run command will download the image. Next it will run the image and will execute either the default command or the comman line given to the container. In the following example the default command is executed which will run the model and generate the PEcAn output files. # get test data (only need to do this once) curl -o sipnet.data.zip http://isda.ncsa.illinois.edu/~kooper/PEcAn/sipnet/sipnet.data.zip unzip sipnet.data.zip # cleanup if you rerun rm -f sipnet.data/{*.nc*,DONE,ERROR,sipnet.out,std*.log} # run the actual model docker run -t -i --rm -v ${PWD}/sipnet.data:/work pecan/pecan-sipnet:136 41.2 PEcAn Project use to teach Ecological model-data synthesis 41.2.1 University classes 41.2.1.1 GE 375 - Environmental Modeling - Spring 2013, 2014 (Mike Dietze, Boston University) The final “Case Study: Terrestrial Ecosystem Models” is a PEcAn-based hands-on activity. Each class has been 25 students. GE 585 - Ecological forecasting Fall 2013 (Mike Dietze, Boston University) 41.2.2 Summer Courses / Workshops 41.2.2.1 Annual summer course in flux measurement and advanced modeling (Mike Dietze, Ankur Desai) Niwot Ridge, CO About 1/3 lecture, 2/3 hands-on (the syllabus is actually wrong as it list the other way around). Each class has 24 students. 2013 Syllabus see Tuesday Week 2 Data Assimilation lectures and PEcAn demo and the Class projects and presentations on Thursday and Friday. (Most students use PEcAn for their group projects. 2014 will be the third year that PEcAn has been used for this course. 41.2.2.2 Assimilating Long-Term Data into Ecosystem Models: Paleo-Ecological Observatory Network (PalEON) Project Here is a link to the course: https://www3.nd.edu/~paleolab/paleonproject/summer-course/ This course uses the same demo as above, including collecting data in the field and assimilating it (part 3) 41.2.2.3 Integrating Evidence on Forest Response to Climate Change: Physiology to Regional Abundance http://blue.for.msu.edu/macrosystems/workshop May 13-14, 2013 Session 4: Integrating Forest Data Into Ecosystem Models 41.2.2.4 Ecological Society of America meetings Workshop: Combining Field Measurements and Ecosystem Models 41.2.3 Selected Publications Dietze, M.C., D.S LeBauer, R. Kooper (2013) On improving the communication between models and data. Plant, Cell, &amp; Environment doi:10.1111/pce.12043 LeBauer, D.S., D. Wang, K. Richter, C. Davidson, &amp; M.C. Dietze. (2013). Facilitating feedbacks between field measurements and ecosystem models. Ecological Monographs. doi:10.1890/12-0137.1 "],
["faq.html", "42 FAQ", " 42 FAQ "],
["user-appendix.html", "43 User Appendix", " 43 User Appendix 43.0.1 File download: Alternative options when behind a firewall 43.0.1.1 Using the PEcAn download.file() function download.file(url, destination_file, method) This custom PEcAn function works together with the base R function download.file (https://stat.ethz.ch/R-manual/R-devel/library/utils/html/download.file.html). However, it provides expanded functionality to generalize the use for a broad range of environments. This is because some computing environments are behind a firewall or proxy, including FTP firewalls. This may require the use of a custom FTP program and/or initial proxy server authentication to retrieve the files needed by PEcAn (e.g. meteorology drivers, other inputs) to run certain model simulations or tools. For example, the Brookhaven National Laboratory (BNL) requires an initial connection to a FTP proxy before downloading files via FTP protocol. As a result, the computers running PEcAn behind the BNL firewall (e.g. https://modex.bnl.gov) use the ncftp cleint (http://www.ncftp.com/) to download files for PEcAn because the base options with R::base download.file() such as curl, libcurl which don’t have the functionality to provide credentials for a proxy or even those such as wget which do but don’t easily allow for connecting through a proxy server before downloading files. The current option for use in these instances is ncftp, specifically ncftpget Examples: HTTP download.file(&quot;http://lib.stat.cmu.edu/datasets/csb/ch11b.txt&quot;,&quot;~/test.download.txt&quot;) FTP download.file(&quot;ftp://ftp.cdc.noaa.gov/Datasets/NARR/monolevel/pres.sfc.2000.nc&quot;, &quot;~/pres.sfc.2000.nc&quot;) customizing to use ncftp when running behind an FTP firewall (requires ncftp to be installed and availible) download.file(&quot;ftp://ftp.cdc.noaa.gov/Datasets/NARR/monolevel/pres.sfc.2000.nc&quot;, &quot;~/pres.sfc.2000.nc&quot;, method=&quot;&quot;ncftpget&quot;) On modex.bnl.gov, the ncftp firewall configuration file (e.g. ~/.ncftp/firewall) is configured as: firewall-type=1 firewall-host=ftpgateway.sec.bnl.local firewall-port=21 which then allows for direct connection through the firewall using a command like: ncftpget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-fortran-4.4.4.tar.gz To allow the use of ncftpget from within the download.file() function you need to set your R profile download.ftp.method option in your options list. To see your current R options run options() from R cmd, which should look something like this: &gt; options() $add.smooth [1] TRUE $bitmapType [1] &quot;cairo&quot; $browser [1] &quot;/usr/bin/xdg-open&quot; $browserNLdisabled [1] FALSE $CBoundsCheck [1] FALSE $check.bounds [1] FALSE $citation.bibtex.max [1] 1 $continue [1] &quot;+ &quot; $contrasts unordered ordered &quot;contr.treatment&quot; &quot;contr.poly&quot; In order to set your download.ftp.method option you need to add a line such as # set default FTP options(download.ftp.method = &quot;ncftpget&quot;) In your ~/.Rprofile. On modex at BNL we have set the global option in /usr/lib64/R/etc/Rprofile.site. Once this is done you should be able to see the option set using this command in R: &gt; options(&quot;download.ftp.method&quot;) $download.ftp.method [1] &quot;ncftpget&quot; "]
]
